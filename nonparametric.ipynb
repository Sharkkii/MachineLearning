{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ノンパラメトリックモデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "機械学習のモデルはパラメトリックモデルとノンパラメトリックモデルに大別される。パラメトリックモデルでは関数形を仮定して、必要なパラメータを最尤推定などによって決定する。それに対してノンパラメトリックな方法では関数形を仮定しないため、表現力が制限されない。また、ノンパラメトリックモデルではOverfitting/Underfittingが起きないことも利点である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "領域$U$上の分布$p({\\bf x})$を推定することを考える。あるデータ点を観測したとき、領域$R \\subseteq U$上に存在する確率は\n",
    "$$\n",
    "P \\equiv \\int_{R} p({\\bf x}) d {\\bf x}\n",
    "$$\n",
    "である。$N$個のデータ点を観測したとき、$K$個の領域$R$上に存在する確率は\n",
    "$$\n",
    "Q \\equiv Bin(K | P, N) = {}_N C_K P^K (1 - P)^{N - K}\n",
    "$$\n",
    "である。ここでBinは二項分布を表す。二項分布の期待値および分散は\n",
    "\\begin{eqnarray*}\n",
    "\\mathbf{E}[K] &=& N P \\\\\n",
    "\\mathbf{V}[K] &=& N P (1 - P)\n",
    "\\end{eqnarray*}\n",
    "であるから、領域$R$上に存在するデータ点の平均割合の期待値および分散は\n",
    "\\begin{eqnarray*}\n",
    "\\mathbf{E}[K/N] &=& P \\\\\n",
    "\\mathbf{V}[K/N] &=& P (1 - P) / N\n",
    "\\end{eqnarray*}\n",
    "となる。$K, N \\rightarrow \\infty$とすると$\\mathbf{V}[K/N] \\rightarrow 0$となることから、観測するデータ点数を限りなく大きくとれば\n",
    "$$\n",
    "K/N \\simeq P\n",
    "$$\n",
    "が成立する。\n",
    "ところで、領域$R$上において$p({\\bf x})$が一定であるとみなせるほど$R$が微小であると仮定できれば\n",
    "$$\n",
    "P \\simeq p({\\bf x}) V\n",
    "$$\n",
    "が成立する。ここで$V$は領域$R$の体積であり、$V \\equiv \\int_{R} d{\\bf x}$である。\n",
    "上の2式が成立すると仮定すれば\n",
    "$$\n",
    "p({\\bf x}) \\simeq \\frac{K}{N V}\n",
    "$$\n",
    "が成立する。ただし、領域$R$上に無数のデータ点が含まれるという仮定と、領域$R$が微小であるという仮定は相反するということに注意する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K近傍法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K近傍法はノンパラメトリックモデルの1つで、$p({\\bf x}) = \\frac{K}{NV}$の$K$を固定した上で$V$を推定し、分布$p$を求める手法である。\n",
    "点${\\bf x}$を含む領域$R$として、その点${\\bf x}$を中心とし、ちょうど$K$点を含むような超球を考える。この球の体積を$V$とすれば、上の式より点${\\bf x}$における分布の値$p({\\bf x})$を求めることができる。\n",
    "\n",
    "K近傍法をクラス分類問題モデルに利用することもできる。訓練データを$\\{ ({\\bf x_n}, y_n) \\}$とし、それらがクラス$C_1, \\dots, C_K$のいずれかに属するとする。テストデータ${\\bf x}$に対して、それが属するクラスを予測する問題を考える。\n",
    "求めるのは(${\\bf x}$を観測した条件下における)クラスの事後確率$p(C_k | {\\bf x})$である。ベイズの定理より\n",
    "\\begin{eqnarray*}\n",
    "p(C_k | {\\bf x}) &=& \\frac{p(C_k) p({\\bf x_k} | C_k)}{p({\\bf x}}\n",
    "\\end{eqnarray*}\n",
    "であるから、右辺に現れる値を求めれば良い。\n",
    "\n",
    "クラスの事前分布$p(C_k)$は訓練データから決める。訓練データ数を$N$とし、そのうちクラス$C_k$に属するものの数を$N_k$とする。このとき分布は\n",
    "$$\n",
    "p(C_k) = \\frac{N_k}{N}\n",
    "$$\n",
    "と書ける。\n",
    "\n",
    "データ点の分布$p({\\bf x})$およびクラス$C_k$に属するデータ点の分布については、K近傍法を利用して密度推定を行う。すなわち、訓練データ$K$個を含むような、${\\bf x}$を中心とする超球を考える。この超球に含まれる点でクラス$C_k$に属するものの数を$K_k$とすれば\n",
    "\\begin{eqnarray*}\n",
    "p({\\bf x}) = \\frac{K}{NV} \\\\\n",
    "p({\\bf x} | C_k) = \\frac{K_k}{N_k V}\n",
    "\\end{eqnarray*}\n",
    "となる。\n",
    "\n",
    "以上より\n",
    "\\begin{eqnarray*}\n",
    "p(C_k | {\\bf x}) = \\frac{p(C_k) p({\\bf x_k} | C_k)}{p({\\bf x})}\n",
    "= \\frac{\\frac{N_k}{N} \\frac{K_k}{N_k V}}{\\frac{K}{N V}}\n",
    "= \\frac{K_k}{K}\n",
    "\\end{eqnarray*}\n",
    "が成立する。これは予測したいデータ${\\bf x}$の周りの$K$個の点から分布を推定できることを示している。このデータ点をあるクラスに分類するときには、分布の値が最大となるようなクラスを選択すれば良い。すなわち\n",
    "$$\n",
    "k^{\\ast} = argmax_{k} p(C_k | {\\bf x}) = argmax_{k} \\frac{K_k}{K}\n",
    "$$\n",
    "を満たすクラス$C_{k^{\\ast}}$を選択すれば良い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この議論から、K近傍法の学習・予測は次のようになる。\n",
    "まず、訓練データに対する学習では単にそれをを記憶するだけである。テストデータに対する予測は、テストデータ点${\\bf x}$に最も近い$K$点を探し、その中で最も多数派のクラスを割り当てる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# カーネル密度推定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "カーネル密度推定法はノンパラメトリックモデルの1つで、$p({\\bf x}) = \\frac{K}{N V}$の$V$を固定した上で$K$を推定し、分布$p$を求める手法である。データ点${\\bf x_n}$に対して、その近傍では1、それ以外は0となるようなカーネル関数$k$を定義する。\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "k({\\bf x}, {\\bf x_n}) \\equiv\n",
    "\\begin{cases}\n",
    "1 \\text{ if } | {\\bf x} - {\\bf x_n} | \\le \\frac{1}{2} \\\\\n",
    "0 \\text{ otherwise }\n",
    "\\end{cases}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "ここで近傍はデータ点${\\bf x_n}$を中心とする1辺の長さが1の超立方体としている。$N$個のデータ集合$\\{ {\\bf x_n} \\}$についてカーネル関数の和をとることで、点${\\bf x}$における分布の値$p({\\bf x})$を\n",
    "\n",
    "$$\n",
    "p({\\bf x}) \\equiv \\frac{1}{N} \\sum_{n=1}^N k({\\bf x}, {\\bf x_n})\n",
    "$$\n",
    "\n",
    "のように推定することができる。これが定義域上で非負の値をとり、かつ正規化されていることは定義から明らかである。\n",
    "1辺の長さが$h$の超立方体を考えるならば\n",
    "\n",
    "$$\n",
    "p({\\bf x}) = \\frac{1}{N} \\frac{1}{h^D} \\sum_{n=1}^N k(\\frac{\\bf x}{h}, \\frac{\\bf x_n}{h})\n",
    "$$\n",
    "\n",
    "とすれば良い。これは${\\bf x_n}$を中心とする1辺の長さが$h$の超立方体に${\\bf x}$が含まれているような${\\bf x_n}$の個数であるが、逆に言えば${\\bf x}$を中心とする1辺の長さが$h$の超立方体に含まれる${\\bf x_n}$の個数と考えることができる。\n",
    "ここで、1辺の長さが$h$の超立方体の体積は(データの次元を$D$として)$h^D$であるが、この値は固定されていることに注意する。\n",
    "\n",
    "より一般に、カーネル関数として確率の要請を満たす任意の関数をとることができる。代表的なものとしては以下に示すガウスカーネルがある。\n",
    "\n",
    "$$\n",
    "k({\\bf x}, {\\bf x_n}) \\equiv \\frac{1}{2 \\pi h^2} \\exp \\left( - \\frac{1}{2 h^2} ||{\\bf x} - {\\bf x_n}||^2 \\right)\n",
    "$$\n",
    "\n",
    "上と同様にガウスカーネルをすべてのデータ点に関して足し合わせることで分布の推定とすることができる。ガウスカーネルはデータ点の近傍では大きな値をとり、離れたところでは小さな値をとることから、スパースな分布を推定することができる。これは一種の次元削減の効果がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
