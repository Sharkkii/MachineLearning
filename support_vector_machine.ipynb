{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# サポートベクトルマシン"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二値分類問題を解くモデルで頑健なものがサポートベクトルマシンである。\n",
    "\n",
    "サポートベクトルマシンでは、マージンを最大化するように分類境界を決める。ここで分類境界は\n",
    "\n",
    "$$\n",
    "f({\\bf x}) \\equiv {\\bf w}^T {\\bf x} + b = 0\n",
    "$$\n",
    "\n",
    "を満たす超平面である。いま、データ集合が線形分離可能であると仮定し、それを線形分離するような超平面(のパラメータ${\\bf w}$)をとる。このときデータ$x_n$と分類境界との距離は$\\hat{y_n} = f({\\bf x_n})$として\n",
    "\n",
    "$$\n",
    "D({\\bf x_n}) \\equiv \\frac{|f({\\bf x_n})|}{||{\\bf w}||} = \\frac{y_n \\hat{y_n}}{||{\\bf w}||} = \\frac{y_n ({\\bf w}^T {\\bf x_n} + b)}{||{\\bf w}||}\n",
    "$$\n",
    "\n",
    "と求められる。ここで正しく予測できたデータについては\n",
    "\n",
    "$$\n",
    "y_n \\hat{y_n} > 0\n",
    "$$\n",
    "\n",
    "が成立していることを利用している。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2つのクラスを$C_+, C_-$と書き、\n",
    "\n",
    "$$\n",
    "S_+ = \\{ {\\bf x_n} | y_n \\in C_+ \\} \\\\\n",
    "S_- = \\{ {\\bf x_n} | y_n \\in C_- \\}\n",
    "$$\n",
    "\n",
    "とする。また、クラス$C_+$, $C_-$と分類境界との距離をそれぞれ\n",
    "\n",
    "$$\n",
    "D(C_+) \\equiv min_{\\bf x \\in S_+} D({\\bf x}) \\\\\n",
    "D(C_-) \\equiv min_{\\bf x \\in S_-} D({\\bf x})\n",
    "$$\n",
    "\n",
    "と定義する。超平面${\\bf w}^T {\\bf x} + b = 0$データ集合を線形分離したとき、この$D(C_+)$または$D(C_-)$を\n",
    "マージンという。ただし、分類境界は$D(C_+) = D(C_-)$となるように定めることにする。さらに、\n",
    "\n",
    "$$\n",
    "{\\bf x_+} \\equiv argmin_{\\bf x \\in S_+} D({\\bf x}) \\\\\n",
    "{\\bf x_-} \\equiv argmin_{\\bf x \\in S_-} D({\\bf x}) \\\\\n",
    "$$\n",
    "\n",
    "をサポートベクトルと呼ぶ。定義より、\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\forall {\\bf x} \\in S_+. D({\\bf x_+}) &\\le& D({\\bf x}) \\\\\n",
    "\\forall {\\bf x} \\in S_-. D({\\bf x_-}) &\\le& D({\\bf x}) \\\\\n",
    "D({\\bf x_+}) &=& D({\\bf x_-})\n",
    "\\end{eqnarray*}\n",
    "\n",
    "が成立する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サポートベクトルマシンでは超平面${\\bf w}^T {\\bf x} + b = 0$を分類境界とし、各データ${\\bf x}$に対するスコア\n",
    "\n",
    "$$\n",
    "f({\\bf x}) = {\\bf w}^T {\\bf x} + b\n",
    "$$\n",
    "\n",
    "が正の値ならば$C_1$に属すると予測し、負の値ならば$C_2$に属すると予測する。\n",
    "\n",
    "さて、${\\bf x_+}, {\\bf x_-}$について、\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\forall {\\bf x} \\in S_+. D({\\bf x_+}) \\le D({\\bf x})\n",
    "&\\Leftrightarrow& \\forall {\\bf x} \\in S_+. {\\bf w}^T {\\bf x_+} + b \\le {\\bf w}^T {\\bf x} + b \\\\\n",
    "&\\Leftrightarrow& \\forall {\\bf x} \\in S_+. f({\\bf x_+}) \\le f({\\bf x})\n",
    "\\end{eqnarray*}\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\forall {\\bf x} \\in S_-. D({\\bf x_-}) \\le D({\\bf x})\n",
    "&\\Leftrightarrow& \\forall {\\bf x} \\in S_-. {\\bf w}^T {\\bf x_-} + b \\le {\\bf w}^T {\\bf x} + b \\\\\n",
    "&\\Leftrightarrow& \\forall {\\bf x} \\in S_-. f({\\bf x_-}) \\le f({\\bf x})\n",
    "\\end{eqnarray*}\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "D({\\bf x_+}) = D({\\bf x_-})\n",
    "&\\Leftrightarrow& {\\bf w}^T {\\bf x_+} + b = {\\bf w}^T {\\bf x_-} + b \\\\\n",
    "&\\Leftrightarrow& f({\\bf x_+}) = f({\\bf x_-})\n",
    "\\end{eqnarray*}\n",
    "\n",
    "が成立している。また、定数$c > 0$に対して\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "{\\bf w'} &=& c {\\bf w} \\\\\n",
    "b' &=& c b\n",
    "\\end{eqnarray*}\n",
    "\n",
    "と変換しても予測結果は変化せず、スコアの大小関係も変化しない。ゆえに${\\bf w}, b$を適当にスケーリングすることで\n",
    "\n",
    "$$\n",
    "{\\bf w'}^T {\\bf x_+} + b' = 1 \\\\\n",
    "{\\bf w'}^T {\\bf x_-} + b' = 1 \\\\\n",
    "\\forall {\\bf x}. | {\\bf w'}^T {\\bf x} + b' | \\ge 1\n",
    "$$\n",
    "\n",
    "を満たすようにできる。以下ではこの${\\bf w'}, b'$を改めて${\\bf w}, b$と書くことにする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上の設定を踏まえ、マージンの最大化問題を定式化すると次のようになる。\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "argmax_{{\\bf w},b} \\frac{1}{||{\\bf w}||} \\text{ s.t. } t_n ({\\bf w}^T {\\bf x_n} + b) \\ge 1\n",
    "\\end{eqnarray*}\n",
    "\n",
    "ここでパラメータのスケーリングによって$f({\\bf x_+}) = f({\\bf x_-}) = 1$となっていることに注意する。この最大化問題は\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "argmin_{{\\bf w},b} \\frac{1}{2} ||{\\bf w}||^2 \\text{ s.t. } t_n ({\\bf w}^T {\\bf x_n} + b) \\ge 1\n",
    "\\end{eqnarray*}\n",
    "\n",
    "という最小化問題と同値である。ここで最小化したい値$\\frac{1}{2} ||{\\bf w}||^2$をハードマージンという。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般に線形分離可能とは限らないデータ集合について二値分類を行うことを考える。線形分離可能とは限らないデータ集合に対してはペナルティを導入し、ソフトマージンを最小化する。\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "argmin_{{\\bf w},b} \\frac{1}{2} ||{\\bf w}||^2 + C \\Sigma_n \\xi_n \\\\\n",
    "\\text{ s.t. } y_n ({\\bf w}^T {\\bf x_n} + b) \\ge 1 - \\xi_n, {\\bf \\xi} \\ge {\\bf 0}\n",
    "\\end{eqnarray*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
