{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kmeans法はクラスタリングを行う機械学習モデルである。教師なし学習の1つにクラスタリングがあるが、これはデータ集合をいくつかのクラスタに分割することを目標としている。クラスタリングによって類似した性質をもつデータ集合を見つけることができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kmeans法ではクラスタの数をハイパーパラメータ$k$として決めた上でアルゴリズムを実行する。この$k$は後で述べるエルボー法などの手法で決めることが多い。\n",
    "\n",
    "クラスタが$K$個あると仮定して、その$K$個のクラスタを見つけることを考える。\n",
    "\n",
    "Kmeans法ではクラスタを代表するクラスタ中心というものを利用する。$K$個のクラスタ中心が定まれば、各データ点は最も近いクラスタ中心と同じクラスに属するとしてクラスタを形成することができる。逆に$K$個のクラスタが存在すれば、そこからクラスタ中心を求めることができる。これを繰り返し行うことで$k$個のクラスタ(およびクラスタ中心)を見つけることができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クラスタを$C_1, \\dots, C_K$とし、各クラスタ中心を$\\{ {\\bf \\mu_k} \\}$とする。データ集合$\\{ {\\bf x_n} \\}$に対して、クラスタへの割り当て$r$を\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "r_{nk} \\equiv \n",
    "\\begin{cases}\n",
    "1 & \\text{ if } {\\bf x_n \\in C_k } \\\\\n",
    "0 & \\text{ otherwise }\n",
    "\\end{cases}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "と定義する。このとき、クラスタリングはクラスタ中心と各データ点の(クラスタへの)割り当てを決める問題となる。\n",
    "\n",
    "データ点${\\bf x}$がクラスタ$k$に属するとき、この${\\bf x}$と${\\bf \\mu_k}$の距離が小さいことが望ましい。よって、損失関数を次のように定義する。\n",
    "\n",
    "$$\n",
    "E \\equiv \\Sigma_{n=1}^N \\Sigma_{k=1}^K r_{nk} || {\\bf x_n} - {\\bf \\mu_k} ||^2\n",
    "$$\n",
    "\n",
    "このとき、クラスタリングは上の損失関数を最小化するような$\\{ \\bf \\mu_k \\}$および$\\{ r_{nk} \\}$を求める問題となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各$n$について、損失関数を最小化するような$r_{nk}$は明らかに\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "r_{nk} = \n",
    "\\begin{cases}\n",
    "1 & \\text{ if } k = argmin_j || {\\bf x_n} - {\\bf \\mu_j} ||^2 \\\\\n",
    "0 & \\text{ otherwise }\n",
    "\\end{cases}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "となる。損失関数を最小化するような${\\bf \\mu_k}$については、微分を0とおいて\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial {\\bf \\mu_k}} E = {\\bf 0}\n",
    "\\Leftrightarrow - 2 \\Sigma_{n=1}^N r_{nk} ({\\bf x_n} - {\\bf \\mu_k}) {\\bf x_n} = {\\bf 0} \\\\\n",
    "\\Leftrightarrow \\mu_k = \\frac{\\Sigma_{n=1}^N r_{nk} {\\bf x_n}}{\\Sigma_{n=1}^N {\\bf x_n}}\n",
    "$$\n",
    "\n",
    "を得る。これはクラスタ$k$に属するデータ点の平均値を表している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "互いに依存する2変数の最適化問題では、2変数をそれぞれ交互に最適化すれば良い。すなわち、次に示すEステップとMステップを交互に実行し、収束する(か最大反復回数に達する)まで繰り返せば良い。\n",
    "\n",
    "- E step  \n",
    "各データ点について、最も近いクラスタ中心のクラスタを割り当てる。\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "r_{nk} = \n",
    "\\begin{cases}\n",
    "1 & \\text{ if } k = argmin_j || {\\bf x_n} - {\\bf \\mu_j} ||^2 \\\\\n",
    "0 & \\text{ otherwise }\n",
    "\\end{cases}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n",
    "- M step  \n",
    "各クラスタについて、それに属しているデータ点すべての平均をとり、新しいクラスタ中心とする。\n",
    "\n",
    "$$\n",
    "\\mu_k = \\frac{\\Sigma_{n=1}^N r_{nk} {\\bf x_n}}{\\Sigma_{n=1}^N {\\bf x_n}}\n",
    "$$\n",
    "\n",
    "なお、クラスタ中心の初期値によっては収束に時間がかかることがあるため、それを改善する方法としてKmeans++などが開発されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 混合ガウスモデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "混合ガウスモデルは、与えられたデータ集合$\\{ {\\bf x_n} \\}$の分布を混合ガウス分布によって近似する教師なし学習のモデルである。このモデルでは、各データ点${\\bf x_n}$は暗に$K$個のクラス$C_1, \\dots, C_K$のいずれかに属するとし、それぞれが異なるガウス分布に従って生成されると仮定している。モデルの学習では、混合ガウス分布の混合係数および各ガウス分布のパラメータを推定することが目標となる。\n",
    "\n",
    "一般に混合モデルとは、複数の確率分布$\\{ p_k \\}$の重み付き和をとったもので\n",
    "\n",
    "$$\n",
    "p({\\bf x}) = \\sum_k \\pi_k p_k({\\bf x})\n",
    "$$\n",
    "\n",
    "と表される。ここで重み$\\pi_i$は混合係数と呼ばれ、$\\forall k. \\pi_k \\ge 0, \\sum_k \\pi_k = 1$を満たすとする。ここでは混合分布として\n",
    "\n",
    "$$\n",
    "p({\\bf x}) \\equiv \\sum_{k=1}^K \\pi_k \\mathcal{N}({\\bf x} | {\\bf \\mu_k}, \\Sigma_k) \n",
    "$$\n",
    "\n",
    "の形を仮定する。この式では、データ点${\\bf x}$は確率$\\pi_k$でクラス$C_k$に(暗に)属しており、クラス$C_k$に属するデータはガウス分布$\\mathcal{N}({\\bf x} | {\\bf \\mu_k}, Sigma_k)$に従うとしている。それを陽に表すために、潜在変数${\\bf z}$を導入する。\n",
    "${\\bf z}$は${\\bf x}$がどのクラスに属するかを表すOneHotベクトルである。すなわち、\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "z_k =\n",
    "\\begin{cases}\n",
    "1 \\text{ if } {\\bf x} \\in C_k \\\\\n",
    "0 \\text{ otherwise }\n",
    "\\end{cases}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "を満たすものとする。この${\\bf z}$を用いて\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\pi_k &=& p(z_k = 1) \\\\\n",
    "p({\\bf x} | z_k = 1) &=& \\mathcal{N}({\\bf x} | {\\bf \\mu_k}, \\Sigma_k)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "のように書くことができる。よって\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "p({\\bf z}) &=& \\prod_{k=1}^K \\pi_k^{z_k} \\\\\n",
    "p({\\bf x} | {\\bf z}) &=& \\prod_{k=1}^K \\mathcal{N}({\\bf x} | {\\bf \\mu_k}, \\Sigma_k)^{z_k}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "となることがわかる。確率の乗法定理から\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "p({\\bf x}) &=& \\sum_{\\bf z} p({\\bf x} | {\\bf z}) p({\\bf z}) \\\\\n",
    "&=& \\sum_{\\bf z} \\prod_{k=1}^K \\pi_k^{z_k} \\mathcal{N}({\\bf x} | {\\bf \\mu_k}, \\Sigma_k)^{z_k} \\\\\n",
    "&=& \\sum_{k=1}^K \\pi_k \\mathcal{N}({\\bf x} | {\\bf \\mu_k}, \\Sigma_k)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "が成立する。これは上で仮定したモデルの表式と同じになっていることがわかる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "混合ガウスモデルの学習では、混合係数および各ガウス分布のパラメータを(データ集合に対して)最適化させる。\n",
    "今、与えられたデータ集合を$X \\equiv \\{ {\\bf x_n} \\}_{n=1}^N$とする。データ集合の生成確率(尤度)は\n",
    "\n",
    "$$\n",
    "p(X) = \\prod_{n=1}^N p({\\bf x_n}) = \\prod_{n=1}^N \\sum_{k=1}^K \\pi_k \\mathcal{N}({\\bf x_n} | {\\bf \\mu_k}, \\Sigma_k)\n",
    "$$\n",
    "\n",
    "であるから、負の対数尤度は\n",
    "\n",
    "$$\n",
    "- \\log p(X) = - \\sum_{n=1}^N \\log p({\\bf x_n})\n",
    "$$\n",
    "\n",
    "となる。この問題は制約付き最適化問題である。具体的には\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^K \\pi_k = 1\n",
    "$$\n",
    "\n",
    "という等式制約が課されている。これを解くためにラグランジュの未定乗数法を用いる。\n",
    "パラメータ集合を$\\theta \\equiv \\{ \\pi_1, \\dots, \\pi_K, {\\bf \\mu_1}, \\dots, {\\bf \\mu_K}, \\Sigma_1, \\dots, \\Sigma_K \\}$として、ラグランジュ関数$L$を\n",
    "\n",
    "$$\n",
    "L(\\theta) \\equiv - \\log p(X) + \\lambda (1 - \\sum_{k=1}^K \\pi_k)\n",
    "$$\n",
    "\n",
    "とする。ここで$\\lambda$はラグランジュ乗数である。$L$を各パラメータで偏微分すれば\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial}{\\partial \\lambda} L &=& 1 - \\sum_{k=1}^K \\pi_k \\\\\n",
    "\\frac{\\partial}{\\partial \\pi_k} L &=& \\lambda - \\sum_{n=1}^N \\frac{\\mathcal{N}({\\bf x_n} | {\\bf \\mu_k}, \\Sigma_k)}{\\sum_{j=1}^K \\pi_j \\mathcal{N}({\\bf x_n} | {\\bf \\mu_j}, \\Sigma_j)} \\\\\n",
    "\\frac{\\partial}{\\partial \\mu_k} L\n",
    "&=& \\sum_{n=1}^N \\frac{\\pi_k \\mathcal{N}({\\bf x_n} | {\\bf \\mu_k}, \\Sigma_k)}{\\sum_{k=1}^K \\mathcal{N}({\\bf x_n} | {\\bf \\mu_k}, \\Sigma_k)} \\Sigma_k^{-1} ({\\bf x_n} - {\\bf \\mu_k}) \\\\\n",
    "\\frac{\\partial}{\\partial \\Sigma_k} L\n",
    "&=& \\sum_{n=1}^N \\frac{\\pi_k \\mathcal{N}({\\bf x_n} | {\\bf \\mu_k}, \\Sigma_k)}{\\sum_{k=1}^K \\pi_k \\mathcal{N}({\\bf x_n} | {\\bf \\mu_k}, \\Sigma_k)} (- \\frac{1}{2} \\Sigma_k^{-1} + \\frac{1}{2} \\Sigma_k^{-1} ({\\bf x_n} - {\\bf \\mu_k}) ({\\bf x_n} - {\\bf \\mu_k})^T \\Sigma_k^{-1})\n",
    "\\end{eqnarray*}\n",
    "\n",
    "となる。ここで\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\gamma_{nk} = \\frac{\\pi_k \\mathcal{N}({\\bf x_n} | {\\bf \\mu_k}, \\Sigma_k)}{\\sum_{k=1}^K \\pi_k \\mathcal{N}({\\bf x_n} | {\\bf \\mu_k}, \\Sigma_k)}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "と定義することで下の2式については\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial}{\\partial \\mu_k} L\n",
    "&=& \\sum_{n=1}^N \\gamma_{nk} \\Sigma_k^{-1} ({\\bf x_n} - {\\bf \\mu_k}) \\\\\n",
    "\\frac{\\partial}{\\partial \\Sigma_k} L\n",
    "&=& \\sum_{n=1}^N \\gamma_{nk} (- \\frac{1}{2} \\Sigma_k^{-1} + \\frac{1}{2} \\Sigma_k^{-1} ({\\bf x_n} - {\\bf \\mu_k}) ({\\bf x_n} - {\\bf \\mu_k})^T \\Sigma_k^{-1})\n",
    "\\end{eqnarray*}\n",
    "\n",
    "と書くことができる。これらを0とおくと\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial}{\\partial \\lambda} L = 0\n",
    "&\\Leftrightarrow& \\sum_{k=1}^K \\pi_k = 1 \\\\\n",
    "\\frac{\\partial}{\\partial \\pi_k} L = 0\n",
    "&\\Leftrightarrow& \\lambda = \\sum_{n=1}^N \\frac{\\mathcal{N}({\\bf x_n} | {\\bf \\mu_k}, \\Sigma_k)}{\\sum_{j=1}^K \\pi_j \\mathcal{N}({\\bf x_n} | {\\bf \\mu_j}, \\Sigma_j)} \\\\\n",
    "&\\Leftrightarrow& \\lambda = \\frac{1}{\\pi_k} \\sum_{n=1}^N \\gamma_{nk} \\\\\n",
    "\\frac{\\partial}{\\partial \\mu_k} L = 0\n",
    "&\\Leftrightarrow& \\sum_{n=1}^N \\gamma_{nk} \\Sigma_k^{-1} {\\bf x_n} = \\sum_{n=1}^N \\gamma_{nk} \\Sigma_k^{-1} {\\bf \\mu_k} \\\\\n",
    "&\\Leftrightarrow& \\mu_k = \\frac{\\sum_{n=1}^N \\gamma_{nk} {\\bf x_n}}{\\sum_{n=1}^N \\gamma_{nk}} \\\\\n",
    "\\frac{\\partial}{\\partial \\Sigma_k} L = 0\n",
    "&\\Leftrightarrow& \\Sigma_k \\sum_{n=1}^N \\gamma_{nk} = \\Sigma_k^{-1} ({\\bf x_n} - {\\bf \\mu_k}) ({\\bf x_n} - {\\bf \\mu_k})^T \\Sigma_k^{-1} \\\\\n",
    "&\\Leftrightarrow& \\Sigma_k = \\frac{\\sum_{n=1}^N \\gamma_{nk} ({\\bf x_n} - {\\bf \\mu_k}) ({\\bf x_n} - {\\bf \\mu_k})^T}{\\sum_{n=1}^N \\gamma_{nk}}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "となる。第2式の両辺に$\\pi_j$をかけた式を$j$について和をとれば\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\sum_{j=1}^K \\lambda \\pi_j = \\sum_{n=1}^N \\frac{\\sum_{j=1}^K \\pi_j \\mathcal{N}({\\bf x_n} | {\\bf \\mu_k}, \\Sigma_k)}{\\sum_{j=1}^K \\pi_j \\mathcal{N}({\\bf x_n} | {\\bf \\mu_j}, \\Sigma_j)}\n",
    "&\\Leftrightarrow& \\lambda \\sum_{j=1} \\pi_j = \\sum_{n=1}^N 1 \\\\\n",
    "&\\Leftrightarrow& \\lambda = N\n",
    "\\end{eqnarray*}\n",
    "\n",
    "となることが第1式からわかる。これを再び第2式に代入すれば\n",
    "\n",
    "$$\n",
    "\\pi_k = \\frac{N_k}{N}\n",
    "$$\n",
    "\n",
    "が成立する。ただし、$N_k \\equiv \\sum_{n=1}^N \\gamma_{nk}$とした。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータ$\\pi_k, {\\bf \\mu_k}, \\Sigma_k$は途中で定義した$\\gamma_{nk}$に依存しており、逆に$\\gamma_{nk}$はパラメータを用いて計算される。よって、以下のアルゴリズムを収束するまで繰り返すことによって最適化することができる。\n",
    "\n",
    "- E step  \n",
    "$\\gamma_{nk}$を更新前のパラメータを用いて計算する。\n",
    "\n",
    "$$\n",
    "\\gamma_{nk} = \\frac{\\pi_k \\mathcal{N}({\\bf x_n} | {\\bf \\mu_k}, \\Sigma_k)}{\\sum_{k=1}^K \\pi_k \\mathcal{N}({\\bf x_n} | {\\bf \\mu_k}, \\Sigma_k)}\n",
    "$$\n",
    "\n",
    "- M step  \n",
    "E stepで求めた$\\gamma_{nk}$を利用してパラメータを最適化する。\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\pi_k &=& \\frac{N_k}{N} \\\\\n",
    "\\mu_k &=& \\frac{\\sum_{n=1}^N \\gamma_{nk} {\\bf x_n}}{\\sum_{n=1}^N \\gamma_{nk}} \\\\\n",
    "\\Sigma_k &=& \\frac{\\sum_{n=1}^N \\gamma_{nk} ({\\bf x_n} - {\\bf \\mu_k}) ({\\bf x_n} - {\\bf \\mu_k})^T}{\\sum_{n=1}^N \\gamma_{nk}}\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の$\\gamma_{nk}$は負担率と呼ばれているが、この式の意味について考える。最初に潜在変数${\\bf z}$を導入し、\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\pi_k &=& p(z_k = 1) \\\\\n",
    "p({\\bf x} | z_k = 1) &=& \\mathcal{N}({\\bf x} | {\\bf \\mu_k}, \\Sigma_k)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "としたことを思い出せば、ベイズの定理より\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\gamma_{nk} &=& \\frac{\\pi_k \\mathcal{N}({\\bf x_n} | {\\bf \\mu_k}, \\Sigma_k)}{\\sum_{k=1}^K \\pi_k \\mathcal{N}({\\bf x_n} | {\\bf \\mu_k}, \\Sigma_k)} \\\\\n",
    "&=& \\frac{p(z_k = 1) p({\\bf x_n} | z_k = 1)}{\\sum_{k=1}^K p(z_k = 1) p({\\bf x_n} | z_k = 1)} \\\\\n",
    "&=& p(z_k = 1 | {\\bf x_n})\n",
    "\\end{eqnarray*}\n",
    "\n",
    "と書くことができる。$\\pi_k$が$z_k$の事前確率であったのに対して、$\\gamma_{nk}$はデータ${\\bf x_n}$を観測した後の$z_k$の事後確率であると解釈できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
