{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ロジスティック回帰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「識別モデル」を学習することで二値分類問題を解くのがロジスティック回帰である。「回帰」という名前がついているが、回帰問題ではなく二値分類問題で利用される。\n",
    "\n",
    "識別モデルとは、データを観測したときに、それに対するクラスの事後分布を推定するモデルである。すなわち、入力${\\bf x}$に対してそれが各クラス(二値分類問題ならばクラス$C_0$とクラス$C_1$)に属する確率を出力するようなモデルである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下では2つのクラスを$C_0, C_1$と書くことにする。\n",
    "\n",
    "データ${\\bf x}$を観測した下でのクラス$C_1$に属する(事後)確率を次のようにモデル化する。\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "p(C_1 | {\\bf x_i}) \\equiv \\sigma ({\\bf w}^T {\\bf x_i})\n",
    "\\end{eqnarray*}\n",
    "\n",
    "ここで、$\\sigma(\\dot)$はシグモイド関数で、次のように定義される。\n",
    "\n",
    "$$\n",
    "\\sigma(x) \\equiv \\frac{1}{1 + \\exp(-x)}\n",
    "$$\n",
    "\n",
    "シグモイド関数は$(0, 1)$の値をとり、$x \\rightarrow - \\infty$のときに$\\sigma(x) \\rightarrow 0$、$x \\rightarrow \\infty$のときに$\\sigma(x) \\rightarrow 1$となる。\n",
    "\n",
    "上の識別モデルにおけるパラメータ${\\bf w}$がもっともらしいかどうかは、上のモデルにおいて実現値が生成される確率がどのくらい大きいかによって評価される。パラメータを${\\bf w}$としたときの$y$の(条件付き)生成確率を尤度と呼ぶ。ロジスティック回帰の学習では、この尤度が大きくなるようにパラメータの学習を行う。\n",
    "\n",
    "実際には$\\log$をとって対数尤度を最大化する。さらに、対数尤度に$-1$をかけることで負の対数尤度となるが、これを損失関数として最小化することと対数尤度を最大化することは同値である。\n",
    "\n",
    "そこで負の対数尤度を最小化することを考える。まず、尤度(${\\bf y}$の生成確率)Lは\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "L({\\bf y} | {\\bf w}) = \\Pi_{n=1}^N \\hat{y_n}^{y_n} (1 - \\hat{y_n})^{1 - y_n}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "である。ゆえに負の対数尤度(損失関数)Eは\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "E({\\bf w}) \\equiv - \\log (L({\\bf w}) = - \\Sigma_{n=1}^N y_n \\log \\hat{y_n} + (1 - y_n) \\log(1 - \\hat{y_n})\n",
    "\\end{eqnarray*}\n",
    "\n",
    "となる。\n",
    "損失関数の({\\bf w}に関する)勾配は\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial}{\\partial {\\bf w}} E({\\bf w}) &=&\n",
    "\\end{eqnarray*}\n",
    "\n",
    "のようになる。線形回帰の場合とは異なり解析的に解けないため、勾配法(ここでは最急降下法)を利用する。更新式は\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "{\\bf w_{t+1}} &=& {\\bf w_t} - \\epsilon \\frac{\\partial}{\\partial {\\bf w}} E({\\bf w}) \\\\\n",
    "&=& {\\bf w_t} - \\epsilon \\Sigma_{n=1}^N (\\hat{y_n} - y_n) {\\bf x_n}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "である。更新式を見るとわかるように、パラメータの更新度合いは${\\bf x}$の大きさに依存するので、学習の際にはデータを標準化することが望ましい。\n",
    "\n",
    "ロジスティック回帰でも線形回帰の場合と同様に、正則化項を加えることで過学習を抑制することができる。\n",
    "\n",
    "さて、クラス$C_1$の事後確率を入力の線形和にシグモイド関数を適用したものとしてモデル化したが、その背景を説明する。\n",
    "\n",
    "クラス$C_1$の事後確率は、ベイズの定理より\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "p(C_0 | {\\bf x})\n",
    "&=& \\frac{p({\\bf x} | C_0) p(C_0)}{p({\\bf x} | C_0) p(C_0) + p({\\bf x} | C_1) p(C_1)} \\\\\n",
    "&=& \\frac{1}{1 + \\frac{p({\\bf x} | C_1) p(C_1)}{p({\\bf x} | C_0) p(C_0)}} \\\\\n",
    "&=& \\frac{1}{1 + \\exp \\left( - \\log \\frac{p({\\bf x} | C_0) p(C_0)}{p({\\bf x} | C_1) p(C_1)} \\right) } \\\\\n",
    "&=& \\sigma \\left( \\log \\frac{p({\\bf x} | C_0) p(C_0)}{p({\\bf x} | C_1) p(C_1)} \\right)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "となる。ここで条件付き分布$p({\\bf x} | C_0), p({\\bf x} | C_1)$がガウス分布であり、かつそれらの共分散行列が等しいと仮定する。このとき\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "p({\\bf x} | C_0) &=& \\frac{1}{(2 \\pi)^{\\frac{D}{2}} |\\Sigma|^{\\frac{1}{2}}} \\exp \\left( ({\\bf x} - {\\bf \\mu_0})^T \\Sigma^{-1} ({\\bf x} - {\\bf \\mu_0})) \\right) \\\\\n",
    "p({\\bf x} | C_1) &=& \\frac{1}{(2 \\pi)^{\\frac{D}{2}} |\\Sigma|^{\\frac{1}{2}}} \\exp \\left( ({\\bf x} - {\\bf \\mu_1})^T \\Sigma^{-1} ({\\bf x} - {\\bf \\mu_1})) \\right)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "とすれば\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\frac{p({\\bf x} | C_1)}{p({\\bf x} | C_2)} &=&\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ロジスティック回帰は(対数)オッズと密接に関係している。ある事象が生起する確率を$p$としたときに、\n",
    "\n",
    "$$\n",
    "\\text{Odds} \\equiv \\frac{p}{1 - p}\n",
    "$$\n",
    "\n",
    "を(その事象の)オッズという。確率とオッズは($p \\in [0, 1)$では)1対1に対応づけることができる。ただし、確率が$p \\in [0, 1]$の値をとるのに対し、オッズは$Odds \\in [0, \\infty)$の値をとる。\n",
    "\n",
    "オッズの$\\log$をとったものが対数オッズである。対数オッズは実数値をとり、\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\text{Odds} \\rightarrow - \\infty \\text { as } p \\rightarrow 0 \\\\\n",
    "\\text{Odds} \\rightarrow \\infty \\text{ as } p \\rightarrow 1\n",
    "\\end{eqnarray*}\n",
    "\n",
    "となる。よってクラス$C_0$に属する確率を$p$とし、その対数オッズ$\\frac{p}{1-p}$を線形和${\\bf w}^T {\\bf x}$で表現することが考えられる。クラス$C_0$に属するデータについては線形和を小さくし、クラス$C_1$に属するデータについては線形和を大きくするようにパラメータを学習すれば良い。\n",
    "\n",
    "なお、このモデルが先に述べた式に一致することは\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\log \\frac{p}{1-p} = {\\bf w}^T {\\bf x} &\\Leftrightarrow& \\frac{p}{1-p} \\exp({\\bf w}^T {\\bf x}) \\\\\n",
    "&\\Leftrightarrow& p = \\frac{\\exp({\\bf w}^T {\\bf x})}{1 + \\exp({\\bf w}^T {\\bf x})} \\\\\n",
    "&\\Leftrightarrow& p = \\frac{1}{1 + \\exp(- {\\bf w}^T {\\bf x})} \\\\\n",
    "&\\Leftrightarrow& p = \\sigma({\\bf w}^T {\\bf x})\n",
    "\\end{eqnarray*}\n",
    "\n",
    "という変形から導ける。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
