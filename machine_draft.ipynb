{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 線形回帰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "もともと$y$を近似する関数$f(\\vec{x}) = \\vec{w}^T \\phi(\\vec{x})$のパラメータ$\\vec{w}$を求めることが目標であった。そして二乗和誤差を$|| y_i - f(\\vec{x_i}) ||^2$としてとるが、これを各$i$に対してまとめて求めるため、結局下のような式になる。\n",
    "\\begin{eqnarray*}\n",
    "\\vec{w} = ( \\Phi^T \\Phi)^{-1} \\Phi^T) \\vec{y}\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A \\vec{x} = \\vec{b}$を解くことを考える。ここでは二乗和誤差が最小となる解を求める、すなわち\n",
    "\\begin{eqnarray*}\n",
    "\\vec{x}^{\\ast} = argmin_{\\vec{x}} L(\\vec{x}) \\equiv argmin_{\\vec{x}} || A \\vec{x} - \\vec{b} ||^2\n",
    "\\end{eqnarray*}\n",
    "を満たすような$\\vec{x}^{\\ast}$を求める。  \n",
    "$A$が正則ならば明らかに$\\vec{x}^{\\ast} = A^{-1} \\vec{b}$であり、そのときの誤差は0になる。$A$が正則でないときには、\n",
    "\\begin{eqnarray*}\n",
    "L(\\vec{x}) &=& || A \\vec{x} - \\vec{b} ||^2 \\\\\n",
    "&=& (A \\vec{x} - \\vec{b})^T (A \\vec{x} - \\vec{b}) \\\\\n",
    "&=& \\vec{x}^T A^T A \\vec{x} - \\vec{x}^T A^T \\vec{b} - \\vec{b}^T A \\vec{x} + \\vec{b}^T \\vec{b}\n",
    "\\end{eqnarray*}\n",
    "とし、これを$\\vec{x}$で微分した\n",
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial}{\\partial \\vec{x}} L(\\vec{x})\n",
    "&=& (A^T A + (A^T A)^T) \\vec{x} - A^T \\vec{b} - A^T \\vec{b} \\\\\n",
    "&=& 2 A^T A \\vec{x} - 2 A^T \\vec{b}\n",
    "\\end{eqnarray*}\n",
    "が0になるような$\\vec{x}$を考える。すなわち\n",
    "\\begin{eqnarray*}\n",
    "A^T A \\vec{x} = A^T \\vec{b}\n",
    "\\end{eqnarray*}\n",
    "を満たすような$\\vec{x}$をとれば良い。  \n",
    "これを$\\vec{x}$について解くために、擬似逆行列$A^{\\dagger}$を導入する。以下の4条件\n",
    "\\begin{eqnarray}\n",
    "A A^{\\dagger} A &=& A \\\\\n",
    "A^{\\dagger} A A^{\\dagger} &=& A^{\\dagger} \\\\\n",
    "(A^{\\dagger} A)^T &=& A^{\\dagger} A \\\\\n",
    "(A A^{\\dagger})^T &=& A A^{\\dagger}\n",
    "\\end{eqnarray}\n",
    "を満たす$A^{\\dagger}$を$A$の擬似逆行列と言う。\n",
    "$A \\in M_{mn}$について$rank(A) = m$ならば$A A^T$は正則であって$A^{\\dagger} = A^T (A A^T)^{-1}$が成立する。\n",
    "また、$rank(A) = n$ならば$A^T A$は正則であって$A^{\\dagger} \\equiv (A^T A)^{-1} A^T$が成立する。  \n",
    "最小二乗解を(解析的に)求めるには$rank(A) = n$が必要であるが、もしこれを仮定できれば\n",
    "\\begin{eqnarray*}\n",
    "\\vec{x}^{\\ast} = A^{\\dagger} \\vec{b}\n",
    "\\end{eqnarray*}\n",
    "が成立する。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正則化線形回帰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "線形回帰の問題において解$\\vec{x}^{\\ast}$が大きくなりすぎないように正則化項を加えることを考える。すなわち\n",
    "\\begin{eqnarray*}\n",
    "\\vec{x}^{\\ast} = argmin_{\\vec{x}} L(\\vec{x}) \\equiv argmin_{\\vec{x}} (|| A \\vec{x} - \\vec{b} ||^2 + \\lambda || \\vec{x} || ^2)\n",
    "\\end{eqnarray*}\n",
    "という問題を考える。正則化項がない場合の線形回帰問題と同様に$L(\\vec{x})$の微分をとって\n",
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial}{\\partial \\vec{x}} L(\\vec{x})\n",
    "= 2 A^T A \\vec{x} - 2 A^T \\vec{b} + 2 \\lambda \\vec{x}\n",
    "\\end{eqnarray*}\n",
    "が得られる。これを0とおくと\n",
    "\\begin{eqnarray*}\n",
    "(A^T A + \\lambda I) \\vec{x} = A^T \\vec{b}\n",
    "\\end{eqnarray*}\n",
    "となる。この後も同様にして解くことができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign(x):\n",
    "    return np.where(x>=0, 1, -1)    \n",
    "\n",
    "def step_function(x):\n",
    "    return np.where(x>=0, 1, 0)\n",
    "    \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class Base():\n",
    "    def __init__(self, *func):\n",
    "        self.func = list(func)\n",
    "        self.n = len(self.func)\n",
    "    def apply(self, x):\n",
    "        y = self.func[0](x).reshape(-1,1)\n",
    "        for k in range(1,self.n):\n",
    "            y = np.concatenate((y, self.func[k](x).reshape(-1,1)), axis=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression():\n",
    "    def __init__(self, base, input_dim=1, output_dim=1, lam=0.0):\n",
    "        self.weight = np.zeros((output_dim,))\n",
    "        self.base = base\n",
    "        self.lam = lam\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "    def fit(self, x, y):\n",
    "        phi = self.base.apply(x)\n",
    "        w_dagger = np.linalg.inv(np.dot(phi.T, phi) + self.lam * np.eye(phi.shape[1])).dot(phi.T)\n",
    "        self.weight = w_dagger.dot(y)\n",
    "        return self.weight\n",
    "    def predict(self, x):\n",
    "        phi = self.base.apply(x)\n",
    "        f = np.dot(phi, self.weight)\n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1],\n",
       "       [ 2,  4],\n",
       "       [ 3,  9],\n",
       "       [ 4, 16]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1,2,3,4]).reshape(-1, 1)\n",
    "y = np.array([2,4,6,7])\n",
    "base = Base(lambda x: x, lambda x: x*x)\n",
    "base.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.16774194, 4.07741935, 5.72903226, 7.12258065])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression(base, output_dim=2)\n",
    "lr.fit(x,y)\n",
    "lr.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.2, 3.9, 5.6, 7.3])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(x,y)\n",
    "lr.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# パーセプトロン"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2値分類について考える。各データをクラス$\\pm 1$のいずれかに分類することを考え、\n",
    "\\begin{eqnarray*}\n",
    "y_i = f(\\vec{w}^T \\phi(\\vec{x_i}))\n",
    "\\end{eqnarray*}\n",
    "によってクラスを予測する。ここで$f$は活性化関数であるが、これは引数の値が非負ならば$+1$に割り当て、負ならば$-1$に割り当てる。  \n",
    "誤差は(正解ラベルを$t_i$として)\n",
    "\\begin{eqnarray*}\n",
    "L \\equiv \\Sigma_i - \\vec{w}^T \\phi(\\vec{x_i}) t_i\n",
    "\\end{eqnarray*}\n",
    "と定義する。ただし、和は誤分類したデータすべてについてとる。決定境界が$\\vec{w}^T \\phi(\\vec{x_i}) = 0$にあることを考えると、境界面から離れるに従って誤差が大きくなっていくことがわかる。  この誤差関数に従って、パラメータ$\\vec{w}$を\n",
    "\\begin{eqnarray*}\n",
    "\\vec{w}_{k+1} = \\vec{w}_k + \\eta \\phi(\\vec{x_i}) t_i\n",
    "\\end{eqnarray*}\n",
    "によって更新する。ここで、スケーリングに対して予測結果は不変であることから、$\\eta$を1として良い。すなわち\n",
    "\\begin{eqnarray*}\n",
    "\\vec{w}_{k+1} = \\vec{w}_k + \\phi(\\vec{x_i}) t_i\n",
    "\\end{eqnarray*}\n",
    "とすれば良い。\n",
    "パーセプトロンの収束定理より有限回の繰り返しで厳密解に収束する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron():\n",
    "    def __init__(self, base, input_dim=1, output_dim=1):\n",
    "        self.weight = np.zeros((output_dim,))\n",
    "        self.base = base\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "    def predict(self, x):\n",
    "        phi = self.base.apply(x)\n",
    "        linear_combination = np.dot(phi, self.weight)\n",
    "        return sign(linear_combination)\n",
    "    def fit(self, x, y, iteration=10):\n",
    "        phi = self.base.apply(x)\n",
    "        for _ in range(iteration):\n",
    "            y_pred = self.predict(x)\n",
    "            false = (y != y_pred)\n",
    "            x_false = x[false]\n",
    "            y_false = y[false]\n",
    "            phi_false = phi[false]\n",
    "            self.weight = self.weight + np.dot(y_false, phi_false)\n",
    "        return self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  4],\n",
       "       [ 1,  2,  1],\n",
       "       [ 1, -1,  0],\n",
       "       [ 1, -5,  1]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1,2],[2,1],[-1,0],[-5,-1]])\n",
    "y = np.array([1,1,-1,-1])\n",
    "base = Base(lambda x: np.ones_like(x[:, 0]), lambda x: x[:, 0], lambda x: x[:, 1]*x[:, 1])\n",
    "base.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2]\n",
      " [ 2  1]\n",
      " [-1  0]\n",
      " [-5 -1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1,  1, -1, -1])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Perceptron(base, input_dim=3, output_dim=3)\n",
    "p.fit(x,y)\n",
    "p.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ロジスティック回帰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2値分類問題を生成的なアプローチで考える。生成的なアプローチでは、分布の形を仮定して、そのパラメータを最尤法で決定する。ここではガウス分布\n",
    "\\begin{eqnarray*}\n",
    "p(\\vec{x}) = \\frac{1}{(2 \\pi)^{\\frac{D}{2}} |\\Sigma|^{\\frac{1}{2}}} \\exp \\left( - \\frac{1}{2} (\\vec{x} - \\vec{\\mu})^T \\Sigma^{-1} (\\vec{x} - \\vec{\\mu}) \\right)\n",
    "\\end{eqnarray*}\n",
    "をとることにする。クラスを$C_1, C_2$とし、データ点$\\vec{x}$のクラス$C_1$に対する事後確率はベイズの定理より\n",
    "\\begin{eqnarray*}\n",
    "p(C_1 | \\vec{x})\n",
    "&=& \\frac{p(\\vec{x} | C_1) p(C_1)}{p(\\vec{x} | C_1) p(C_1) + p(\\vec{x} | C_2) p(C_2)} \\\\\n",
    "&=& \\frac{1}{1 + \\frac{p(\\vec{x} | C_2) p(C_2)}{p(\\vec{x} | C_1) p(C_1)}} \\\\\n",
    "&=& \\frac{1}{1 + \\exp \\left( \\log \\frac{p(\\vec{x} | C_2) p(C_2)}{p(\\vec{x} | C_1) p(C_1)} \\right) } \\\\\n",
    "\\end{eqnarray*}\n",
    "となる。シグモイド関数$\\sigma(a) \\equiv \\frac{1}{1 + \\exp(-a)}$を使えば\n",
    "\\begin{eqnarray*}\n",
    "p(C_1 | \\vec{x}) = \\sigma \\left( \\log \\frac{p(\\vec{x} | C_1) p(C_1)}{p(\\vec{x} | C_2) p(C_2)} \\right)\n",
    "\\end{eqnarray*}\n",
    "となる。ここで、クラスの条件付き分布の共分散行列が同じであるという仮定をおくと、$\\log \\frac{p(\\vec{x} | C_1) p(C_1)}{p(\\vec{x} | C_2) p(C_2)}$は$\\vec{x}$に関して線形となる。よって\n",
    "\\begin{eqnarray*}\n",
    "p(C_1 | \\vec{x}) = \\sigma (\\vec{w}^T \\vec{x})\n",
    "\\end{eqnarray*}\n",
    "のように書くことができる。  \n",
    "誤差関数は負の対数尤度とし、これを最小化することを考える。尤度は$y_i = p(C_1 | \\vec{x_i}) = \\sigma (\\vec{w}^T \\vec{x_i})$として\n",
    "\\begin{eqnarray*}\n",
    "L \\equiv p(\\vec{t} | \\vec{w}) = \\Pi_i y_i^{t_i} (1 - y_i)^{1 - t_i}\n",
    "\\end{eqnarray*}\n",
    "であるから、負の対数尤度は\n",
    "\\begin{eqnarray*}\n",
    "E(\\vec{w}) = - \\log L(\\vec{w}) = - \\Sigma_i ( t_i \\log(y_i) + (1 - t_i) \\log(1 - y_i) )\n",
    "\\end{eqnarray*}\n",
    "とかける。$\\vec{w}$で微分すれば\n",
    "\\begin{eqnarray*}\n",
    "\\nabla E = \\Sigma_i (y_i - t_i) \\phi(\\vec{x_i})\n",
    "\\end{eqnarray*}\n",
    "が得られる。(計算過程はまた今度書く。)これに基づいて勾配法を適用する。\n",
    "\n",
    "正則化項を含めると、損失関数は\n",
    "\\begin{eqnarray*}\n",
    "E(\\vec{w}) = - \\Sigma_i ( t_i \\log(y_i) + (1 - t_i) \\log(1 - y_i) ) + \\frac{1}{2} \\lambda || w ||^2\n",
    "\\end{eqnarray*}\n",
    "となる。よって\n",
    "\\begin{eqnarray*}\n",
    "\\nabla E = \\Sigma_i (y_i - t_i) \\phi(\\vec{x_i}) + \\lambda w\n",
    "\\end{eqnarray*}\n",
    "となる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self, base, input_dim=1, output_dim=1, lam=0.0, random_state=0):\n",
    "#         self.weight = np.random.randn(output_dim)\n",
    "        self.weight = np.zeros((output_dim,))\n",
    "        self.base = base\n",
    "        self.lam = lam\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "    def predict(self, x):\n",
    "        phi = self.base.apply(x)\n",
    "        linear_combination = np.dot(phi, self.weight)\n",
    "        return sigmoid(linear_combination)\n",
    "    def fit(self, x, y, iteration=10, learning_rate=0.001, threshold=0.001):\n",
    "        phi = self.base.apply(x)\n",
    "        for _ in range(iteration):\n",
    "            y_pred = self.predict(x)\n",
    "            res = y_pred - y\n",
    "            if (np.all(np.abs(res) < threshold)):\n",
    "                break\n",
    "            self.weight = self.weight - learning_rate * (np.dot(y_pred - y, phi) - self.lam * self.weight)\n",
    "        return self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05188025, 0.08912967, 0.06669013, 0.10111529, 0.04822055,\n",
       "       0.05778963, 0.06967287, 0.06836266, 0.11114234, 0.08805511,\n",
       "       0.04567992, 0.08350144, 0.08588767, 0.06262912, 0.01893462,\n",
       "       0.02362814, 0.03128403, 0.05536201, 0.05524711, 0.04780443,\n",
       "       0.08290521, 0.05636013, 0.02897625, 0.11831485, 0.12846346,\n",
       "       0.11592324, 0.08993026, 0.05878908, 0.05580142, 0.10362345,\n",
       "       0.11099758, 0.06998198, 0.03011787, 0.02291222, 0.09372463,\n",
       "       0.05299383, 0.04000122, 0.04639079, 0.08750222, 0.06659893,\n",
       "       0.04883503, 0.17240587, 0.07211862, 0.09260347, 0.09266061,\n",
       "       0.09729535, 0.05215725, 0.07941737, 0.0469175 , 0.06492203,\n",
       "       0.95223557, 0.94827096, 0.97111023, 0.95957833, 0.96956023,\n",
       "       0.96731788, 0.96161591, 0.87009952, 0.96048504, 0.93930904,\n",
       "       0.93179988, 0.94146187, 0.94912223, 0.97234496, 0.8661622 ,\n",
       "       0.93706037, 0.96587248, 0.93197031, 0.98227735, 0.93285518,\n",
       "       0.97670067, 0.92228279, 0.98678814, 0.97146485, 0.94079587,\n",
       "       0.94446663, 0.97414589, 0.9815429 , 0.96563822, 0.85671315,\n",
       "       0.93112509, 0.91490921, 0.91937634, 0.98981615, 0.96767285,\n",
       "       0.94681381, 0.96268351, 0.97299444, 0.92852265, 0.950596  ,\n",
       "       0.96847167, 0.96423986, 0.9369853 , 0.87855793, 0.9543349 ,\n",
       "       0.93263644, 0.94276118, 0.94384223, 0.79055862, 0.93970148])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: スケールが大きいと更新が早い(どうやって実装する？)\n",
    "_x = x[:100]\n",
    "_y = y[:100]\n",
    "base = Base(lambda x: np.ones_like(x[:, 0]), lambda x: x[:, 0], lambda x: x[:, 1], lambda x: x[:, 2], lambda x: x[:, 3])\n",
    "lr = LogisticRegression(base, input_dim=4, output_dim=5)\n",
    "lr.fit(_x, _y,iteration=100,learning_rate=0.001)\n",
    "lr.predict(_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Noahz-ark/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9837306 , 0.0162694 ],\n",
       "       [0.96407227, 0.03592773],\n",
       "       [0.97647105, 0.02352895],\n",
       "       [0.95654126, 0.04345874],\n",
       "       [0.98534488, 0.01465512],\n",
       "       [0.98086592, 0.01913408],\n",
       "       [0.97477662, 0.02522338],\n",
       "       [0.9756756 , 0.0243244 ],\n",
       "       [0.94988578, 0.05011422],\n",
       "       [0.96484713, 0.03515287],\n",
       "       [0.98650419, 0.01349581],\n",
       "       [0.96730747, 0.03269253],\n",
       "       [0.9660858 , 0.0339142 ],\n",
       "       [0.97853302, 0.02146698],\n",
       "       [0.9961785 , 0.0038215 ],\n",
       "       [0.99471294, 0.00528706],\n",
       "       [0.9920933 , 0.0079067 ],\n",
       "       [0.98205008, 0.01794992],\n",
       "       [0.98221287, 0.01778713],\n",
       "       [0.9854774 , 0.0145226 ],\n",
       "       [0.96786256, 0.03213744],\n",
       "       [0.98149485, 0.01850515],\n",
       "       [0.99291282, 0.00708718],\n",
       "       [0.94469065, 0.05530935],\n",
       "       [0.93796494, 0.06203506],\n",
       "       [0.94696615, 0.05303385],\n",
       "       [0.96330912, 0.03669088],\n",
       "       [0.98051681, 0.01948319],\n",
       "       [0.98194177, 0.01805823],\n",
       "       [0.95497024, 0.04502976],\n",
       "       [0.95017934, 0.04982066],\n",
       "       [0.97473194, 0.02526806],\n",
       "       [0.9925852 , 0.0074148 ],\n",
       "       [0.99497222, 0.00502778],\n",
       "       [0.96129283, 0.03870717],\n",
       "       [0.98321334, 0.01678666],\n",
       "       [0.98886915, 0.01113085],\n",
       "       [0.98618329, 0.01381671],\n",
       "       [0.96484517, 0.03515483],\n",
       "       [0.97661271, 0.02338729],\n",
       "       [0.98501482, 0.01498518],\n",
       "       [0.90271509, 0.09728491],\n",
       "       [0.97353141, 0.02646859],\n",
       "       [0.96136555, 0.03863445],\n",
       "       [0.96166372, 0.03833628],\n",
       "       [0.95888508, 0.04111492],\n",
       "       [0.98359525, 0.01640475],\n",
       "       [0.96957758, 0.03042242],\n",
       "       [0.98595773, 0.01404227],\n",
       "       [0.97744649, 0.02255351],\n",
       "       [0.01540679, 0.98459321],\n",
       "       [0.01710145, 0.98289855],\n",
       "       [0.0074532 , 0.9925468 ],\n",
       "       [0.01198469, 0.98801531],\n",
       "       [0.00799896, 0.99200104],\n",
       "       [0.00884878, 0.99115122],\n",
       "       [0.01106559, 0.98893441],\n",
       "       [0.06656811, 0.93343189],\n",
       "       [0.0117305 , 0.9882695 ],\n",
       "       [0.02138387, 0.97861613],\n",
       "       [0.02574336, 0.97425664],\n",
       "       [0.02035252, 0.97964748],\n",
       "       [0.0170035 , 0.9829965 ],\n",
       "       [0.00697243, 0.99302757],\n",
       "       [0.0692427 , 0.9307573 ],\n",
       "       [0.02290591, 0.97709409],\n",
       "       [0.00932131, 0.99067869],\n",
       "       [0.02584251, 0.97415749],\n",
       "       [0.00370018, 0.99629982],\n",
       "       [0.02520338, 0.97479662],\n",
       "       [0.00535708, 0.99464292],\n",
       "       [0.03110856, 0.96889144],\n",
       "       [0.00244463, 0.99755537],\n",
       "       [0.00735378, 0.99264622],\n",
       "       [0.02098167, 0.97901833],\n",
       "       [0.01908059, 0.98091941],\n",
       "       [0.00638605, 0.99361395],\n",
       "       [0.00390217, 0.99609783],\n",
       "       [0.00945716, 0.99054284],\n",
       "       [0.07773918, 0.92226082],\n",
       "       [0.0261325 , 0.9738675 ],\n",
       "       [0.03576723, 0.96423277],\n",
       "       [0.03286278, 0.96713722],\n",
       "       [0.00168052, 0.99831948],\n",
       "       [0.0086066 , 0.9913934 ],\n",
       "       [0.0176437 , 0.9823563 ],\n",
       "       [0.01072331, 0.98927669],\n",
       "       [0.00679175, 0.99320825],\n",
       "       [0.02736015, 0.97263985],\n",
       "       [0.01599582, 0.98400418],\n",
       "       [0.00842532, 0.99157468],\n",
       "       [0.01006449, 0.98993551],\n",
       "       [0.02292875, 0.97707125],\n",
       "       [0.06026888, 0.93973112],\n",
       "       [0.01428918, 0.98571082],\n",
       "       [0.02522293, 0.97477707],\n",
       "       [0.01982383, 0.98017617],\n",
       "       [0.01939041, 0.98060959],\n",
       "       [0.13676568, 0.86323432],\n",
       "       [0.02138508, 0.97861492]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 比較実験\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(_x,_y)\n",
    "lr.predict_proba(_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# サポートベクターマシン"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2値分類問題を考える。具体的には、\n",
    "\\begin{eqnarray*}\n",
    "y(x) = w^T \\phi(x) + b\n",
    "\\end{eqnarray*}\n",
    "の正負によってクラスを分類する。ここで分類境界は\n",
    "\\begin{eqnarray*}\n",
    "y(x) = w^T x + b = 0\n",
    "\\end{eqnarray*}\n",
    "を満たす超平面である。データ集合$\\{ (x_n,t_n) \\}_n$が線形分離できているとすると、各データ点から分類境界までの距離(マージン)は\n",
    "\\begin{eqnarray*}\n",
    "\\frac{|y(x)|}{||w||} = \\frac{t_n y_n}{||w||} = \\frac{t_n w^T \\phi(x_n)}{||w||}\n",
    "\\end{eqnarray*}\n",
    "とかける。ここで正しく分類できているデータ点に関して$t_n y_n > 0$が成り立つことを利用している。\n",
    "分類境界に最も近いデータ点を$(x,t)$とする。スケーリングしても予測が不変であるから、適当にスケーリングすることで\n",
    "\\begin{eqnarray*}\n",
    "t (w^T \\phi(x) + b) = 1\n",
    "\\end{eqnarray*}\n",
    "となるようにできる。それ以外の点については\n",
    "\\begin{eqnarray*}\n",
    "t_n (w^T \\phi(x_n) + b) \\ge 1\n",
    "\\end{eqnarray*}\n",
    "が成り立つ。この条件のもとでマージンを最大化したい。すなわち\n",
    "\\begin{eqnarray*}\n",
    "argmax_{w,b} \\frac{1}{||w||} s.t. t_n (w^T \\phi(x_n) + b) \\ge 1\n",
    "\\end{eqnarray*}\n",
    "なる最適化問題を解きたい。これは\n",
    "\\begin{eqnarray*}\n",
    "argmin_{w,b} \\frac{1}{2}||w||^2 s.t. t_n (w^T \\phi(x_n) + b) \\ge 1\n",
    "\\end{eqnarray*}\n",
    "を解くことと同値である。ここで最小化したい値$\\frac{1}{2}||w||^2$のことをハードマージンという。\n",
    "\n",
    "さて、線形分離不能なデータ集合に対しては、ペナルティを導入し、ソフトマージンを最小化することを考える。\n",
    "\\begin{eqnarray*}\n",
    "argmin_{w,b,\\xi} \\frac{1}{2} ||w||^2 + C \\Sigma_n \\xi_n\n",
    "s.t. t_n y(x_n) \\ge 1 - \\xi_n, \\xi \\ge 0\n",
    "\\end{eqnarray*}\n",
    "ここで$\\xi$はスラック変数であり、正しく分類されているデータ$(x_n, t_n)$に対しては$\\xi_n = 0$となり、マージン内部にある、または誤分類しているときに限り正の値をとる。$C$は正則化パラメータである。\n",
    "目的関数を条件を利用して書き直せば\n",
    "\\begin{eqnarray*}\n",
    "argmin_{w,b,\\xi} \\frac{1}{2} ||w||^2 + C \\Sigma_n \\max(0, 1 - t_n y(x_n))\n",
    "\\end{eqnarray*}\n",
    "とできる。これは制約なし最適化問題になっている。$\\max(f(x), 0)$関数については$x=0$において微分が定義されないが、劣微分を用いることで解決できる。具体的には\n",
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial}{\\partial x} \\max(f(x), 0) = \n",
    "\\begin{cases}\n",
    "0 & \\text{if } x \\le 0\\\\\n",
    "\\frac{\\partial}{\\partial x} f(x) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{eqnarray*}\n",
    "である。\n",
    "(ロジスティック回帰との関係はまた今度書く)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray*}\n",
    "y(x) &=& w^T \\phi(x) + b \\\\\n",
    "\\frac{\\partial}{\\partial w} \\frac{1}{2} w^T w &=& w \\\\\n",
    "\\frac{\\partial}{\\partial w} y(x) &=& \\phi(x) \\\\\n",
    "\\frac{\\partial}{\\partial w} \\max(0, 1 - t_n y_n) &=& \n",
    "\\begin{cases}\n",
    "0 & \\text{if } x \\le 0 \\\\\n",
    "- t_n \\phi(x_n) & \\text{otherwise} \\\\\n",
    "\\end{cases} \\\\\n",
    "\\frac{\\partial}{\\partial w} \\frac{1}{2} ||w||^2 + C \\Sigma_n \\max(0, 1 - t_n y(x_n)) &=& w - C \\Sigma_n t_n \\phi(x_n) \\\\\n",
    "\\frac{\\partial}{\\partial b} \\frac{1}{2} ||w||^2 + C \\Sigma_n \\max(0, 1 - t_n y(x_n)) &=& - C \\Sigma_n t_n\n",
    "\\end{eqnarray*}\n",
    "となる。ただし、和は$1 - t_n y(x_n) > 0$なるデータについてとる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupportVectorMachine():\n",
    "    def __init__(self, base, input_dim, output_dim, C=1.0):\n",
    "        self.weight = np.zeros((output_dim),)\n",
    "        self.bias = 0.0\n",
    "        self.base = base\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.C = C\n",
    "    def predict(self, x):\n",
    "        phi = self.base.apply(x)\n",
    "        linear_combination = np.dot(phi, self.weight) + self.bias\n",
    "        return sign(linear_combination)\n",
    "    def fit(self, x, y, iteration=10, learning_rate=0.001):\n",
    "        phi = self.base.apply(x)\n",
    "        for _ in range(iteration):\n",
    "            y_pred = self.predict(x)\n",
    "            false = (y * y_pred) < 1\n",
    "            if (not np.any(false)):\n",
    "                break\n",
    "            x_false = x[false]\n",
    "            y_false = y[false]\n",
    "            phi_false = phi[false]\n",
    "            self.weight = self.weight - learning_rate * (self.weight - self.C * np.dot(y_false, phi_false))\n",
    "            self.bias = self.bias + learning_rate * (self.C * np.sum(y_false))\n",
    "        return self.weight, self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_x = x[:100]\n",
    "_y = np.where(y[:100] > 0, y[:100], -1)\n",
    "base = Base(lambda x: np.ones_like(x[:, 0]), lambda x: x[:, 0], lambda x: x[:, 1], lambda x: x[:, 2], lambda x: x[:, 3])\n",
    "svm = SupportVectorMachine(base, input_dim=4, output_dim=5)\n",
    "svm.fit(_x, _y,iteration=10,learning_rate=0.001)\n",
    "svm.predict(_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K近傍法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習用データが与えられた際、それらをすべて記憶しておく。テストデータが来た際に、その点から(ある距離指標に関して)最も近い$k$点を選び、それらのクラスラベルの多数決によってテストデータのクラスラベルを決定する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighbor():\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.x = np.array([])\n",
    "        self.y = np.array([], dtype=np.int64)\n",
    "        self.classes = np.unique(y)\n",
    "    def fit(self, x, y):\n",
    "        self.x = self.x.reshape(-1, x.shape[1])\n",
    "        self.x = np.concatenate([self.x, x], axis=0)\n",
    "        self.y = np.concatenate([self.y, y])\n",
    "    def predict(self, x):\n",
    "        distance = np.array([np.linalg.norm(self.x - _x, axis=1) for _x in x])\n",
    "        index = np.argsort(distance, axis=1)\n",
    "        k_nearest = np.array([np.argmax(np.bincount(self.y[_index < self.k])) for _index in index])\n",
    "        return k_nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3dd5xU1dnA8d+502cLu7BLWUCWjoAIuoBKQsQeNKIRjRpjDSTGbmJs8bViSYw1llgwkqioWEBjjYoIKh0LTYqUpS5t2T47M+f942ybndk+u7Mz+3z97IedM7c8d4Vn7p77nHOU1hohhBDxz4p1AEIIIaJDEroQQiQISehCCJEgJKELIUSCkIQuhBAJwh6rE2dkZOjs7OxYnV4IIeLS0qVL92itMyO9F7OEnp2dzZIlS2J1eiGEiEtKqc11vSddLkIIkSAkoQshRIKQhC6EEAlCEroQQiQISehCCJEgJKELIUSCkIQuhBAJImZ16EKIjqusbCdFRd/idvfF6x3YpH39QT9f535NUAc5qtdROG1OtNYs3bGU/NJ8xvYaS7IzuZUib98aTOhKqd7ADKA7EASe0Vo/WmubY4HZwI8VTW9qre+KbqhCiHindZAffriCnTtfwLLcaO0jNfUohg9/G7s9tcH952+Zz5kzz6QsUIZSCktZPHjig0z7Yhp5xXlYysIf9PPIKY8w5YgpbXBF7YtqaIELpVQPoIfWeplSKgVYCpyhtV5VY5tjgT9prU9r7IlzcnK0jBQVomPJzX2cjRtvIhgsrmpTyklGxiSGDXut3n3zS/Pp/XBvCnwFIe0KBYCmOpd5HV4+v/hzcrJyohh9+6CUWqq1jnhhDfaha613aK2XVXxfAKwGekY3RCFER5Cb+0hIMgfQ2seePbMJBIrr2Mt4c/WbBHUwrF1X/FdTqb+UJxc/2fKA40yTHooqpbKBUcDCCG8frZT6Rin1vlJqWB37T1VKLVFKLcnLy2tysEKI+Ob3H6zjHUUgUFTvvvtL91MeLG/UeYI6yK6iXU2MLv41OqErpZKBN4Brtda1/68sA/porQ8HHgfejnQMrfUzWuscrXVOZmbEycKEEAmsc+cTiZR2XK4sHI6Mevc9ru9x2K3G1XEkOZI4c8iZzQkxrjUqoSulHJhk/pLW+s3a72utD2qtCyu+fw9wKKXq/78jhOhw+va9F7s9DaVcFS02LMvL4MHPopSqd9+R3UcyeehkkhxJVW1JjiSGdBmC1+GtavM6vAzoPIALRlzQGpfQrjWmykUBzwOrtdYP1bFNd2CX1lorpcZgPij2RjVSIUTc83iyGT16Jdu2PU5+/hd4vYPp1es6kpKGNmr/f036F6cPOp3nlz+PP+jnosMv4tzh5/L55s95YvET7CvZx+RDJ3PpqEtx292tfDXtT2OqXH4CfAF8hylbBLgFOARAa/20UupK4HLAD5QA12utv6zvuFLlIkRslfpL+X7392R6M+mT1qfVzrPjwGo2711E/67jyUzp22rn6Sjqq3Jp8A5daz0fqPd3Ia31P4B/NC88IURbe3bZs1z/4fVYysIX8JGTlcOb57xJZlL0nm2VlRcz49MR9HFsoDwI+Ztha2AElxy/FFsj+8JF08jQfyE6mM83fc61H1xLoa+Qg2UHKfWXsjB3IZNmTorqef4zbzy97RtwWpBkB5cFvaxveXl+dM8jqklCF6KDeeirhyguD635Lg+Ws2LnCtbvWx+VcwSCfnqyFLcttN1tg+TSD6JyDhFOEroQHcy2gm0R2x02B7uLdkflHOWBUpx1ZJcke/jgIBEdktCF6GBOGXAKLpsrrL08UM6IbiOicg63I5ldZc6I7233dYrKOUQ4SehCdDDXHnUtnT2dcdqqE67X4WXacdOiOkthjz4PUBqAQMUNuT8IJQEYNviJqJ1DhJJHzUJ0MBneDL75/Tc8+OWDvL/+fbond+f6o6/nlAGnRPU8xx56LUs8PVix9ga8ejdFqifHDP8Hw3r+PKrnEdUarENvLVKHLkRsaR2gtHQTdns6DkfnijZNWdkWLMuN09mtatvtBdsJBAP07tS7qs3nyyMQKMTtzm5wlGdtZf4yth7cSrekbqS4UuqJMXI8keQV5VHoK6RPWh8s1T47Hw6UHmBv8V4O6XQIDpujWcdoUR26ECLx7N49i3XrLicQKEFrP507n0zPnn/ghx9+j8+3C62DpKQcibv7NM6fcy1r964FIDstm5cnPYnaczf5+V+ilA27PZ0hQ6bTufNJjTr3w189zO1zb0ej8Qf9XDjiQh6f+HhIFxBAfv4CVq++oEY8RzB06Ku43b1DtttdtJvz3jiPBVsWYLNspLnTeGHSC5zUv3HxtIXi8mIum30Zb615C7tlx2Fz8NBJD3HJqEuieh65Qxeigzl4cCErVhxXaxpbB2YgeKBGm41dpZrzFwapWZcyPceib5INqJ750LK85OQsw+sdXO+5Z34/k8vmXBZSNumxe7h01KX8Y2L12MTS0lwWLRpCMFhzBkYbbvchjB27HlVxB661ZtQ/R7Eqb1XITIxeh5dlU5cxOKP+eNrKOa+fwzs/vEOpv7Sqzevw8vav3ubE/ic26Vgtmg9dCJFYtmx5kGCwpFZrOaHJHCBAij3IyLTqlkHJ0N0dpGYyBwgGfWzb1vBg8Xvm3RNWA1/iL2H68ukhyW7HjufR2h8WT3n5Hvbv/7SqZdmOZazftz5sWl2f38c/FrWPwet7i/cyZ+2ckOsDc9d+7/x7o3ouSehCdDClpRuBxv1mroDMGhWOmS4IRNzVT0nJhgaPt7NwZ8T2oA5ysKx6Vu7S0o1oXRa2ndZByspyq17nHszFZtnCtvNrP+v3R2eQVEvtKtoV1p1UaUv+lqieSxK6EB1MWtoElIqcYGqzFKyuseLb2gIiDhiyLA9pacc1eLwxPcdEbO/k7kSGt3rG7bS0n2FZSRG2DJKaOrbq1ZFZR1LmD0/8HruH4/se32A8baFfer+wFZUAbMrG+EPGR/VcktCF6GB69/4jNlsKNWsilPJit3cOSfSW5WVdaSZ5Pk9VW2HQzZKDGViWt8a+Duz2zmRlNbwo833H30eSI6lqHVAwfckPn/xwSGVK167n4XL1qDFvuomnS5fTSEo6tKqtV2ovLhl5Sch86A7LQWdPZ357xG8b8dNofW67m2nHTQuJ0aZsJDmT+Mv4v0T1XPJQVIgOqLR0K5s23cn+/R/hcGTSu/cNpKefxNat95OX9zqW5SEr63Iyul3Gowsf5/nlzxPQAX4z4jf86eg/cnDvq2zb9hh+fz4ZGWfQp8+tOJ1dG3Xu73d/zx1z72DhtoX0TevLbeNvi/hgsLx8P1u23BcST8+ef0Cp0C6WoA4yffl0Hlv4GPll+Zwx+AxuHX8rXZMaF09bmb1mNvfOv5ftBdv5WZ+fccexdzCg84AmH6e+h6KS0IUQrcof9JNfmk+aOy1if3clrTX7S/eT5EjCZQ+fmqA9K/QVorWut6Y+WqTKRQjR5rTWTJs3jc4PdKbnQz3p+mBXnlgUedj/xxs+pv9j/enxYA/S7k/jkrcvCauGaY+25m/luBePo/MDneny1y4c/fzR/LD3h5jFI3foQohW8bcFf+OOz+8IScxeh5enTn2KCw+/sKrtm53fcMz0Y0K2c9vdnNTvJGafN7tNY24KX8BH/8f6s6NgBwFtSj4Vii7eLvx4zY9RnRenJrlDF0K0Ka01982/L+wuu7i8mDvm3hHS9tcFfw2r0S71l/LRxo/IPZhLe/XuD++SX5pflcwBNJqS8hJmfj8zJjFJQhdCRJ0v4CO/LD/ie9sLtoe8XrN3DUEdPke6y+Zi84HNrRJfNGzcvzHsgwigqLyIDfsarslvDZLQhRBR57Q56ZnSM+J7QzOHhrw+ptcxOKzwiarKAmUMyRjSKvFFw6juoyI+vE12JnNk1pExiEgSuhCiFSilePCkB/HavSHtHruHv534t5C2G8bdgNfhDatNn3rkVLp4u7RJvM0xoe8EhmQMCVkspPKDbNLg2KybKgldCNEqzhl2Dq+d/Roju48k1ZXKUb2O4r1fv8fx/UJHcB7S6RAWTVnEpMGTSHOn0S+tH3894a88cvIjMYq8cSxlMfeiuVw15iq6JnUlw5vBlCOm8NVlXzV7atyWkioXIUSzaK3R6JARnpHawAz+aUxbJEEdRKGaPOd6a4l1PFLlIoSImuLyYn7/7u9JujcJx90Oxj0/juU7lnP7Z7eT/kA6trtsDHtyGJ9s/ISnlzxNjwd7YLvLRp+H+/Dydy8ze81sBjw2ANtdNjL/msmDXz5IpBvLH/b+wHEvHofjbgeeaR4ufOtC8ksjP2htC+0tnkjkDl0I0SQnzjiR+Vvnh1R4OCwHNssW2qYc2GyhbS6bC601vqCvqs3r8HLzT24OmddkX8k+Bjw2gAOlB6omtnLanIzoOoJFUxa1+d3xvpJ9DHx8IPtL9ofEc1jXw1g8ZXGbxiN36EKIqFidt5oFWxeEleuVB8vD23R4W1mgLCSZg7nj/+uCv1IeqJ7TvHJ+9JqzFPoCPlbvWc3XuV9H63Ia7YXlL1BSXhIWz9q9a2MST10koQshGm3t3rWt8sCvPFjOvpJ9Va+/2fUNJf7ai3BUx9DW6opHa82aPWvaPJ66SEIXQjTa0MyhIXfS0eK0OUNKFHN65IRMN1vTsMxhUT9/Q47scWTd8XRt+3jqIgldCNFog7oM4ri+x+GxV8+RrlC4bK6QNjD96m67O6TNZXPhtEIX1/A6vNw2/jbsVvX87BeNvIgkR1JIFYzL5mJk95HkZEXsPm5VdcVzePfDGZ01us3jqYskdCFEk8w6ZxZXjLmCNFcaTsvJif1PZNnvlnHnhDvpltQNu2VndNZo5l48l2d+8QzZadnYLTuDuwzmtbNf493z32VE1xHYLTu9Unvx0EkP8cej/xhyjjR3GoumLOLUgafisrlIcaZw2ajL+PCCD2NSLtje4qmLVLkIIUQcaVGVi1Kqt1LqM6XUaqXUSqXUNRG2UUqpx5RS65VS3yqljohG4EKIpnl95esM+ccQPNM8HP7U4by/7n3mbprL6GdG45nmof9j/XlhxQt8u+tbTphxAt5pXnr8vQcPzH+A4uJNrFx5NvPmJTN/fhfWr/8jgUD7n5O8pWr+fPo92o8XVrzQ6H3zS/P5/bu/J+3+NFLvS+Wity4iryivFaOtX4N36EqpHkAPrfUypVQKsBQ4Q2u9qsY2E4GrgInAWOBRrfXYiAesIHfoQkTXjBUzuPy9y0OmrK2cZ6QsUL2QssfuIRAMhJQPZrg9/Hs0uC0fUDG3t3KTmnoUo0Z91jYXEAMLtizgpP+cFDZn+7TjpnHtUdfWu29QBxn1z1Gs2bMGX8D8LB2Wg16pvVh9xepWW3WpRXfoWusdWutlFd8XAKuB2tOoTQJmaONrIK3ig0AI0Qa01tz0yU1h84+XBcpCkjlAib8krBZ8fOcStC6hMpmbY5ZSULCIgoKlrRZ3rN38yc0R52y/c+6d+IP+evf9eMPH/Lj/x6pkDqb8Mq84j7fWvNUq8TakSQ9FlVLZwChgYa23egJba7zOJTzpCyFaiS/gY1fRrmbvPzgFPBGX+7QoLPyu2cdt71bmrYzYXhooZU/xnnr3/W73dxHnQy/0FbJi54qoxNdUjU7oSqlk4A3gWq31wdpvR9glrC9HKTVVKbVEKbUkLy92/UxCJBqnzUm6O73Z+28sgrJApHc0Xu/AZh+3veuX3i9iu92y09nTud59B3YeGFaWCZDkSIrZPO6NSuhKKQcmmb+ktX4zwia5QO8ar3sB22tvpLV+Rmudo7XOyczMbE68QogIlFLc9rPbwga/uGyusMUj3HZ3SM03wNw9blAOat6bKeXE4xlIauoxrRZ3rN0z4Z6wn5nX4eW6o67DaXPWsZdx6qBTyfBmYFfVP0tLWSQ5kzhn2DmtEm9DGlPlooDngdVa64fq2GwOcGFFtctRQL7WekcU4xRCNODqMVdz7/H30sXTBZuy0T25O/887Z+8fNbLHJJ6CJaySHOlcdv42/j4Nx8zLHMYlrLw2D2cM2IKY3IWVSRvC6UcZGScyciRn7arOutoO3nAybx4xotVP59Ork7c8tNbuOPYOxrc127Z+fKyLzll4CnYLTs2ZWNC9gQW/nZhnaNKW1tjqlx+AnwBfAdULvx3C3AIgNb66Yqk/w/gFKAYuERrXW8Ji1S5CNE6tNaUBcpw2VxVyThSG0CZvwyHzREyAjIY9KGUDaUidqonpLp+Po3lD/rRWrfJwhb1VbnYIzXWpLWeT+Q+8prbaOCK5oUnhIgmpVRI325BwVI2bryFgoJluN2HkJ19O3vLk/nim4vJtOVSErSjUs/j7LEvYFkWVo2h+cXlxUybN40Z384gEAxw/mHn838/+z9SXamxuLSIAsEAjy96nMcXPU6hr5CJAyZyz3H30DO18XUZtX9mTVW7CytWZKSoEAns4MElrFjxM4LB6tI8pdyU+EtxKLBV3JiXBmAbo7ns+EVV22mtGTd9HMt3Lq+q5nDZXAzoPIAVv1/RbpLYxW9fzOurXq8qP7QrO529nVl9xeoGH2zGI5kPXYgO6scfbw5J5mDqy11WdTIHcNugF4vJK/ixqm3uprlhpXllgTI252/mnbXvtHrsjbH5wGZe/f7VkFpyv/ZTUFbAM0ufiWFksSEJXYgEVtegoEjdxH4Na3Z8UPV6yfYllPnLwrYr9BWyaNuisPZYWL5zOU57eDVKib+Ezzd9HoOIYksSuhAJzOXqHbE9Uk+rXUGv9OppmLLTsuuss66rfrutZadlRxzR6bAcDM4YHIOIYksSuhAJLDv7diyrVgmdchKoldDLApBbnknfzOopmE4ffDpJztA5wBUKp83JucPPbc2wG21k95EMyxwWVmvvtDm5csyVMYoqdiShC5HAMjN/Sf/+D2O3d8ayPFiWl149r8Tf5Tb2+mz4guALwtbyPkz+2bch+7rsLr689EuO6nUUTpsTp83JqB6jWHDpAlJcKTG6onAfXPABpww4BafNicvmon96f9779XsM6Dwg1qG1OalyEaID0DqAz7cbuz0dm810owSDQbbnf0+quzupnq717r+/ZD9BHQxZJq69KSgroLi8mK5JXRN6MJRUuQiRgO589td0+osdx+2KQbd2Yv7C13lu2XN0/VtXHHc76PlQT2atmgWAUjZcrh5VybysbDvr119N7tpfsn71JPLyIs3oUS3dk95gMr9z7p2k3Z+G424Hgx4fxPwt8/k692tOfflU+j3aj8mvTebbXd9G3Hd7wXaufO9KBjw2gKOfP5o3V0eOxxfw8fBXDzP8yeEc+sSh3D//fkrKzeLNKa4UuiV3qzeZL8xdyGkvn1YVzzc7v6n3muKN3KELEYfOvfdIXvUtqx7yV/nPOEIue+4Xz3HZEZdVvfb5drF48WH4/QfQ2iz4bFlJHHLIzWRn39qseM6bdR4zV84Ma3fZXFXT9yoUHoeHTy/8lLG9qvvqdxXu4rCnDuNA6QHKgyaeJEcSN/3kJv4y/i9V22mtOeU/pzB/6/yqMkWP3cOIbiNYcOkCbFb9I1s/2vARZ756ZtW+dcXT3skduhAJ5GB+Xmgyh3rHcl/34XUhr7dufQi//2BVMgcIBovYsmUafn/tiVQbEU/pwYjJHEIX1tBoisuLuf6j60O2efjrh8kvy69K5gBF5UXc+8W9HCyrjuer3K9YsHVBSM15ib+ElXkr+WD9BzTkqvevCtm3Mp7aP594JgldiDjz2cLIybOupF7gKwh5vX//J2gdXl+ulIOioqbPfT5389wmbb90e2ht/P82/i9kkYhKTpuT73ZVx/PV1q9Ckn6lQl8hC7YuqPecvoCPdXvXRXxv2Y5ljQk7LkhCFyLODMpu2pK9tlqTbLndhxAp+2tdjtOZ1eR4BnZu2nzpGd6MkNd90vqgIsTjC/jokVK98FlWSlbVkno1ee1eeqX2qvecDstBsjO5UfHEM0noQsSZQweNo3u5K3wJmToeh00aPCnkde/ef8KyPCFtSjlJSRmDx9O36fFkHkr35O4R36tdH+51ePnzuD+HtP3p6D/hcYTG47ScjOk5JmQA0xlDzsBpc4Ylf7tl57zh59Ubo1KKK8dcGXHu8xvG3VDvvvFEEroQcWj5VSvpUZnUK75+5TqSYZnDQrY7ptcxvH726yFtnTodw+DBz1SUMCajlIu0tAkMH978dTCX/245PZJDlxE+d9i5TDliCm67mxRnCh67h2vHXstVY64K2e7o3kfzzC+eId2dTrIzGZfNxYS+E3jrV6HxeBwe5l0yjyEZQ/DYPXgdXvql9+OTiz4h3dPwak13TbiLiw+/OCSea8Zew9Vjrm72dbc3UuUiRBxbu+5r1m1ZzvicyaR2MquAbcnfwoqdKziq51F0Ta67vjwYLKekZAMORzpOZ7foxLNnLev2rmN8n/Gkus0UuwfLDpJ7MJdDOh1SZ7cHQHmgnA37N5DuTqdbcv3xbDqwiUAwQL/0fk2uOW9sPO1VfVUuktCFiJGS8hKeWfoML3//MsnOZC7PuZyzDj0r+oNitm6Fv/0NFiyAQYPgz3+GUaOiew7RZlq0wIUQIvrKA+X89IWfsipvFSV+MzBmYe5CPt/0OY9PfDx6J1q/HnJyoLgYysthxQqYMwdmzYKf/zx65xHtgvShCxEDs1bNYu3etVXJHEzt9XPLn+PH/T/Ws2cT3XQTFBSYZA4QDJrk/vvfR55yUcQ1SehCxMD769+n0FcY1m5Xdr7Y8kX0TjR3rknite3aBXv2RO88ol2QhC5EDGSlZIWV9AFYyiLTmxm9E3WuZwm25Ph7ICjqJwldiBiYcsSUsIReObfIif1PjN6J/vhH8NaaD93thrPPBo8n8j4ibklCFyIG+nfuz8zJM0lzp5HiTKlaBeiziz6L7uLLU6fC5ZebJN6pk/nzhBPg6aejdw7RbkjZohAxVB4oZ9mOZXgdXoZ3Hd5683jv2wdr1kDv3uZLxC0pWxSiPSovx/HKK4ydOdP0Z0+dau6eI5k/H556yiTms86C3/wGXOHzmtSpc2c45pjq14WFMH06/Pe/kJUFV14JRx7ZsutpgSJfEdOXT+fdH94lKyWLK8ZcQU5WxJwl6iF36ELEQiAAJ54IixZBUZFp83rh+uvh7rtDt33oIbjtNigpMaWGXi8MHWqSfFOSeqWDB01t+rZtpoTRskxXzFNPwYUXtvzamqigrICcZ3PIzc+l2F+MpSzcNjdPnfYUFx7e9vG0dzIfuhDtzezZsHhxdTIHk1wffBByc6vb9u6FW28171XefBUXw+rV8MorzTv3k0+acxRXzA1eWZt+5ZVQWtq8Y7bAE4ufYGv+Vor9Jp6gDlLsL+bK966k1N/28cQzSehCxMKcOabboza7HT77rPr1ggXgdIZvV1QEb9a/bFyd3nzT3O3XphQsX968Y7bAm6vfDBlgVR2OYvmOto8nnklCFyIWunQxybs2pSAtrfp1WlrkEZ2WBRnNnMe7rtp0vz/03G2kiyfyWqX+oJ80d9vHE88koQsRC7/9LTjCBxZht8NJJ1W/HjcOUlPDt3O7zfD95rj6akhKCm2zLOjXDw49tHnHbIGrxl5FkiM0HktZ9Evrx6GZbR9PPJOELkQsHHoo/POf5gFnaiqkpEDXrvDxx6EPOm02+Ogj6NXLbJOaagYE/e1vMGZM8849cSLccIP5UEhNNRU2/fvDu+9G59qaGs7Aidw47kbcdjeprlSSncn0T+/PO+e/E5N44plUuQgRS4WFplrF6zV347Y6Vq4PBuGrr0yFyjHHmEFCLbVnj6myyciA0aNNd08M7Snew6Jti8jwZjA6a3Tr1eTHOalDF6K17N4Nzz0H339vSgEvvbTx/dDFxeYue80ak0xPPx3eegu++AL+8x/Tp33eeXD88bByJbz2Guzfb/abNClyH/zKlXDddbB2LYwcCY89RkH3zsz4ZgYLti5gcJfBTDlyChneDF7b/gHvl75PVn4WU/alMajLoOj+bJoow5vBxIETYxpDvGvwDl0pNR04DdittR4e4f1jgdlA5Zyfb2qt72roxHKHLuLeypXmrrqszJT7eb2mb3rxYujTp/59i4vD+7HB9Ks7HNU150lJcMQRsGQJ+Hymfj0pCcaOhQ8/DE3qs2fDGWeEHG5XMhx5ayb7g0UUlxfjsrmwW3Z6pfYi92AuReVFOCwHdsvOK2e9wqQhkxDtW0vr0P8FnNLANl9orUdWfDWYzIVICFOnmi6Qytrt4mJTN3799Q3vW1f/d3l5aM15UZG5Yy8pMcm8sm3hQnPHXtMFF4Qd7tYJsKs4j+JyU+NdFiijqLyIH/b+QFG5qYEvD5ZT4i/horcvwhfwNRy7aLcaTOha63nAvjaIRYj4UV4OX38dXlIYDMIHHzS8/6pVLTt/URG8+mr16+LiiHXtc4aAP0K3vCb8N3OttdR9x7loVbkcrZT6Rin1vlJqWF0bKaWmKqWWKKWW5OXlRenUQsSAZdX9ANPtbtz+LaFUaDljpP50wFPe+EMGdACvw9vwhqLdikZCXwb00VofDjwOvF3XhlrrZ7TWOVrrnMzMKE7iL0Rbs9nMnOK1R3G63XDJJQ3vf845LTu/xwNTplS/djoj9tv/bil4/KHVIjZlw6ZCP4wUiqyULIZ3DXtMJuJIixO61vqg1rqw4vv3AIdSqplD2ISII088AYcdZh5SJiebh6LHHBM+uVYkL78cecDQsGHmOCkp5svjgbvugvT06np1lwtuvhnGjw/d9/PPwxazuGFVGicOPBmP3UOyI5lkZzKDugziklGX4La7SXYmk+JMoVtyN+acN0dKBeNco+rQlVLZwLt1VLl0B3ZprbVSagwwC3PHXu+BpcpFJAStzQPKdetMch85smn7v/SSSc5eL8yaBcOHm0WdP/rIPAQ96SRTBunzmbaDB2HCBOjRI/LxgkEzLe6SJWa7X/0KgJW7V7J853Ky07IZ13scSik2HdjEgi0LyPBmcHy/46O7sIZoNfVVuTSmbPEV4FggA9gF3A44ALTWTyulrgQuB/xACXC91vrLhoKShC7aje++Mw8YAzP0Kx4AAB1USURBVAGYPLl15gX3++Gee+Cdd6BbN7j33jqTf+F3hex+dTc6oOk6uSspR6aYwUd33AEHDpja9OuuMxU1//63mTnxpz+FX/yizr70WCkoK+CV719hVd4qRnUfxTnDzsHjkKXvWqJFCb21SEIX7cL995suDZ/P3G273Wauk/vui945CgvN0P38/ND2v/7VDMGvYfP9m9l812aCviBosNwWPYeupf+SqaH7ZmSYcslAwJQ0JifD4MEwb174GqIxsnH/Ro567iiKy4spKi8i2ZFMuiedRVMW0T25e6zDi1syH7oQkWzcCHfeWV3jXTkv+KOPwrffRu88U6aEJ3OAG280HyQVSjaWsPnOzQRLghAAghAsDrJtSW8K6Re675495oOichrcwkJTCvnII9GLu4WmvjOVvSV7q+rdC8sL2VG4gz9++McYR5a4JKGLjuuddyJPTevzmSH40VLXpFdahyxSsfedvUT6jTmIgzx+0vB5SkpMF0w74A/6mbtpLkEdDGufvXZ2jKJKfJLQRcflcESuB7esyFPbNld9Nec1ataVQ6Gs8CoTRRALf+POFWkxjBhQKCwV+brl4WvrkYQuOq4zz4x8h26ztbxOvKZf/zpye2Ute4WMMzOIMIATRZBM5jZ8Hq/XTEfQDtgsG78Y9Iuw5O2yuTj/sPNjFFXik4QuOq4ePeDZZ81dclKSSYhuN/z97zBgQPTO89hj0LdvaJtSpnukxt27q4eLQc8OwnJbWEkWltdCuRX9Ju/Gy/bQ/Q891DwYTUkxMXu9cMIJ8LvfRS/uFnrqtKfom9aXFGcKLpuLZGcyQzOHcv8J98c6tIQlVS5C7N5t1vgMBEzpX1ZW65xn1iyYOdMc/4476lwKzrfbx545eyAAXX7RBVeWC7ZvN9U4+/bBZZfBySebWR7feQd27ICjjzbT97YzgWCAjzZ8xLp96xjedTgTsifI4KUWkrJFkdiCQTPoZtkyyM6GX/6ycfOp1GfBArjmGjPI5+KL4eab8e/MJ+8vH+PbWkSniX3odNV4AsVBNv3fJoq+LyJldAp9buuDZYd9939C4bwduAemkjHtZGydPGaBis8/h8xM09XSqZOZR/2998yI0MmTzW8Nmzebh7KBgJn3PJq/LYi4JwldJK7CQjMEft06U3JYOSf5l1+aNTKb46KLYMaM0NO4h7Oi9D6C2AjixMKHN2UfhUU9oWYhhx28tp2UlaUSwIWNMizl44ij/4Pnmw9M7bjbbbpaTjsN3n7bDDqy2Uw3zAUXmK4Yrc2XZZm7+RtvbPaPSCQWSegicd1wAzz+uOl+qGRZpgti/vymH2/nzrBh9RpYxL8pIYvQx06V/3ZUrTZda7sAqaziCK5uejxg7t6XLYMhQ5q3v0goMrBIJK6XXgpN5mC6YBYtijyYpyER7oRLyaKMDML/uShCk3llW+3tbBQwBD8RVihqjPJy0/8uRAMkoYv4Fu3fMIPBsKZonaFFx4nRb9IivkhCF/HtvPPMdLI1WZaZYKtTp6Yf7/7wkjoP23GyN8LGld0rtdtqfygESOEHHBQ1PR4wg5zOOqt5+4oORRK6iG933AGDBpnJqcD82aVL2EPNRuvZ03xI1KCAYa77sVGERQkQxKKEpKQd5kFmTTaFx5mHjeKq7RyqkCGjPzEPa5UyD26Tk01Vi8djHoi6XOZh6cUXmz+dTpPI3W649VYYOrR51yM6FHkoKuJfIGBK/5YvN2WLkye3fMbBuXNN2WJhoak8ufNO/Ln72X3LR/hyS0g9pTfpf5qA/2CAH2/9keJVxaZs8Y4+2Oyw9+6PKFywG/eAFDLvPQlbl2QzE2Jl2eKvfmXq0FesgP/+18R79tlmVsaNG+GNN8x1nXmmmUVRiApS5SJEffbuNRNoBQJw6qnQrRv+Qj97391L4GCA9BPS8fSLPIe3Dmr2f7qfknUlJA1PotNPOtU5cKZodREHPj+AM9NJ51M7Y3PXsSZpxJ2LzCCigwfNiNDmlmSKuFdfQpdZckTH9sorcOml1Qs+X3EFB676J9893Re0SdgEoefVPen/QP+QXX17fKwYv4KyrWXogEbZFN6hXg7/5HDsydX/tHRQs+a3a8ibaRZGV3aFcipGfjqS5BHJDcc4fz5MnGi+r5zm96qrzHzqQtQgfeii49q50wyjLy01d8BFRQRL/Xz/t3QCBQEChQGCxUGCpUG2PbGN/Z/sD9n9h9//QMn6ErNdSZBAYYDCbwrZePPGkO12v7qbvNfyCJYEzXYFAfx7/Xx3+ncRp8sN4fPB6aebEasFBWbwVGkpPPkk/O9/0f6JiDgnCV10XBHmPD/A4egI/yyCRUG2P1c9QVbQH2TvnL3o8tCErMs0u/6zK6Rt+z+3EywKL4cs31tO0bcNVL7Mm2fuymsrKoLnn69/X9HhSEIXHVdZWVjducZBXRXjukzX3NB0x0TarlaSD5aGJ3MApRTBssjvhcTYnPdEhyQJXXRcp50WVnbYiRXoCI+WrCSLrud3rX7tsOj0k07hA0XtkPGLjJCmbhd0w/KG/1NTDkXyEQ30of/sZ2aul9qSksLKK4WQhC46rgED4JZbTC24ZYFS2L0WgyeuxPJYKIfJ1laSRfpx6WSemRmy++BnB2PvbK9K1laShbOrk/5/D314mjUli+SRydiSzYNX5VJYXouhLw/FsjfwTzA5GZ57zsRYuYpSUhJMmGBmlRSiBilbFGL5clPtEgiYlYrGjqVkQwk7X9yJ/4CfLqd1If2E9IjLw/nz/ez8905Th35kCl3P7YotKbwcUQc0e9/dy76P9+Hs7qT7xd1x92rCFL8bNsCLL8KBA+Y3ixNOqH9pO5GwpA5dtInS3FIOLjiII9NB2s/SULZ2uJDBvn3w6admIM/xx5sRmsXFpmIkEDCJMiUl1lEKUSepQxetSmvNhj9tYNsT27Cc5q7Rnmbn8E8OxzuwhSM2o+mpp+D6603XhVLmDvfWW830AZV16H4/vPBCdNcUFaKNyB26aLG8N/NYfeHq0NI8BZ5BHsasHtM+lhz75hszR3pJScPbejywdi307t36cQnRRDIfumhV257YFl5nraFsaxnFq4tjE1Rt06ebQTqNEQzCq6+2bjxCtAJJ6KLF/AcjlNUByqYIFEYYFBMLBQWRB+hE4vOZ7YWIM5LQRYt1PacrlifCXyULkkc2Yq6StnDmmdVT7DbE4zGTdAkRZyShixbr+YeeeAZ4sJIq/jrZwfJaDH5+cNVD0pg79VSzmHRSxTJwlmUqXcaNq56nHMz3v/oVjB4du1iFaCapchEtZkuyccSiI9g9czf73tuHM8tJ1u+ySDq0mWtotgbLgjlzzNdrr5nSxMsugzFjzNznM2aYCpdf/xpOPjl84Qoh4oBUuYh2QQeDFLy4EH9eMakX5mDvXs/ycZ9+Cl99ZWrGx46te7v9+2HxYrOgxMiRTUvSWpt9Dx4056isTf/uO9ixA444AjIy6j+GEK2gRXXoSqnpwGnAbq318AjvK+BRYCJQDFystV7WspBFR1L03vd8e/oa/AEvoNE3fsWA3+STNeNXoRvu2QOHHmr+BPjLX6BPH1i1KnyFovvug7vuMku5BQJmuw8+aFwp4po1cMopZuELy4Lycrj7bnjpJVPO6HCYibGuuw6mTZO7edFuNHiHrpQaDxQCM+pI6BOBqzAJfSzwqNa6ntsmQ+7QBYD2B/jK/Ta+QDo1H+lYlDLyxXRSL6zxV2nIEJNQazv2WPjss+rXH3xglqErqjE1rc0Gw4ebJd/qEwya5L9tm7lLrwrIzPUSUimTlGTKIWUQkmhDLapD11rPA/bVs8kkTLLXWuuvgTSlVI/mhSo6mvyn5hMIeKj9VzGIg+33fFvd4PNFTuZg1ums6ZFHQpM5mES8bl3dx6g0fz7k54cmczCJvnbZY1ERPPRQ/ccTog1FowShJ7C1xuvcirYwSqmpSqklSqkleXl5UTi1iHf+HYVEnn/chi+/xl/P0tK6D1I7+VZ2ydRmt5u5XOqzf3/TulAaOp4QbSgaCT3S3/6I/Tha62e01jla65zMzMxIm4gOJvWiIwniCGu3KCFzYmqNDVPBXcfshLUfTp5xRuRtg0EYNar+gMaNa/yIUqfTLA8nRDsRjYSeC9R80tQL2F7HtkKEcA7uTvbJO7AoBcz0ARaleD176fpwrcE9//hH5IO8+GLo66uugqwsM0AIzB231wuPPVb3h0KljAy4/fbQh6xeL2Rnmz8rp6x1u031zI03Nuo6hWgL0Ujoc4ALlXEUkK+13hGF44oOos8HF3HY311k9NxIWvqP9D/vAKO2/xJbWq3Klcsug08+gREjoFMnU0O+eDFMnBi6XadOZo7zu+82C0Gcf74pdbzkksYFdNNN8O675sHqccfBgw+aSpr58+E3vzEPYW+/3ZQwym+aoh1pTJXLK8CxQAawC7gdzO/IWuunK8oW/wGcgilbvERr3WD5ilS5CCFE07WoDl1rXe/Chdp8IlzRzNiEEEJESTuZaEMIIURLSUIXQogEIQldCCEShCR0IYRIEJLQhRAiQUhCF0KIBCEJXQghEoQkdCGESBCS0IUQIkFIQhdCiAQhCV0IIRKEJHQhhEgQktCFECJBSEIXQogEIQldCCEShCR0IYRIEJLQhRAiQUhCF0KIBCEJXQghEoQkdCGESBCS0IUQIkFIQhdCiAQhCb2J/MEgWutYhyGEEGEkoTfSgvx8Ri5ejHPePFK++II/rV+PLxiMdVhCCFHFHusA4sHKoiJO+uYbiisSeFEwyJPbt7PT5+M/Q4fGODohhDDkDr0RHtiyhbJad+MlwSBv7NnDLp8vRlEJIUQoSeiN8G1hIYEI7S6l2FhS0ubxCCFEJJLQGyEnJQVbhPbSYJABHk+bxyOEEJFIQm+EPx9yCB4r9EfltSwu6t6dTKczRlEJIUQoSeiNMMjrZd6oUYzv1AmnUmQ6HNzSpw9PDhoU69CEEKKKVLk00qiUFD4fNSrWYQghRJ0adYeulDpFKbVWKbVeKXVThPcvVkrlKaVWVHz9Nvqhtj+bN8MvfwleL6Snw/XXgzwjFULESoN36EopG/AEcCKQCyxWSs3RWq+qtemrWusrWyHGdunAARg9GvbuhWDQJPKnnoIVK+DTT2MdnRCiI2rMHfoYYL3WeqPW2gfMBCa1bljt3wsvQFGRSeaVSkth4UJYtix2cQkhOq7GJPSewNYar3Mr2mo7Syn1rVJqllKqd1Sia8eWLIHi4vB2y4Lvv2/7eIQQojEJXUVoqz071TtAttZ6BPA/4MWIB1JqqlJqiVJqSV5eXtMibWcOOwwilaBrDQMHtn08QgjRmISeC9S84+4FbK+5gdZ6r9a6rOLls8CRkQ6ktX5Ga52jtc7JzMxsTrztxmWXgcsFqsbHndMJQ4bAUUfFLi4hRMfVmIS+GBiolOqrlHIC5wJzam6glOpR4+XpwOrohdg+ZWbCggUwbpzpZnE64eyz4X//C03yQgjRVhqsctFa+5VSVwIfAjZgutZ6pVLqLmCJ1noOcLVS6nTAD+wDLm7FmNuNoUPhiy+gvNwkdVuk+QGEEKKNNKoOXWv9ntZ6kNa6v9Z6WkXb/1Ukc7TWN2uth2mtD9daT9Bar2nNoJtDa3jxRZOEMzJg0iRYVbvwssJtt4HDYe60LQsmT4Z168yfGRkweDA8+yzs3AlTp0LPntCvH9x7r0nutfmDQR7csoW+X39N5oIFXLx6NdvKysI3FEKIFlCxWn0nJydHL1mypM3Od/fdcP/91ZUpSkFyMixdGvoQ87bb4J57wvevnMqlskzR6zVtZWXVSdzjgRNPhNmzQ/e9YNUq3tqzp2o+dTvQ2eFgzZgxpDsc0btIIUTCU0ot1VrnRHqvQ8zlUlQUmszB3LEXF4cn7/vui3yMYDC05ry4GAoLQ+/IS0rg449DyxZ/LCnhjRrJHEy/VEEgwLPbQ54tCyFEi3SIhL5hA9gjPC0IBOCrr8LbWsJmg+XLq18vLyzEGeEpaUkwyBf5+S07mRBC1NAhEnrPnlDXwkL9+4e+jkaFSp8+1d9nu90EInRrOZRisNfb8pMJIUSFDpHQu3Qxk2jVHgjk9cKtt4a2nX56447pcoVXtdjt0Ls3/PSn1W2jkpM51OvFUeuTwqkUV/SMNOBWCCGap0MkdIDnn4df/xrcbpOMe/SAGTPgJz8J3e7tt2HMmNC27t1h1iyTrF0u83XOOfDZZ6Zqxuk0XyecYNpq5m6lFB8efjg/79wZp1I4lWKgx8MHI0bQV1Y7EkJEUYepcqlUWgr5+WZgkFXPx1lJiZloa8QI6NzZtGkNeXmQkhJ6t793r0noKSn1n7vQ76c0GKSLw4GS0UdCiGaor8qlQy1wcdvGjTy6bRslgQDZmzy8MHgwc2YrHtm3ifIeJbi2JHPXgGxOG5LMnXfCokWmj/0vf4FjjzV33l27Vh/vwAF44AFz9+71wh/+AFOm1P1BkWy3k9wmVyqE6Ig6zB36Wd9/z5t79oQ2aqBcgV2bzqcgUGbhuHEkgZWpITXn//qXGdpfqaQEDj8ctmwxtegASUlw5pnw73+3/vUIITqmDl+HfqC8PDyZV3Lq6p+CBXiClP92fVjN+TXXmC6XSq+8Atu3VydzMPXub7wBP/wQ7SsQQoiGdYiE/sn+/ZHfqKsbe1BhWNO+faavvNKnn5oEXpvNZvrehRCirXWIhD6kqfXe+eHD8S0r9KFndrZ5EFqbUpCV1bTTCSFENHSIhD4sOZlukeZMqexDr6nEQs08JKTJ44FLLjHlipWmTDETeNVks5ma9wkTohO3EEI0RYdI6AArcnLoUeuW+ryMTNK/6gFlFhTboNSi99e9uWdMFklJ5o7c7YbzzoOHHw49Xp8+8O670KuXeWjqdsORR8Lnn9dfDimEEK2lw1S5VFpXXMy64mLGp6WRXDHBy+otfuatLOPnR7o5pKsZ/llSAps3mwFInTrVfTytzVwxHo+ZYkAIIVpTwtShFxfDU0/BzJlm6ts//MHMUV57jE4waGrHn37aVKH89KcwfTpc8/wB3rRvJZhRhnd1Ov8+vTf3zTnAktEbIN0H77s5dcMgsrSHZwu2wLB82Opl9MZD+OMFLn4zO5fyUftgn5Mxm3vz2d/TGxWP1jBnDjz+uKldnzwZrrii4YFIQgjRFHFzh+7zwdixsHatuXsGU/d9ySUmUdZ0zDHhsyhy8g64dh04g6ajyafAr8BTUZ+oqF76urSiNt0BBIByy3TLeAKmzBFMX/uz/XB/0KvBeG65BR57rLoqxu02D1WXLjXdNUII0VgJUYf++utm1aDK5AkmQT73HGzaVN327bcRkrk9CFeuB3ew+oqd2iRzRXX5YuWf7opkDmbRPXcQUv3VyRzAE0RP2UhJsHq+3cp4fvyxerOdO03/e80Sx9JSMyBpxowm/hCEEKIecZPQ33svct233W7W9az00ksRdu5dHPlKI9Whq3raawso6Bdas263w/z51a+/+ipyeWNxMfz3vxGOKYQQzRQ3Cb1nz/AyQQifXyU7O8LO+Q6wBSO80UJ2Dfmh2bp2PF27hq50VMlmMxUyQggRLXGT0KdODU/oleuCnnBCdduUKRFWJ9rngu86mX7zmgJU95tX0hXtNfmUWTeupnJgQxJsr552sTKe44+v3uzooyPP7OhymYeoQggRLXGT0AcMMNUk6emmOiQpybR99lnoQhN2O3zySXg3xy9XDoNVqebhZqGpOe/0djbsdpokXvm10Quv96zersyCRZ3x/qcflFTvyw8pXLvnsIjx1PxAsSwTz6GHmgegqanma/p0OOyw1v+5CSE6jripcqlUXg4rVpjkOHRo/UvGvfce7NplZklMrpi39ok3Sli4zsefzkpixECTea9+tIA56wq4/Kg0brzAlJ08/1o5D88q5qSRbh66xQwRXbEmwM3PFjIy28l9V3maHM+aNXDwIIwcGblfXQghGlJflUvcJfTatIbZs+GFF0xf9YUXwllnNX605rx5ZibFzZth8GBT5+5wmO6Q774z87L8/e9w8sktDlUIIVosoRP6pZfCa69VV8AkJcHEifDqqw0v+PzCC2b/xnjkEZP4hRAilhI2oa9YAePGmRLAmpKS4KOPzACj+ng8pia8MRwOs63M0yKEiKWEGFgUyccfmz7s2oqL4cMP69/34MHGJ3Mw59mwoWnxCSFEW4rrhJ6eHvnhostVvbBzXdzupp+vS5em7yOEEG0lrhP6WWdFbrcsOPfc+vd1OmHgwMafq1evhj8khBAiluI6oaenwzvvQFpadX13airMmgXdujW8//z54Um6Vy/o3z+0LTUVFiyIXtxCCNEa4mr63EgmTIDdu01yDgbhJz8JXVmoPl27mnVCP/wQvv7ajDgdN868t3gxvP8+HHEEnHZa68UvhBDREtdVLkII0dG0uMpFKXWKUmqtUmq9UuqmCO+7lFKvVry/UCmV3bKQhRBCNFWDCV0pZQOeAH4ODAXOU0oNrbXZZcB+rfUA4GHggWgHKoQQon6NuUMfA6zXWm/UWvuAmcCkWttMAl6s+H4WcLxSDY3TFEIIEU2NSeg9ga01XudWtEXcRmvtB/KBsKptpdRUpdQSpdSSvLy85kUshBAiosYk9Eh32rWfpDZmG7TWz2itc7TWOZmZmY2JTwghRCM1JqHnAr1rvO4FbK9rG6WUHegE7ItGgEIIIRqnMXXoi4GBSqm+wDbgXOD8WtvMAS4CvgImA5/qBuohly5dukcptbnpIVfJAPa0YP/2JJGuBRLrehLpWiCxrqejXkufut5oMKFrrf1KqSuBDwEbMF1rvVIpdRewRGs9B3ge+LdSaj3mzryBgfegtW5Rn4tSakldtZjxJpGuBRLrehLpWiCxrkeuJVyjRopqrd8D3qvV9n81vi8Fzm5pMEIIIZovrudyEUIIUS2eE/ozsQ4gihLpWiCxrieRrgUS63rkWmqJ2VwuQgghoiue79CFEELUIAldCCESRNwldKXUdKXUbqXU97GOpaWUUr2VUp8ppVYrpVYqpa6JdUzNpZRyK6UWKaW+qbiWO2MdU0sppWxKqeVKqXdjHUtLKaU2KaW+U0qtUErF/bzVSqk0pdQspdSain8/R8c6puZQSg2u+H9S+XVQKXVts48Xb33oSqnxQCEwQ2s9PNbxtIRSqgfQQ2u9TCmVAiwFztBar4pxaE1WMRlbkta6UCnlAOYD12itv45xaM2mlLoeyAFStdZxvcyJUmoTkKO1ToiBOEqpF4EvtNbPKaWcgFdrfSDWcbVExcy224CxWutmDbqMuzt0rfU8EmRaAa31Dq31sorvC4DVhE98Fhe0UVjx0lHxFV93CzUopXoBpwLPxToWEUoplQqMxwxoRGvti/dkXuF4YENzkznEYUJPVBWLgowCFsY2kuar6KJYAewGPtZax+21AI8AfwaCsQ4kSjTwkVJqqVJqaqyDaaF+QB7wQkWX2HNKqaRYBxUF5wKvtOQAktDbAaVUMvAGcK3W+mCs42kurXVAaz0SM4HbGKVUXHaJKaVOA3ZrrZfGOpYoGqe1PgKzUM0VFV2X8coOHAE8pbUeBRQBYSupxZOKbqPTgddbchxJ6DFW0d/8BvCS1vrNWMcTDRW//s4FTolxKM01Dji9ot95JnCcUuo/sQ2pZbTW2yv+3A28hVm4Jl7lArk1fgOchUnw8eznwDKt9a6WHEQSegxVPEh8HlittX4o1vG0hFIqUymVVvG9BzgBWBPbqJpHa32z1rqX1job82vwp1rrC2IcVrMppZIqHrpT0TVxEhC3VWJa653AVqXU4Iqm44G4KySo5Txa2N0CjZycqz1RSr0CHAtkKKVygdu11s/HNqpmGwf8Bviuou8Z4JaKydDiTQ/gxYon9RbwmtY67sv9EkQ34K2KVSHtwMta6w9iG1KLXQW8VNFVsRG4JMbxNJtSygucCPyuxceKt7JFIYQQkUmXixBCJAhJ6EIIkSAkoQshRIKQhC6EEAlCEroQQiQISehCCJEgJKELIUSC+H8t3A8z/vJazwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "color = np.repeat(\"\", 120)\n",
    "color[y_train == 0] = \"b\"\n",
    "color[y_train == 1] = \"r\"\n",
    "color[y_train == 2] = \"g\"\n",
    "plt.scatter(x_train[:, 0], x_train[:, 1], color=color)\n",
    "color = np.repeat(\"\", 30)\n",
    "color[y_test == 0] = \"c\"\n",
    "color[y_test == 1] = \"m\"\n",
    "color[y_test == 2] = \"y\"\n",
    "plt.scatter(x_test[:, 0], x_test[:, 1], color=color)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2]\n",
      "[2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_x = x[:, [2,3]]\n",
    "_y = y\n",
    "test_index = set(np.arange(40,50)) | set(np.arange(90,100)) | set(np.arange(140,150))\n",
    "train_index = set(np.arange(0,150)) - test_index\n",
    "test_index = np.array(list(test_index))\n",
    "train_index = np.array(list(train_index))\n",
    "x_train = _x[train_index]\n",
    "y_train = _y[train_index]\n",
    "x_test = _x[test_index]\n",
    "y_test = _y[test_index]\n",
    "print(y_train)\n",
    "print(y_test)\n",
    "knn = KNearestNeighbor(k=5)\n",
    "knn.fit(x_train,y_train)\n",
    "knn.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 決定木"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ノンパラメトリック手法の1つで、入力空間を分割することでクラスを分類する。(回帰ならばそのクラスに定数値を割り当てる。)分割の仕方は、IG(Information Gain)がより大きくなるように行う。ここで、\n",
    "\\begin{eqnarray*}\n",
    "IG = I_{parent} - \\Sigma_{child} \\frac{N_{child}}{N_{parent}} I_{child}\n",
    "\\end{eqnarray*}\n",
    "とする。IはImpurity(不純度)で、その指標として次の3つが提唱されている。\n",
    "- エントロピー\n",
    "$I_H = - \\Sigma_c p(c|t) \\log{p(c|t)}$\n",
    "- ジニ係数\n",
    "$I_G = \\Sigma_c p(c|t) (1 - p(c|t)) = 1 - \\Sigma_c p(c|t)^2$\n",
    "- 誤差分類率\n",
    "$I_E = 1 - max_c p(c|t)$  \n",
    "最初の木は1つのノードからなる(つまりすべてのデータ点が同じクラスに分類される)として、木を成長させていく。ノードを増やしていくことで分類境界が複雑になり、より表現力が上がる。最小化したい誤差関数を\n",
    "\\begin{eqnarray*}\n",
    "E = \\Sigma I + c | T |\n",
    "\\end{eqnarray*}\n",
    "とおく。ここで$|T|$は木のノード数であり、$\\Sigma$は木のノード全体に対して和をとる。$|T|$は正則化項に対応している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(c):\n",
    "    _, counts = np.unique(c, return_counts=True)\n",
    "    total_counts = np.sum(counts)\n",
    "    ps = counts / total_counts\n",
    "    IH = np.sum(- ps * np.log(ps))\n",
    "    return IH\n",
    "\n",
    "def gini(c):\n",
    "    _, counts = np.unique(c, return_counts=True)\n",
    "    total_counts = np.sum(counts)\n",
    "    ps = counts / total_counts\n",
    "    IG = 1 - np.sum(ps**2)\n",
    "    return IG\n",
    "\n",
    "def error_rate(c):\n",
    "    _, counts = np.unique(c, return_counts=True)\n",
    "    total_counts = np.sum(counts)\n",
    "    ps = counts / total_counts\n",
    "    IE = 1 - np.max(ps)\n",
    "    return IE\n",
    "\n",
    "class DecisionTree():\n",
    "    def __init__(self, x=np.array([]), y=np.array([]), impurity=entropy):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.label = np.argmax(np.bincount(y)) if (y.size > 0) else None\n",
    "        self.impurity = impurity\n",
    "        self.condition = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "    def search(self, x):\n",
    "        if (self.condition):\n",
    "            truth_value = self.condition(x)\n",
    "            if (truth_value):\n",
    "                return self.left.search(x)\n",
    "            else:\n",
    "                return self.right.search(x)\n",
    "        else:\n",
    "            return self.label\n",
    "    def expand(self, condition):\n",
    "        self.condition = condition\n",
    "        truth_values = np.array([self.condition(_x) for _x in self.x])\n",
    "        left = DecisionTree(x=self.x[truth_values == True], y=self.y[truth_values == True], impurity=self.impurity)\n",
    "        right = DecisionTree(x=self.x[truth_values == False], y=self.y[truth_values == False], impurity=self.impurity)\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "    def cut(self):\n",
    "        self.condition = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "    def information_gain(self):\n",
    "        I_parent = self.impurity(self.y)\n",
    "        if (self.left or self.right):\n",
    "            I_left = self.impurity(self.left.y)\n",
    "            I_right = self.impurity(self.right.y)\n",
    "            n_parent = len(self.y)\n",
    "            n_left = len(self.left.y)\n",
    "            n_right = len(self.right.y)\n",
    "            IG = I_parent - I_left*n_left/n_parent - I_right*n_right/n_parent\n",
    "        else:\n",
    "            IG = 0\n",
    "        return IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier(DecisionTree):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, x, y):\n",
    "        super().__init__(x, y)\n",
    "    def predict(self, x):\n",
    "        labels = np.array([self.search(_x) for _x in x])\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       0, 2, 0, 0, 0, 2, 2, 0, 0, 2, 2, 2, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0,\n",
       "       2, 0, 2, 0, 2, 0, 2, 2, 2, 1, 1, 1, 2, 0, 1, 1, 0, 2, 1, 1, 1, 2,\n",
       "       2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1,\n",
       "       2, 1, 1, 2, 1, 0, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 2, 0, 1, 1, 2,\n",
       "       2, 1, 2, 1, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2,\n",
       "       1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(x, y)\n",
    "tree.expand(lambda x: x[0] > 5.0)\n",
    "tree.left.expand(lambda x: x[1] < 3.0)\n",
    "tree.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMアルゴリズムを利用したアルゴリズムの1つである。データセットをいくつかのクラスタに分割することを目標とする。\n",
    "アルゴリズムでは、\n",
    "- E step  \n",
    "各データ点について、最も近いクラスタ中心のクラスタを割り当てる。\n",
    "- M step  \n",
    "各クラスタについて、それに属しているデータ点すべての平均をとり、新しいクラスタ中心とする。 \n",
    "の2ステップを反復実行して解を求める。\n",
    "\n",
    "初期値をランダムに決めると収束が遅いことがある。これを改善するためにK-means++という手法がある。K-means++では以下のようにしてクラスタ中心を決定する。\n",
    "> 1. データ点をランダムに1つ選ぶ。\n",
    "> 2. クラスタ中心がk個選ばれるまで以下を繰り返す。\n",
    ">> 各データ点$\\vec{x}$について最も近いクラスタ中心までの距離を$D(\\vec{x})$とする。このとき\n",
    "\\begin{eqnarray*}\n",
    "p(\\vec{x}) = \\frac{D(\\vec{x})^2}{\\Sigma D(\\vec{x})^2}\n",
    "\\end{eqnarray*}\n",
    ">> という確率分布に従って次のクラスタ中心を(データ点の中から)選ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 混合ガウスモデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p({\\bf x}) \\equiv \\sum_{k=1}^K \\pi_k N({\\bf x} | {\\bf \\mu_k}, \\Sigma_k)\n",
    "$$\n",
    "\n",
    "潜在変数${\\bf z}$\n",
    "$$\n",
    "p(z_k = 1) = \\pi_k \\\\\n",
    "\\sum_{k=1}^K z_k = 1 \\\\\n",
    "\\sum_{k=1}^K \\pi_k = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "p({\\bf z}) = \\prod_{k=1}^K \\pi_k^{z_k} \\\\\n",
    "p({\\bf x} | z_k = 1) = \\mathcal{N} ({\\bf x} | \\mu_k, \\Sigma_k) \\\\\n",
    "p({\\bf x} | {\\bf z}) = \\prod_{k=1}^K \\mathcal{N} ({\\bf x} | \\mu_k, \\Sigma_k)^{z_k} \\\\\n",
    "p({\\bf x}) = \\sum_{\\bf z} p({\\bf x} | {\\bf z}) p({\\bf z}) = \\sum_{k=1}^K \\pi_k \\mathcal{N}({\\bf x} | {\\bf \\mu_k}, \\Sigma_k) \\\\\n",
    "\\gamma(z_k) \\equiv p(z_k=1 | {\\bf x}) = \\frac{p(z_k=1)p({\\bf x} | z_k=1)}{\\sum_{j=1}^K p(z_j=1)p({\\bf x} | z_j=1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p({\\bf x_n}) = \\sum_{k=1}^K \\pi_k \\mathcal{N}({\\bf x_n} | {\\bf \\mu_k}, \\Sigma_k) \\\\\n",
    "\\log p(X) = \\sum_{n=1}^N \\log p({\\bf x_n}) \\\\\n",
    "L = - \\log p(X) + \\sum_{k=1}^K \\pi_k \\\\\n",
    "\\frac{\\partial}{\\partial \\pi_k} L = 1 - \\sum_{n=1}^N \\mathcal{N}({\\bf x_n} | {\\bf \\mu_k}, \\Sigma_k) \\\\\n",
    "\\frac{\\partial}{\\partial \\mu_k} L = \\sum_{n=1}^N \\frac{\\pi_k \\mathcal{N}({\\bf x_n} | {\\bf \\mu_k}, \\Sigma_k)}{\\sum_{k=1}^K \\mathcal{N}({\\bf x_n} | {\\bf \\mu_k}, \\Sigma_k)} \\Sigma_k^{-1} ({\\bf x_n} - {\\bf \\mu_k}) = \\sum_{n=1}^N \\gamma_{nk} \\Sigma_k^{-1} ({\\bf x_n} - {\\bf \\mu_k}) \\\\\n",
    "\\frac{\\partial}{\\partial \\Sigma_k} L = \\sum_{n=1}^N \\frac{\\pi_k \\mathcal{N}({\\bf x_n} | {\\bf \\mu_k}, \\Sigma_k)}{\\sum_{k=1}^K \\mathcal{N}({\\bf x_n} | {\\bf \\mu_k}, \\Sigma_k)} (- \\frac{1}{2} \\Sigma_k^{-1} + \\frac{1}{2} \\Sigma_k^{-1} ({\\bf x_n} - {\\bf \\mu_k}) ({\\bf x_n} - {\\bf \\mu_k})^T \\Sigma_k^{-1}) = \\sum_{n=1}^N \\gamma_{nk} (- \\frac{1}{2} \\Sigma_k^{-1} + \\frac{1}{2} \\Sigma_k^{-1} ({\\bf x_n} - {\\bf \\mu_k}) ({\\bf x_n} - {\\bf \\mu_k})^T \\Sigma_k^{-1}) \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\pi_k = \\sum_{n=1}^N \\mathcal{N}({\\bf x_n} | {\\bf \\mu_k}, \\Sigma_k) \\\\\n",
    "\\mu_k = \\frac{\\sum_{n=1}^N \\gamma_{nk} {\\bf x_n}}{\\sum_{n=1}^N \\gamma_{nk}} \\\\\n",
    "\\Sigma_k = \\frac{\\sum_{n=1}^N \\gamma_{nk} ({\\bf x_n} - {\\bf \\mu_k}) ({\\bf x_n} - {\\bf \\mu_k})^T}{\\sum_{n=1}^N \\gamma_{nk}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial}{\\partial \\mu} \\mathcal{N} = \\Sigma^{-1} ({\\bf x} - {\\bf \\mu}) \\\\\n",
    "\\frac{\\partial}{\\partial \\Sigma} \\mathcal{N} = \\frac{\\partial}{\\partial \\Sigma} \\exp (\\log(\\mathcal{N})) = \\mathcal{N} \\frac{\\partial}{\\partial \\Sigma} \\log(\\mathcal{N}) \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\Sigma} \\log |\\Sigma| = \\Sigma^{-T} \\\\\n",
    "\\frac{\\partial}{\\partial x} A^{-1} = - A^{-1} \\left( \\frac{\\partial}{\\partial x} A \\right) A^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans():\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.centroid = []\n",
    "        self.category = []\n",
    "    def e_step(self, x):\n",
    "        loss = np.zeros(x.shape[0], self.k)\n",
    "        for l in range(k):\n",
    "            loss[:, l] = np.linalg.norm(x - self.centroid[k].reshape(-1,1), axis=0)\n",
    "        self.category = np.argmin(loss, axis=1)\n",
    "        return self.category\n",
    "    def m_step(self, x):\n",
    "        self.centroid = np.zeros(x.shape[1], self.k)\n",
    "        for l in range(k):\n",
    "            x_l = x[:, category==l]\n",
    "            self.centroid[:, k] = np.mean(x_l, axis=1)\n",
    "        return self.centroid\n",
    "    def fit(self, x, iteration=10):\n",
    "        for _ in range(iteration):\n",
    "            category = e_step(x)\n",
    "            centroid = m_step(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主成分分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n$次元データ$\\vec{x}$を$m (m<n)$次元データに射影することを考える。射影行列を$T \\in M_{mn}$とすると、$T \\vec{x}$は平面上に射影されたデータ点である。これをもとの$n$次元空間に引き戻したときにもとのデータとの誤差が小さくなるように射影する。すなわち\n",
    "\\begin{eqnarray*}\n",
    "T^{\\ast} = argmin_T || T^T T \\vec{x} - \\vec{x} ||^2\n",
    "\\end{eqnarray*}\n",
    "を求める。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pca():\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.T = np.zeros((input_dim, output_dim))\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "    def fit(self, x):\n",
    "        C = np.dot(x.T, x)\n",
    "        eigen_vals, eigen_vecs = np.linalg.eig(C)\n",
    "        eigen_vals, eigen_vecs = eigen_vals[::-1], eigen_vecs[::-1]\n",
    "        self.T = eigen_vecs[:output_dim]\n",
    "        return self.T\n",
    "    def transform(self, x):\n",
    "        y = np.dot(self.T, x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 線形判別分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを射影することを考える。この際に「クラス間分散が大きく」「クラス内分散が小さく」なるように射影する平面を決める。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial}{\\partial x} x + y &=& 1 \\\\\n",
    "\\frac{\\partial}{\\partial y} x + y &=& 1 \\\\\n",
    "\\frac{\\partial}{\\partial x} x * y &=& y \\\\\n",
    "\\frac{\\partial}{\\partial y} x * y &=& x\n",
    "\\end{eqnarray*}\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "Z = X + Y \\\\\n",
    "(\\frac{\\partial}{\\partial X} L(Z))_{ij} &=&\n",
    "\\Sigma_{kl} \\frac{\\partial}{\\partial z_{kl}} L(Z) \\frac{\\partial}{\\partial x_{ij}} z_{kl} =\n",
    "\\Sigma_{kl} \\frac{\\partial}{\\partial z_{kl}} L(Z) \\frac{\\partial}{\\partial x_{ij}} (x_{kl} + y_{kl}) =\n",
    "\\frac{\\partial}{\\partial z_{ij}} L(Z) =\n",
    "(\\frac{\\partial}{\\partial Z} L(Z))_{ij} \\\\\n",
    "Z = X Y \\\\\n",
    "(\\frac{\\partial}{\\partial X} L(Z))_{ij} &=&\n",
    "\\Sigma_{kl} \\frac{\\partial}{\\partial z_{kl}} L(Z) \\frac{\\partial}{\\partial x_{ij}} z_{kl} =\n",
    "\\Sigma_{kl} \\frac{\\partial}{\\partial z_{kl}} L(Z) \\frac{\\partial}{\\partial x_{ij}} (\\Sigma_m x_{km} y_{ml}) =\n",
    "\\Sigma_{l} \\frac{\\partial}{\\partial z_{il}} L(Z) y_{jl} =\n",
    "(\\frac{\\partial}{\\partial Z} L(Z) Y^T)_{ij} \\\\\n",
    "(\\frac{\\partial}{\\partial Y} L(Z))_{ij} &=&\n",
    "\\Sigma_{kl} \\frac{\\partial}{\\partial z_{kl}} L(Z) \\frac{\\partial}{\\partial y_{ij}} z_{kl} =\n",
    "\\Sigma_{kl} \\frac{\\partial}{\\partial z_{kl}} L(Z) \\frac{\\partial}{\\partial y_{ij}} (\\Sigma_m x_{km} y_{ml}) =\n",
    "\\Sigma_{k} \\frac{\\partial}{\\partial z_{kj}} L(Z) x_{ki} =\n",
    "(X^T \\frac{\\partial}{\\partial Z} L(Z))_{ij}\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer():\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x + y\n",
    "        return out\n",
    "    def backward(self, dout):\n",
    "        dx = dout\n",
    "        dy = dout\n",
    "        return dx, dy\n",
    "    \n",
    "class MulLayer():\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        return out\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "        return dx, dy\n",
    "    \n",
    "class MatAddLayer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "    def backward(self, dout):\n",
    "        dx = dout\n",
    "        dy = dout\n",
    "        return dx, dy\n",
    "\n",
    "class MatMulLayer():\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return np.dot(x, y)\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.y.T)\n",
    "        dy = np.dot(self.x.T, dout)\n",
    "        return dx, dy\n",
    "\n",
    "class Relu():\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "    def forward(self, x):\n",
    "        self.mask = x <= 0\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    def backward(self, dout):\n",
    "        dx = dout.copy()\n",
    "        dx[self.mask] = 0\n",
    "        return dx\n",
    "        \n",
    "class Sigmoid():\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        return out\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.x * (1 - self.x)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 固有値分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一般化固有値分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# スペクトル分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特異値分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "行列$A \\in M_{mn}$を直交行列$W \\in M_m, V \\in M_n$と対角行列$\\Sigma \\in M_{mn}$に分解することを考える。つまり\n",
    "\\begin{eqnarray}\n",
    "A = W \\Sigma V^T\n",
    "\\end{eqnarray}\n",
    "となるような$W, \\Sigma, V$を求める。特異値分解は一般の行列$A$に対して常に可能である。  \n",
    "$A^T A$は半正定値対称行列であるから、固有値分解することができて\n",
    "\\begin{eqnarray}\n",
    "V^T A^T A V = diag(\\sigma_1^2, \\dots, \\sigma_n^2)\n",
    "\\end{eqnarray}\n",
    "となる。ここで$V$は固有値ベクトルを並べたもので$V = (\\vec{v}_1, \\dots, \\vec{v}_n)$である。この$V$は直交行列である。(便宜上、$\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\sigma_k > \\sigma_{k+1} = \\dots \\sigma_n = 0$と仮定する。また、$m \\ge n$としても一般性を失わない。$m < n$ならば転置をとったものについて特異値分解を行えば同様の議論ができる。)  \n",
    "$V_1, W_1$を次のように定義する。\n",
    "\\begin{eqnarray}\n",
    "V_1 &=&(\\vec{v}_1, \\dots, \\vec{v}_k)\n",
    "W_1 &=& A V_1 diag(\\sigma_1, \\dots, \\sigma_k) \\in M_{mk}\n",
    "\\end{eqnarray}\n",
    "$W_2 \\in M_{m, m-k}$を$W_1$と直交するように構成することができる。(グラムシュミットの直交化法を用いる。)このとき、$W_1$の定義より$W_2$は$A V_1$とも直交することがわかる。よって、$W = (W_1, W_2)$とすればこれは直交行列であって\n",
    "\\begin{eqnarray}\n",
    "W^T A V_1 = (W_1^T A V_1, W_2^T A V_1)^T = (diag(\\sigma_1, \\dots, \\sigma_k), O_{m_k, k})^T\n",
    "\\end{eqnarray}\n",
    "が成立する。また、$V_2 = (\\vec{v}_{k+1}, \\dots, \\vec{v}_n)$とすると、\n",
    "\\begin{eqnarray}\n",
    "V_2^T A^T A V_2 = O_{m-k}\n",
    "\\end{eqnarray}\n",
    "となる。$|| A V_2 || = 0$すなわち$A V_2 = \\vec{0}$が従うので、結局\n",
    "\\begin{eqnarray}\n",
    "W^T A V = W^T A (V_1, V_2) = \n",
    "\\left(\n",
    "\\begin{array}{cc}\n",
    "(diag(\\sigma_1, \\dots, \\sigma_k) & O_{hoge} \\\\\n",
    "O_{m_k, k} & O_{hoge}\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray*}\n",
    "L_k A_k L^T_k = \n",
    "\\left(\n",
    "\\begin{array}{ccc}\n",
    "I_k & \\vec{0} & O \\\\\n",
    "\\vec{0}^T & \\sqrt{a_{k+1, k+1}} & \\vec{0}^T \\\\\n",
    "O & \\frac{\\vec{a_{k+1}}}{\\sqrt{a_{k+1, k+1}}} & I_{n-k-1}\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\left(\n",
    "\\begin{array}{ccc}\n",
    "I_k & \\vec{0} & \\ast \\\\\n",
    "\\vec{0}^T & a_{k+1, k+1} & \\vec{a_{k+1}}^T \\\\\n",
    "O & \\vec{a_{k+1}} & \\ast\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\left(\n",
    "\\begin{array}{ccc}\n",
    "I_k & \\vec{0} & O \\\\\n",
    "\\vec{0}^T & \\sqrt{a_{k+1, k+1}} & \\vec{0}^T \\\\\n",
    "O & \\frac{\\vec{a_{k+1}}}{\\sqrt{a_{k+1, k+1}}} & I_{n-k-1}\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LU分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "行列$A$を下三角行列$L$と上三角行列$U$に分解することを考える。つまり\n",
    "\\begin{eqnarray*}\n",
    "A = L U\n",
    "\\end{eqnarray*}\n",
    "となるような$L, U$を求める。適当な条件下でLU分解可能であるが、自由度が存在するため、ある成分を固定する。クラウト法では$L$の対角成分を1とし、外積形式ガウス法では$U$の対角成分を1として$L, U$を求める。\n",
    "まず、クラウト法では\n",
    "\\begin{eqnarray*}\n",
    "a_{ii} = l_{ii} u_{ii} = u_{ii} \\\\\n",
    "a_{ij} = \\Sigma_k l_{ik} u_{kj} = l_{il} u_{lj} + \\Sigma_k l_{ik} u_{kj}\n",
    "\\end{eqnarray*}\n",
    "となるので、\n",
    "\\begin{eqnarray*}\n",
    "u_{ii} &=& a_{ii} \\\\\n",
    "l_{ij} &=& (a_{ij} - \\Sigma_k l_{ik} u_{kj}) / u_{jj} \\\\\n",
    "u_{ij} &=& a_{ij} - \\Sigma_k l_{ik} u_{kj}\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lu_factorization(A):\n",
    "    A = A.copy()\n",
    "    L = np.zeros_like(A).astype(np.float64)\n",
    "    U = np.zeros_like(A).astype(np.float64)\n",
    "    L[0:1, 0:1] = 1.0\n",
    "    U[0:1, 0:1] = A[0, 0]\n",
    "    if A.shape[0] > 1:\n",
    "        L[1:, 0:1] = 1.0 * A[1:, 0:1] / A[0, 0]\n",
    "        U[0:1, 1:] = A[0:1, 1:]\n",
    "        L[1:, 1:], U[1:, 1:] = lu_factorization(A[1:, 1:] - np.dot(L[1:, 0:1], U[0:1, 1:]))\n",
    "    return L, U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lu_factorization_with_pivoting(A):\n",
    "    P = np.eye(A.shape[0], A.shape[1])\n",
    "    L = np.eye(A.shape[0], A.shape[1])\n",
    "    U = A.copy()\n",
    "    \n",
    "    for k in range(A.shape[0]-1):\n",
    "        l = np.argmax(U[k:, k]) + k\n",
    "\n",
    "        p = np.eye(A.shape[0], A.shape[1])\n",
    "        p[k, k] = 0.0; p[l, l] = 0.0; p[k, l] = 1.0; p[l, k] = 1.0\n",
    "        P = np.dot(p, P)\n",
    "        \n",
    "        L[k, :k], L[l, :k] = L[l, :k].copy(), L[k, :k].copy()\n",
    "        U[k], U[l] = U[l].copy(), U[k].copy()\n",
    "        \n",
    "        L[k+1:, k:k+1] = U[k+1:, k:k+1] / U[k, k]\n",
    "        U[k+1:, k:] -= L[k+1:, k:k+1] * U[k:k+1, k:]\n",
    "        \n",
    "    return P, L, U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QR分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "行列$A$を直交行列$Q$と上三角行列$R$に分解することを考える。つまり\n",
    "\\begin{eqnarray*}\n",
    "A = Q R\n",
    "\\end{eqnarray*}\n",
    "となるような$Q, R$を求める。QR分解ではハウスホルダー変換を利用する。ハウスホルダー変換はある超平面に関する鏡映変換である。その超平面に直交する単位ベクトルを$\\vec{v}$とすると、\n",
    "\\begin{eqnarray*}\n",
    "H(\\vec{v}) \\equiv I - 2 \\vec{v} \\vec{v}^T\n",
    "\\end{eqnarray*}\n",
    "はこの変換の表現行列である。これをハウスホルダー行列と言う。   \n",
    "$A = (\\vec{a}_1, \\dots, \\vec{a}_n)$として\n",
    "\\begin{eqnarray*}\n",
    "\\vec{x} &\\equiv& \\vec{a}_1 \\\\\n",
    "\\vec{y} &\\equiv& (||\\vec{x}||, 0, \\dots, 0)^T \\\\\n",
    "\\vec{v} &\\equiv& \\frac{x - y}{|| x-y ||}\n",
    "\\end{eqnarray*}\n",
    "と定義する。このとき、この$\\vec{v}$に関するハウスホルダー行列$H$を考えると\n",
    "\\begin{eqnarray*}\n",
    "H x &=& y \\\\\n",
    "H y &=& x\n",
    "\\end{eqnarray*}\n",
    "が成立する。  \n",
    "$A$に直交行列を左からかけることで上三角行列に変換することを考える。$k$ステップ目の$R$を$R_k$とおき、$k$ステップ目にかける直交行列を$Q_k$とする。$R_k$の$k$次首座小行列が上三角であると仮定し、$k+1$行$k+1$列以降の行列を$R'_k$とおく。この行列$R'_k$に対して上のようにハウスホルダー行列を考えることができるので、これを$H_k$とおく。  \n",
    "ここで$Q_k$を\n",
    "\\begin{eqnarray*}\n",
    "Q_k = \n",
    "\\left(\n",
    "\\begin{array}{cc}\n",
    "I_k & O_{k, n-k} \\\\\n",
    "O_{n-k, k} & H_k\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\end{eqnarray*}\n",
    "と定義すると、$Q_k$は直交行列になる。また\n",
    "\\begin{eqnarray*}\n",
    "Q_k R_k =\n",
    "\\left(\n",
    "\\begin{array}{cc}\n",
    "I_k & O_{k, n-k} \\\\\n",
    "O_{n-k, k} & H_k\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\left(\n",
    "\\begin{array}\n",
    "I\\ast & \\ast \\\\\n",
    "\\ast & R'_k\n",
    "\\end{array}\n",
    "\\right) =\n",
    "\\left(\n",
    "\\begin{array}\n",
    "I\\ast & \\ast \\\\\n",
    "O_{n-k, k} & H_k R'_k\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\end{eqnarray*}\n",
    "が成立する。k+1列について、k+2行以降は0となっているので、全体として$k+1$次首座小行列が上三角行列になっている。これを$R_{k+1}$とすれば帰納的に$R_n$を求めることができる。  \n",
    "以上の考察より、$R_n = Q_n Q_{n-1} \\dots Q_1 A$となっているから、\n",
    "\\begin{eqnarray*}\n",
    "Q &\\equiv& Q_1^T Q_2^T \\dots Q_n^T \\\\\n",
    "R &\\equiv& R_n\n",
    "\\end{eqnarray*}\n",
    "とすることで\n",
    "\\begin{eqnarray*}\n",
    "A = Q R\n",
    "\\end{eqnarray*}\n",
    "という分解を実現できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_norm(x, p=2):\n",
    "    return np.sum(x**p)**(1.0/p)\n",
    "\n",
    "def householder_matrix(x, y):\n",
    "    w = x - y\n",
    "    H_k = np.eye(x.shape[0], y.shape[0]) - 2.0 * np.dot(w, w.T) / np.dot(w.T, w)\n",
    "    return H_k\n",
    "\n",
    "def transform_matrix(k, A):\n",
    "    x = A[:, 0:1]\n",
    "    y = np.zeros_like(x); y[0] = vector_norm(x)\n",
    "    H_k = np.zeros((A.shape[0]+k, A.shape[1]+k))\n",
    "    H_k[:k, :k] = np.eye(k, k)\n",
    "    H_k[k:, k:] = householder_matrix(x, y)\n",
    "    return H_k\n",
    "\n",
    "def qr_factorization(A):\n",
    "    Q = np.eye(A.shape[0], A.shape[1])\n",
    "    R = A.copy()\n",
    "    A_k = A.copy()\n",
    "    for k in range(A.shape[0]-1):\n",
    "        H_k = transform_matrix(k, A_k)\n",
    "        Q = np.dot(Q, H_k.T)\n",
    "        R = np.dot(H_k, R)\n",
    "        A_k = R[1:, 1:]\n",
    "    return Q, R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# コレスキー分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A$を正定値対称行列とすると、LU分解の特殊な場合として記述できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A$を正定値対称行列とする。このとき、上三角行列$L$を用いて\n",
    "\\begin{eqnarray*}\n",
    "A = L L^T\n",
    "\\end{eqnarray*}\n",
    "と分解することができる。(必要十分条件である。)  \n",
    "$A$に下三角行列を左右からかけることで単位行列にすることを考える。$k$ステップ目に左右からかける行列を$L_k, L_k^T$とし、そのときの$A$を$A_k$とおく。$A_k$の$k$次首座小行列が単位行列になっていると仮定する。$A_k$を\n",
    "\\begin{eqnarray*}\n",
    "A_k = \n",
    "\\left(\n",
    "\\begin{array}{ccc}\n",
    "I_k & \\vec{0} & \\ast \\\\\n",
    "\\vec{0}^T & a_{k+1, k+1} & \\vec{a_{k+1}}^T \\\\\n",
    "O & \\vec{a_{k+1}} & \\ast\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\end{eqnarray*}\n",
    "とすると、$L_k$として\n",
    "\\begin{eqnarray*}\n",
    "L_k = \n",
    "\\left(\n",
    "\\begin{array}{ccc}\n",
    "I_k & \\vec{0} & O \\\\\n",
    "\\vec{0}^T & \\frac{1}{\\sqrt{a_{k+1, k+1}}} & \\vec{0}^T \\\\\n",
    "O & - \\sqrt{a_{k+1, k+1}} \\vec{a_{k+1}} & I_{n-k-1}\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\end{eqnarray*}\n",
    "を選ぶ。すると\n",
    "\\begin{eqnarray*}\n",
    "L_k A_k L^T_k = \n",
    "\\left(\n",
    "\\begin{array}{ccc}\n",
    "I_k & \\vec{0} & O \\\\\n",
    "\\vec{0}^T & \\frac{1}{\\sqrt{a_{k+1, k+1}}} & \\vec{0}^T \\\\\n",
    "O & - \\sqrt{a_{k+1, k+1}} \\vec{a_{k+1}} & I_{n-k-1}\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\left(\n",
    "\\begin{array}{ccc}\n",
    "I_k & \\vec{0} & \\ast \\\\\n",
    "\\vec{0}^T & a_{k+1, k+1} & \\vec{a_{k+1}}^T \\\\\n",
    "O & \\vec{a_{k+1}} & \\ast\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\left(\n",
    "\\begin{array}{ccc}\n",
    "I_k & \\vec{0} & O \\\\\n",
    "\\vec{0}^T & \\frac{1}{\\sqrt{a_{k+1, k+1}}} & - \\sqrt{a_{k+1, k+1}} \\vec{a_{k+1}}^T \\\\\n",
    "O & \\vec{0} & I_{n-k-1}\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QR法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qr_method(A, limit=10):\n",
    "    A_k = A.copy()\n",
    "    for _ in range(limit):\n",
    "        Q_k, R_k = qr_factorization(A_k)\n",
    "        A_k = np.dot(R_k, Q_k)\n",
    "    return A_k\n",
    "\n",
    "def eigen_values(A, limit=10):\n",
    "    upper_triangle = qr_method(A)\n",
    "    eigen_vals = [upper_triangle[i][i] for i in range(upper_triangle.shape[0])]\n",
    "    return eigen_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 冪乗法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    n = vector_norm(x)\n",
    "    return x / n\n",
    "\n",
    "def power_method(A, limit=10):\n",
    "    x_k = np.random.randn(A.shape[0])\n",
    "    for _ in range(limit):\n",
    "        y_k = A.dot(x_k)\n",
    "        x_k = normalize(y_k)\n",
    "    numerator = x_k.T.dot(A).dot(x_k)\n",
    "    denominator = x_k.T.dot(x_k)\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!!その他のアルゴリズム(反復法など)!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
