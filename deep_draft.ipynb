{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial}{\\partial x} x + y &=& 1 \\\\\n",
    "\\frac{\\partial}{\\partial y} x + y &=& 1 \\\\\n",
    "\\frac{\\partial}{\\partial x} x * y &=& y \\\\\n",
    "\\frac{\\partial}{\\partial y} x * y &=& x\n",
    "\\end{eqnarray*}\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "Z = X + Y \\\\\n",
    "(\\frac{\\partial}{\\partial X} L(Z))_{ij} &=&\n",
    "\\Sigma_{kl} \\frac{\\partial}{\\partial z_{kl}} L(Z) \\frac{\\partial}{\\partial x_{ij}} z_{kl} =\n",
    "\\Sigma_{kl} \\frac{\\partial}{\\partial z_{kl}} L(Z) \\frac{\\partial}{\\partial x_{ij}} (x_{kl} + y_{kl}) =\n",
    "\\frac{\\partial}{\\partial z_{ij}} L(Z) =\n",
    "(\\frac{\\partial}{\\partial Z} L(Z))_{ij} \\\\\n",
    "Z = X Y \\\\\n",
    "(\\frac{\\partial}{\\partial X} L(Z))_{ij} &=&\n",
    "\\Sigma_{kl} \\frac{\\partial}{\\partial z_{kl}} L(Z) \\frac{\\partial}{\\partial x_{ij}} z_{kl} =\n",
    "\\Sigma_{kl} \\frac{\\partial}{\\partial z_{kl}} L(Z) \\frac{\\partial}{\\partial x_{ij}} (\\Sigma_m x_{km} y_{ml}) =\n",
    "\\Sigma_{l} \\frac{\\partial}{\\partial z_{il}} L(Z) y_{jl} =\n",
    "(\\frac{\\partial}{\\partial Z} L(Z) Y^T)_{ij} \\\\\n",
    "(\\frac{\\partial}{\\partial Y} L(Z))_{ij} &=&\n",
    "\\Sigma_{kl} \\frac{\\partial}{\\partial z_{kl}} L(Z) \\frac{\\partial}{\\partial y_{ij}} z_{kl} =\n",
    "\\Sigma_{kl} \\frac{\\partial}{\\partial z_{kl}} L(Z) \\frac{\\partial}{\\partial y_{ij}} (\\Sigma_m x_{km} y_{ml}) =\n",
    "\\Sigma_{k} \\frac{\\partial}{\\partial z_{kj}} L(Z) x_{ki} =\n",
    "(X^T \\frac{\\partial}{\\partial Z} L(Z))_{ij}\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.76405235]\n",
      " [ 0.40015721]\n",
      " [ 0.97873798]\n",
      " [ 2.2408932 ]\n",
      " [ 1.86755799]\n",
      " [-0.97727788]\n",
      " [ 0.95008842]\n",
      " [-0.15135721]\n",
      " [-0.10321885]\n",
      " [ 0.4105985 ]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "x = np.random.randn(10, 1)\n",
    "y = np.where(x > 0, 1., 0.)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bcf5973e24d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cross_entropy_error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Loss' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "loss = Loss(\"cross_entropy_error\")\n",
    "a = Input(input_shape=1)\n",
    "b = Dense(a, input_shape=1, output_shape=2, learning_rate=0.1, verbose=False)\n",
    "c = Sigmoid(b, shape=2)\n",
    "d = Dense(c, input_shape=2, output_shape=1, learning_rate=0.1, verbose=False)\n",
    "e = Sigmoid(d, shape=1)\n",
    "f = Output(e, loss_forward=loss.forward, loss_backward=loss.backward, output_shape=1, verbose=False)\n",
    "for _ in range(1000):\n",
    "    result = f.forward(x, y)\n",
    "    f.backward()\n",
    "    f.update()\n",
    "print(f.predict(x))\n",
    "np.mean(cross_entropy_error(y, f.predict(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray*}\n",
    "\\frac{d L}{d W}\n",
    "&=& \\frac{\\partial L}{\\partial h_n} \\frac{d h_n}{d W} \\\\\n",
    "&=& \\frac{\\partial L}{\\partial h_n} (\\frac{\\partial h_n}{\\partial W} + \\frac{\\partial h_n}{\\partial h_{n-1}} \\frac{d h_{n-1}}{d W})\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray*}\n",
    "h = W x \\\\\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial h} \\frac{\\partial h}{\\partial W} \\\\\n",
    "\\frac{\\partial L}{\\partial h} = dout \\\\\n",
    "\\frac{\\partial h}{\\partial W} = \n",
    "\\left(\n",
    "\\begin{array}{cc}\n",
    "\\frac{\\partial h}{\\partial w_{11}} & \\frac{\\partial h}{\\partial w_{1n}} \\\\\n",
    "\\frac{\\partial h}{\\partial w_{n1}} & \\frac{\\partial h}{\\partial w_{nn}} \\\\\n",
    "\\end{array}\n",
    "\\right) =\n",
    "\\left(\n",
    "\\begin{array}{cc}\n",
    "\\left( \\begin{array}{c} x_1 \\\\ 0 \\\\ \\end{array} \\right) & \\left( \\begin{array}{c} x_n \\\\ 0 \\\\ \\end{array} \\right) \\\\\n",
    "\\left( \\begin{array}{c} 0 \\\\ x_1 \\\\ \\end{array} \\right) & \\left( \\begin{array}{c} 0 \\\\ x_n \\\\ \\end{array} \\right) \\\\\n",
    "\\end{array}\n",
    "\\right) \\\\\n",
    "\\frac{\\partial L}{\\partial W} = dout_1 \\left( \\begin{array}{c} x_1 \\\\ 0 \\\\ \\end{array} \\right) & \\left( \\begin{array}{c} x_n \\\\ 0 \\\\ \\end{array} \\right) + dout_n \\left( \\begin{array}{c} 0 \\\\ x_1 \\\\ \\end{array} \\right) & \\left( \\begin{array}{c} 0 \\\\ x_n \\\\ \\end{array} \\right)\n",
    "= \\left( \\begin{array}{c} dout_1 x^T \\\\ dout_n x^T \\\\ \\end{array} \\right)\n",
    "= dout x^T \n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(- x))\n",
    "\n",
    "def mean_squared_error(answer, predict):\n",
    "    return (answer - predict)**2 / 2\n",
    "\n",
    "def cross_entropy_error(answer, predict):\n",
    "    return - (answer * np.log2(predict) + (1 - answer) * np.log2(1 - predict))\n",
    "\n",
    "class Input:\n",
    "    def __init__(self, input_shape, verbose=False):\n",
    "        self.output_shape = input_shape\n",
    "        self.output = None\n",
    "        self.layer = None\n",
    "        self.verbose = verbose\n",
    "        if (self.verbose):\n",
    "            print(\"init@\", self)\n",
    "    def forward(self, x):\n",
    "        self.output = x\n",
    "        if (self.verbose):\n",
    "            print(\"forward@\", self, \" and return value is\")\n",
    "            print(self.output)\n",
    "        return self.output\n",
    "    def backward(self, dy):\n",
    "        if (self.verbose):\n",
    "            print(\"backward@\", self, \" and return value is\")\n",
    "            print(dy)\n",
    "        return dy\n",
    "    def update(self):\n",
    "        pass\n",
    "    \n",
    "class Output:\n",
    "    def __init__(self, layer, loss_forward, loss_backward, output_shape, verbose=False):\n",
    "        self.input_shape = None\n",
    "        self.output_shape = output_shape\n",
    "        self.pred = None\n",
    "        self.answer = None\n",
    "        self.loss_forward = loss_forward\n",
    "        self.loss_backward = loss_backward\n",
    "        self.layer = layer\n",
    "        self.verbose = verbose\n",
    "        if (self.verbose):\n",
    "            print(\"init@\", self)\n",
    "    def forward(self, x, t):\n",
    "        self.pred = self.layer.forward(x)\n",
    "        self.answer = t\n",
    "        loss = self.loss_forward(self.answer, self.pred)\n",
    "        if (self.verbose):\n",
    "            print(\"forward@\", self, \" and predict is\")\n",
    "            print(self.pred)\n",
    "            print(\"and loss is \", loss)\n",
    "        return loss\n",
    "    def backward(self):\n",
    "        dy = self.loss_backward()\n",
    "        if (self.verbose):\n",
    "            print(\"backward@\", self, \" and return value is\")\n",
    "            print(dy)\n",
    "        return self.layer.backward(dy)\n",
    "    def update(self):\n",
    "        self.layer.update()\n",
    "    def predict(self, x):\n",
    "        y = self.layer.forward(x)\n",
    "        if (self.verbose):\n",
    "            print(\"predict@\", self, \" and return value is\")\n",
    "            print(y)\n",
    "        return y\n",
    "\n",
    "# TODO: RNNに対応する\n",
    "class Dense:\n",
    "    def __init__(self, layer, input_shape, output_shape, learning_rate=0.01, sequential=False, verbose=False):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.layer = layer\n",
    "        self.x = None\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.lr = learning_rate\n",
    "        self.seq = sequential\n",
    "        self.verbose = verbose\n",
    "        if (self.verbose):\n",
    "            print(\"init@\", self)\n",
    "    # NOTE: (batch_size, input_shape) -> (batch_size, output_shape)\n",
    "    def forward(self, x):\n",
    "        self.x = self.layer.forward(x)\n",
    "        if (self.seq):\n",
    "            batch_size, seq_length, _ = self.x.shape\n",
    "            if (self.W is None and self.b is None):\n",
    "                self.W = np.random.randn(seq_length, self.output_shape, self.input_shape) * 0.01\n",
    "                self.b = np.random.randn(seq_length, self.output_shape) * 0.01\n",
    "            y = np.array([\n",
    "                np.dot(self.x[:, t, :], self.W[t, :, :].T) + np.tile(self.b[t, :], (batch_size,1))\n",
    "                for t in range(seq_length)]).transpose((1,0,2))\n",
    "        else:\n",
    "            batch_size, _ = self.x.shape\n",
    "            if (self.W is None and self.b is None):\n",
    "                self.W = np.random.randn(self.output_shape, self.input_shape) * 0.01\n",
    "                self.b = np.random.randn(1, self.output_shape) * 0.01\n",
    "            y = np.dot(self.x, self.W.T) + np.repeat(self.b, batch_size, axis=0)\n",
    "        if (self.verbose):\n",
    "            print(\"forward@\", self, \" and return value is\")\n",
    "            print(y)\n",
    "        return y\n",
    "    # NOTE: (batch_size, output_shape) -> (batch_size, input_shape)\n",
    "    def backward(self, dy):\n",
    "        if (self.seq):\n",
    "            # sum up loss\n",
    "            _, seq_length, _ = dy.shape\n",
    "            self.dW = np.array([np.dot(dy[:, t, :].T, self.x[:, t, :]) for t in range(seq_length)])\n",
    "            self.db = np.sum(dy, axis=0)\n",
    "            dx = np.array([np.dot(dy[:, t, :], self.W[t, :, :]) for t in range(seq_length)]).transpose((1,0,2))\n",
    "        else:\n",
    "            # sum up loss\n",
    "            self.dW = np.dot(dy.T, self.x)\n",
    "            self.db = np.sum(dy, axis=0, keepdims=True)\n",
    "            dx = np.dot(dy, self.W)\n",
    "        if (self.verbose):\n",
    "            print(\"backward@\", self, \" and return value is\")\n",
    "            print(self.dW)\n",
    "        return self.layer.backward(dx)\n",
    "    def update(self):\n",
    "        if (self.verbose):\n",
    "            print(\"update@\", self, \" with\")\n",
    "            print(\"self.W\")\n",
    "            print(self.W)\n",
    "            print(\"self.b\")\n",
    "            print(self.b)\n",
    "        self.W -= self.lr * self.dW\n",
    "        self.b -= self.lr * self.db\n",
    "        if (self.verbose):\n",
    "            print(\"self.W\")\n",
    "            print(self.W)\n",
    "            print(\"self.b\")\n",
    "            print(self.b)\n",
    "        self.layer.update()\n",
    "    \n",
    "# TODO: インターフェースを整える\n",
    "class Relu:\n",
    "    def __init__(self, layer, shape):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.layer = layer\n",
    "    def forward(self, x):\n",
    "        y = self.layer.forward(x)\n",
    "        z = y.copy()\n",
    "        z[z <= 0] = 0\n",
    "        self.input = y\n",
    "        self.output = z\n",
    "        return z\n",
    "    def backward(self, dx):\n",
    "        dy = dx.copy()\n",
    "        dy[self.input <= 0] = 0\n",
    "        return self.layer.backward(dy)\n",
    "        \n",
    "class Sigmoid:\n",
    "    def __init__(self, layer, shape):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.layer = layer\n",
    "    def forward(self, x):\n",
    "        self.input = self.layer.forward(x)\n",
    "        self.output = 1 / (1 + np.exp(- self.input))\n",
    "        return self.output\n",
    "    def backward(self, dx):\n",
    "        dy = dx * self.output * (1 - self.output)\n",
    "        return self.layer.backward(dy)\n",
    "    def update(self):\n",
    "        self.layer.update()\n",
    "    \n",
    "class Loss:\n",
    "    def __init__(self, method, sequential=False, verbose=False):\n",
    "        self.method = method\n",
    "        self.pred = None\n",
    "        self.answer = None\n",
    "        self.seq = sequential\n",
    "        self.verbose = verbose\n",
    "    def forward(self, y_ans, y_pred):\n",
    "        self.pred = y_pred\n",
    "        self.answer = y_ans\n",
    "        if (self.seq):\n",
    "            if (self.method == \"mean_squared_error\"):\n",
    "                result = np.mean((y_ans - y_pred) ** 2 / 2.0, axis=1)\n",
    "            elif (self.method == \"cross_entropy_error\"):\n",
    "                result = - np.mean((y_ans * np.log2(y_pred) + (1.0 - y_ans) * np.log2(1.0 - y_pred)), axis=1)\n",
    "            else:\n",
    "                result = None\n",
    "        else:\n",
    "            if (self.method == \"mean_squared_error\"):\n",
    "                result = np.mean((y_ans - y_pred) ** 2) / 2.0\n",
    "            elif (self.method == \"cross_entropy_error\"):\n",
    "                result = - np.mean(y_ans * np.log2(y_pred) + (1.0 - y_ans) * np.log2(1.0 - y_pred))\n",
    "            else:\n",
    "                result = None\n",
    "        if (self.verbose):\n",
    "            print(\"forward@\", self, \" and return value is \")\n",
    "            print(result)\n",
    "        return result\n",
    "    def backward(self):\n",
    "        if (self.seq):\n",
    "            _, seq_length, _ = self.pred.shape\n",
    "            if (self.method == \"mean_squared_error\"):\n",
    "                result = (self.pred - self.answer) / seq_length\n",
    "            elif (self.method == \"cross_entropy_error\"):\n",
    "                result = - (self.pred - self.answer) / ((self.pred * (self.pred - 1.0)) * seq_length)\n",
    "            else:\n",
    "                result = None\n",
    "        else:\n",
    "            if (self.method == \"mean_squared_error\"):\n",
    "                result = self.pred - self.answer\n",
    "            elif (self.method == \"cross_entropy_error\"):\n",
    "                result = - (self.pred - self.answer) / (self.pred * (self.pred - 1.0))\n",
    "            else:\n",
    "                result = None\n",
    "        if (self.verbose):\n",
    "            print(\"backward@\", self, \" and return value is\")\n",
    "            print(result)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# def identity(xs, n):\n",
    "#     batch_size, *shape = xs.shape\n",
    "#     result = np.zeros((batch_size, n, n, *shape))\n",
    "#     for b in range(batch_size):\n",
    "#         for i in range(n):\n",
    "#             result[b, i, i] = xs[b]\n",
    "#     return result\n",
    "\n",
    "def identity(x, n):\n",
    "    y = np.zeros((n, n, *x.shape))\n",
    "    for i in range(n):\n",
    "        y[i, i, :] = x\n",
    "    return y\n",
    "\n",
    "def clipping(dxs, threshold):\n",
    "    norm = 0.0\n",
    "    for dx in dxs:\n",
    "        norm += np.sum(dx**2)\n",
    "    norm = np.sqrt(norm)\n",
    "    rate = threshold / norm if (norm >= threshold) else 1.0\n",
    "    dys = []\n",
    "    for dx in dxs:\n",
    "        dys.append(dx * rate)\n",
    "    return dys\n",
    "\n",
    "# identity(np.array([[1],[2]]), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 長い系列データはtruncateして学習するようにする\n",
    "# TODO: パディングを実装する\n",
    "# NOTE: そもそも時系列の部分はDenseのバッチ処理を行っているのでそのままバッチ処理して切れば良さそう\n",
    "# TODO: hの初期値を与えられるようにする\n",
    "class Recurrent():\n",
    "    def __init__(self, layer, input_shape, output_shape, learning_rate=0.001, clipping=None, verbose=False):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.x = None\n",
    "        self.h = None\n",
    "        self.W_x = np.random.randn(output_shape, input_shape) * 0.01\n",
    "        self.W_h = np.random.randn(output_shape, output_shape) * 0.01\n",
    "        self.b = np.random.randn(output_shape) * 0.01\n",
    "        self.dx = None\n",
    "        self.dh = None\n",
    "        self.dW_x = np.empty((output_shape, input_shape))\n",
    "        self.dW_h = np.empty((output_shape, output_shape))\n",
    "        self.db = np.empty(output_shape)\n",
    "        self.lr = learning_rate\n",
    "        self.layer = layer\n",
    "        self.clipping = clipping\n",
    "        self.verbose = verbose\n",
    "        if (self.verbose):\n",
    "            print(\"init@\", self)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        self.x = self.layer.forward(x)\n",
    "        batch_size, seq_length, _ = self.x.shape\n",
    "        self.h = np.empty((batch_size, seq_length, self.output_shape))\n",
    "        self.h[:, 0, :] = np.dot(self.layer.forward(self.x[:, 0, :]), self.W_x.T) + np.tile(self.b, (batch_size,1))\n",
    "        for t in range(1, seq_length):\n",
    "            self.h[:, t, :] = np.dot(self.layer.forward(self.x[:, t, :]), self.W_x.T) + np.dot(self.h[:, t-1, :], self.W_h.T) + np.tile(self.b, (batch_size,1))\n",
    "        if (self.verbose):\n",
    "            print(\"forward@\", self, \" and return value is\")\n",
    "            print(self.h)\n",
    "        return self.h\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        batch_size, seq_length, _ = dy.shape\n",
    "        self.dx = np.empty((batch_size, seq_length, self.input_shape))\n",
    "        self.dh = np.empty((batch_size, seq_length, self.output_shape))\n",
    "        # dx, dh\n",
    "        self.dx[:, -1, :] = np.dot(dy[:, -1, :], self.W_x)\n",
    "        self.dh[:, -1, :] = np.dot(dy[:, -1, :], self.W_h)\n",
    "        for t in reversed(range(seq_length-1)):\n",
    "            self.dx[:, t, :] = np.dot(dy[:, t, :] + self.dh[:, t+1, :], self.W_x)\n",
    "            self.dh[:, t, :] = np.dot(dy[:, t, :] + self.dh[:, t+1, :], self.W_h)\n",
    "        # dW_x, dW_h, db\n",
    "        # NOTE: 変数のdW_xは\\frac{\\partial h}{\\partial W}であることに注意する(名前を変えよう)\n",
    "        dW_x = np.array([identity(self.x[n, 0, :], self.output_shape) for n in range(batch_size)])\n",
    "        dW_h = np.array([identity(self.h[n, 0, :], self.output_shape) for n in range(batch_size)])\n",
    "        db = np.array([np.eye(self.output_shape) for n in range(batch_size)])\n",
    "        for t in range(1, seq_length):\n",
    "            dW_x = np.array([identity(self.x[n, t, :], self.output_shape) + np.dot(self.W_h, dW_x[n, ...]) for n in range(batch_size)])\n",
    "            dW_h = np.array([identity(self.h[n, t, :], self.output_shape) + np.dot(self.W_h, dW_h[n, ...]) for n in range(batch_size)])\n",
    "            db = np.array([np.eye(self.output_shape) + np.dot(self.W_h, db[n, ...]) for n in range(batch_size)])\n",
    "        self.dW_x = np.sum([np.dot(self.dh[n, -1, :], dW_x[n, ...]) for n in range(batch_size)], axis=0)\n",
    "        self.dW_h = np.sum([np.dot(self.dh[n, -1, :], dW_h[n, ...]) for n in range(batch_size)], axis=0)\n",
    "        self.db = np.sum([np.dot(self.dh[n, -1, :], db[n, ...]) for n in range(batch_size)], axis=0)\n",
    "        if (self.verbose):\n",
    "            print(\"bakcward@\", self, \" and return value is \")\n",
    "            print(np.sum(self.dW_x**2))\n",
    "        return self.layer.backward(self.dx)\n",
    "    \n",
    "    def update(self):\n",
    "        if (self.clipping):\n",
    "            self.dW_x, self.dW_h, self.b = clipping([self.dW_x, self.dW_h, self.b], self.clipping)  \n",
    "        if (self.verbose):\n",
    "            print(\"self.W_x\")\n",
    "            print(self.W_x)\n",
    "            print(\"self.W_h\")\n",
    "            print(self.W_h)\n",
    "            print(\"self.b\")\n",
    "            print(self.b)\n",
    "        self.W_x -= self.lr * self.dW_x\n",
    "        self.W_h -= self.lr * self.dW_h\n",
    "        self.b -= self.lr * self.db\n",
    "        if (self.verbose):\n",
    "            print(\"self.W_x\")\n",
    "            print(self.W_x)\n",
    "            print(\"self.W_h\")\n",
    "            print(self.W_h)\n",
    "            print(\"self.b\")\n",
    "            print(self.b)\n",
    "        self.layer.update()\n",
    "        \n",
    "            \n",
    "    # DEBUG:\n",
    "    def print_shape(self):\n",
    "        print(\"xs: \", self.xs.shape)\n",
    "        print(\"hs: \", self.hs.shape)\n",
    "        print(\"W_x: \", self.W_x.shape)\n",
    "        print(\"W_h: \", self.W_h.shape)\n",
    "        print(\"b: \", self.b.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 4, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = np.arange(24, dtype=np.float64).reshape(2, 3, 4)\n",
    "# dx = np.arange(18, dtype=np.float64).reshape(2, 3, 3)\n",
    "# y = np.arange(6, dtype=np.float64).reshape(2, 3, 1)\n",
    "x = np.arange(100).reshape(20,5,1)[:, :-1]\n",
    "y = np.arange(100).reshape(20,5,1)[:, 1:]\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq:\n",
    "    def __init__(self, encoder, decoder, verbose=False):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "    def backward(self, dy):\n",
    "        pass\n",
    "    def update(self):\n",
    "        self.encoder.update()\n",
    "        self.decoder.update()\n",
    "    \n",
    "# TODO: ミニバッチ学習を実装する\n",
    "class Network:\n",
    "    def __init__(self, layer, verbose=False):\n",
    "        self.layer = layer\n",
    "        self.verbose = verbose\n",
    "        self.loss = None\n",
    "    def fit(self, x, y, batch_size=10, epoch=1, random_state=None):\n",
    "        self.loss = np.zeros(epoch)\n",
    "        if (self.verbose):\n",
    "            print(\"fit@\", self)\n",
    "        \n",
    "        data_size = x.shape[0]\n",
    "        iteration = data_size // batch_size\n",
    "        \n",
    "        np.random.seed(random_state)\n",
    "        for e in range(epoch):\n",
    "            if (not (random_state is None)):\n",
    "                index = np.random.permutation(data_size)\n",
    "                x_data, y_data = x[index], y[index]\n",
    "            else:\n",
    "                x_data, y_data = x, y\n",
    "            for i in range(iteration):\n",
    "                x_batch, y_batch = x_data[batch_size*i:batch_size*(i+1)], y_data[batch_size*i:batch_size*(i+1)]\n",
    "                loss = self.layer.forward(x_batch, y_batch)\n",
    "                self.loss[e] += np.sum(loss)\n",
    "                self.layer.backward()\n",
    "                self.layer.update()\n",
    "            if (self.verbose):\n",
    "                print(\"%d\\r\" % e, end=\"\")\n",
    "        if (self.verbose):\n",
    "            print(self.loss)\n",
    "            plt.xlim([0, epoch])\n",
    "            plt.ylim([0, 100])\n",
    "            plt.plot(self.loss)\n",
    "#                     print(\"loss(epoch %d, iter %d)\" % (e, i))\n",
    "#                     print(loss)\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_pred = self.layer.predict(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit@ <__main__.Network object at 0x11bc5fe90>\n",
      "[1.16812183e+04 6.12325550e+02 5.75878118e+02 3.93935511e+02\n",
      " 2.39438704e+02 2.18867136e+02 1.58354157e+02 1.11833254e+02\n",
      " 1.32144739e+02 1.27702029e+02 5.90967201e+01 6.41473448e+01\n",
      " 4.58680207e+01 4.52000515e+01 4.04749080e+01 3.81073463e+01\n",
      " 4.06353466e+01 3.54563977e+01 3.63294260e+01 2.90196100e+01\n",
      " 3.03030622e+01 2.97966662e+01 3.03752326e+01 3.52672544e+01\n",
      " 3.20316472e+01 2.90179981e+01 2.49762851e+01 2.72261102e+01\n",
      " 2.75598862e+01 2.48130031e+01 2.44973524e+01 2.36294378e+01\n",
      " 2.34200462e+01 2.49449909e+01 2.40172666e+01 2.46793194e+01\n",
      " 2.31226703e+01 2.48411839e+01 2.21415145e+01 2.41849494e+01\n",
      " 2.29003776e+01 2.21607839e+01 2.52627030e+01 2.18438959e+01\n",
      " 2.27377491e+01 2.26953257e+01 2.07465795e+01 2.12625220e+01\n",
      " 2.03324851e+01 2.17391447e+01 2.00052372e+01 2.25554953e+01\n",
      " 1.99069316e+01 2.02612818e+01 2.04527386e+01 1.90589873e+01\n",
      " 2.13737104e+01 1.91956847e+01 2.10513959e+01 2.03626660e+01\n",
      " 2.02971349e+01 1.92388539e+01 1.82685175e+01 1.90114364e+01\n",
      " 1.94304161e+01 1.86413230e+01 1.90859563e+01 1.88040533e+01\n",
      " 1.82062807e+01 1.84617572e+01 1.76717596e+01 1.73186458e+01\n",
      " 1.82499924e+01 1.70941950e+01 1.73896121e+01 1.89313710e+01\n",
      " 1.65345312e+01 1.74443942e+01 1.67480698e+01 1.72362184e+01\n",
      " 1.61116263e+01 1.76911453e+01 1.61725412e+01 1.67748437e+01\n",
      " 1.57128294e+01 1.59298505e+01 1.74363415e+01 1.62887002e+01\n",
      " 1.55799368e+01 1.58708530e+01 1.57492342e+01 1.64985221e+01\n",
      " 1.48408942e+01 1.53803122e+01 1.53123305e+01 1.52587976e+01\n",
      " 1.49588894e+01 1.51293832e+01 1.58489856e+01 1.60547411e+01\n",
      " 1.48632164e+01 1.52789303e+01 1.53980892e+01 1.50971644e+01\n",
      " 1.44595610e+01 1.51034648e+01 1.51212169e+01 1.58298743e+01\n",
      " 1.44556445e+01 1.44312313e+01 1.48779746e+01 1.55554835e+01\n",
      " 1.40664857e+01 1.43000793e+01 1.47283393e+01 1.37387916e+01\n",
      " 1.46920470e+01 1.40141294e+01 1.44659717e+01 1.38401208e+01\n",
      " 1.37844531e+01 1.34406654e+01 1.34808073e+01 1.36594681e+01\n",
      " 1.41922637e+01 1.35499302e+01 1.35935320e+01 1.31360279e+01\n",
      " 1.36863274e+01 1.34898833e+01 1.45921682e+01 1.35504040e+01\n",
      " 1.32473740e+01 1.26548606e+01 1.28150424e+01 1.23360835e+01\n",
      " 1.30550603e+01 1.35753116e+01 1.31620094e+01 1.27903879e+01\n",
      " 1.22366282e+01 1.35289932e+01 1.25272674e+01 1.28339394e+01\n",
      " 1.25885862e+01 1.21675360e+01 1.26512057e+01 1.23189682e+01\n",
      " 1.25366029e+01 1.23250351e+01 1.27391086e+01 1.23613682e+01\n",
      " 1.25844824e+01 1.22452366e+01 1.23720835e+01 1.28055548e+01\n",
      " 1.22227314e+01 1.24331785e+01 1.18531161e+01 1.17971705e+01\n",
      " 1.19720004e+01 1.18106568e+01 1.18877782e+01 1.25367701e+01\n",
      " 1.21118955e+01 1.20975653e+01 1.18807206e+01 1.18619129e+01\n",
      " 1.23889202e+01 1.20574253e+01 1.15852970e+01 1.26388986e+01\n",
      " 1.19638961e+01 1.12380448e+01 1.21180027e+01 1.16362343e+01\n",
      " 1.14745737e+01 1.16125668e+01 1.13966228e+01 1.18146177e+01\n",
      " 1.21184093e+01 1.09267353e+01 1.16983827e+01 1.16267513e+01\n",
      " 1.15407532e+01 1.14501802e+01 1.12080904e+01 1.11559509e+01\n",
      " 1.08800301e+01 1.12956212e+01 1.11952906e+01 1.11440653e+01\n",
      " 1.11018026e+01 1.14765519e+01 1.11533126e+01 1.11933882e+01\n",
      " 1.09903365e+01 1.12336858e+01 1.14540333e+01 1.09479113e+01\n",
      " 1.10114161e+01 1.15409847e+01 1.06732414e+01 1.10210678e+01\n",
      " 1.04633004e+01 1.09562966e+01 1.07889922e+01 1.10356017e+01\n",
      " 1.11590612e+01 1.07785522e+01 1.09515085e+01 1.08675579e+01\n",
      " 1.06836531e+01 1.08352801e+01 1.04823374e+01 1.05171474e+01\n",
      " 1.13424582e+01 1.07425326e+01 1.08240957e+01 1.08719144e+01\n",
      " 1.03831543e+01 1.04022181e+01 1.05674599e+01 1.07773940e+01\n",
      " 1.07622822e+01 1.02752634e+01 1.01805848e+01 1.06715633e+01\n",
      " 1.01359597e+01 1.01893059e+01 1.01716793e+01 1.04747153e+01\n",
      " 1.02387966e+01 1.03285564e+01 1.05133408e+01 1.07387835e+01\n",
      " 1.02989393e+01 9.97467571e+00 1.09826488e+01 1.05021418e+01\n",
      " 1.00229880e+01 1.03135886e+01 1.01894667e+01 1.02880034e+01\n",
      " 9.89176331e+00 1.02601198e+01 1.01270145e+01 9.92073825e+00\n",
      " 1.01595958e+01 1.08319532e+01 1.02049334e+01 1.06916908e+01\n",
      " 1.01436835e+01 1.00494555e+01 9.88208127e+00 1.03988828e+01\n",
      " 9.92411956e+00 1.05988966e+01 9.69454235e+00 1.01116590e+01\n",
      " 9.48206137e+00 1.02584704e+01 1.01252139e+01 9.78900421e+00\n",
      " 1.02118559e+01 9.72948130e+00 9.80966033e+00 9.66938499e+00\n",
      " 9.76508529e+00 9.43316339e+00 9.82368757e+00 9.91683757e+00\n",
      " 9.77869182e+00 9.73240722e+00 9.82663969e+00 9.82880273e+00\n",
      " 9.44045082e+00 9.82894134e+00 9.27650551e+00 9.48228733e+00\n",
      " 9.25096104e+00 9.46067796e+00 9.47591666e+00 9.71222829e+00\n",
      " 9.88143264e+00 9.30791227e+00 9.56941000e+00 9.27214879e+00\n",
      " 9.55079159e+00 9.55948647e+00 9.86495625e+00 9.56699407e+00\n",
      " 9.46762484e+00 9.45023185e+00 9.24942084e+00 9.88976509e+00\n",
      " 9.58955756e+00 9.39662091e+00 9.41296794e+00 9.58016991e+00\n",
      " 9.42349499e+00 9.36113654e+00 9.63190905e+00 9.19360071e+00\n",
      " 9.52034972e+00 9.46906786e+00 9.36715839e+00 9.09260866e+00\n",
      " 9.15372484e+00 9.26259688e+00 9.59342073e+00 9.13891403e+00\n",
      " 9.43988075e+00 9.35233132e+00 9.29081072e+00 9.10888453e+00\n",
      " 9.41957431e+00 9.23010107e+00 9.17891501e+00 9.26690400e+00\n",
      " 9.41159281e+00 9.27862886e+00 9.28810597e+00 9.25543026e+00\n",
      " 9.04572012e+00 9.46385623e+00 8.99159619e+00 8.96681603e+00\n",
      " 9.10404646e+00 9.19866606e+00 9.35859841e+00 9.34019846e+00\n",
      " 8.85460948e+00 9.26545596e+00 9.19881355e+00 9.01118781e+00\n",
      " 9.07897147e+00 9.53537561e+00 8.89571616e+00 9.03987040e+00\n",
      " 9.15184475e+00 8.92977077e+00 9.17648862e+00 8.53999549e+00\n",
      " 9.07095845e+00 8.57461861e+00 9.13400600e+00 8.93734194e+00\n",
      " 8.95640240e+00 9.10264771e+00 8.77830886e+00 9.05084955e+00\n",
      " 9.05299308e+00 8.78650125e+00 9.21050756e+00 8.66897962e+00\n",
      " 8.82466938e+00 8.84484931e+00 9.04964658e+00 9.02863263e+00\n",
      " 8.74791352e+00 9.00664794e+00 8.80548205e+00 8.91771178e+00\n",
      " 8.86293009e+00 9.02657959e+00 8.82743883e+00 8.84177071e+00\n",
      " 9.07484915e+00 8.79652631e+00 9.20769406e+00 8.79470006e+00\n",
      " 8.63029004e+00 8.99305359e+00 8.75504635e+00 8.78095351e+00\n",
      " 8.72776454e+00 8.54474758e+00 8.82592410e+00 8.73206244e+00\n",
      " 8.54180306e+00 8.54876170e+00 8.89209252e+00 8.49827672e+00\n",
      " 8.59527531e+00 8.69959162e+00 8.39976395e+00 8.61927703e+00\n",
      " 8.74388473e+00 8.81940456e+00 8.69372376e+00 8.62819806e+00\n",
      " 8.93276722e+00 8.47310257e+00 8.44552167e+00 8.74101856e+00\n",
      " 8.58767361e+00 8.66565241e+00 8.77138376e+00 8.67159107e+00\n",
      " 8.75581264e+00 8.50977859e+00 8.55619590e+00 8.56271621e+00\n",
      " 8.56211693e+00 8.57707902e+00 8.63330766e+00 8.71241403e+00\n",
      " 8.36179278e+00 8.43687505e+00 8.33885925e+00 8.61311332e+00\n",
      " 8.39203367e+00 8.39723856e+00 8.42342866e+00 8.56087482e+00\n",
      " 8.72799336e+00 8.77361302e+00 8.68005475e+00 8.45379259e+00\n",
      " 8.41980659e+00 8.54879525e+00 8.73927654e+00 8.24537042e+00\n",
      " 8.40027113e+00 8.61418225e+00 8.53637744e+00 8.70170902e+00\n",
      " 8.26264336e+00 8.35063163e+00 8.67064582e+00 8.37956330e+00\n",
      " 8.26654773e+00 8.62601816e+00 8.49477396e+00 8.21022763e+00\n",
      " 8.51308340e+00 8.37079542e+00 8.54059928e+00 8.44137965e+00\n",
      " 8.44815123e+00 8.60664311e+00 8.28207384e+00 8.41984774e+00\n",
      " 8.48623994e+00 8.26301802e+00 8.30447463e+00 8.36563339e+00\n",
      " 8.52851030e+00 8.24750848e+00 8.62403364e+00 8.34980087e+00\n",
      " 9.22638368e+00 8.35537926e+00 8.53492878e+00 8.21996712e+00\n",
      " 8.32747759e+00 8.70953033e+00 8.44022517e+00 8.07087180e+00\n",
      " 8.54810825e+00 8.14934697e+00 8.36529319e+00 8.40323977e+00\n",
      " 8.00676826e+00 8.40219120e+00 8.67197249e+00 8.18065902e+00\n",
      " 8.51311926e+00 8.13481665e+00 7.92155885e+00 8.38931127e+00\n",
      " 8.16796375e+00 8.51320589e+00 8.64652672e+00 8.54744057e+00\n",
      " 8.18928820e+00 8.62198947e+00 8.24127463e+00 8.34036909e+00\n",
      " 8.24203679e+00 8.04380400e+00 8.24823052e+00 8.53398777e+00\n",
      " 8.24192463e+00 8.17662968e+00 8.11692183e+00 8.47516120e+00\n",
      " 8.14188437e+00 8.10845564e+00 8.57263385e+00 8.23233665e+00\n",
      " 8.39746086e+00 8.29402928e+00 7.97815850e+00 8.32232164e+00\n",
      " 8.02359146e+00 8.30315399e+00 7.88882444e+00 8.36970809e+00\n",
      " 8.16714691e+00 8.21360482e+00 8.32326554e+00 8.33913651e+00\n",
      " 8.14836892e+00 8.32700118e+00 8.25020684e+00 8.32322852e+00\n",
      " 8.23355071e+00 8.33926481e+00 8.29651992e+00 8.25888486e+00\n",
      " 8.25621061e+00 8.43655301e+00 8.27720175e+00 8.22166503e+00\n",
      " 8.34202014e+00 8.31080982e+00 8.33623350e+00 8.17267754e+00\n",
      " 8.30937979e+00 8.14783003e+00 8.27755877e+00 8.18142319e+00\n",
      " 8.03286316e+00 8.11232709e+00 8.23912483e+00 8.31673769e+00\n",
      " 8.35200000e+00 8.32107464e+00 8.13671533e+00 8.18254473e+00\n",
      " 8.42462699e+00 8.29284226e+00 8.24473157e+00 8.09159635e+00\n",
      " 8.19178947e+00 7.68354615e+00 8.50319888e+00 8.28788167e+00\n",
      " 7.90431899e+00 8.27728409e+00 7.97180071e+00 8.14472345e+00\n",
      " 8.26658393e+00 8.26839247e+00 8.20261394e+00 8.14571615e+00\n",
      " 8.23136253e+00 8.22826803e+00 8.25704276e+00 7.96471458e+00\n",
      " 8.25635621e+00 8.51315002e+00 8.37317011e+00 8.32038553e+00\n",
      " 8.11919684e+00 8.30576537e+00 8.04915632e+00 8.16475783e+00\n",
      " 8.25659955e+00 8.15916239e+00 8.17620675e+00 8.35927237e+00\n",
      " 8.19556062e+00 8.18557916e+00 8.04279241e+00 7.95432413e+00\n",
      " 8.14293453e+00 8.19440517e+00 7.93787202e+00 8.51853341e+00\n",
      " 7.96263220e+00 8.13932377e+00 8.32807067e+00 8.09777796e+00\n",
      " 8.02442269e+00 7.87585435e+00 7.90445010e+00 8.30142720e+00\n",
      " 7.99871159e+00 8.14177771e+00 7.93605325e+00 8.13051686e+00\n",
      " 8.14177728e+00 8.17272866e+00 8.32906961e+00 8.08550588e+00\n",
      " 8.03156150e+00 7.86039792e+00 8.08403303e+00 8.19316671e+00\n",
      " 8.26806833e+00 7.90796005e+00 8.45138016e+00 8.28888386e+00\n",
      " 7.99244471e+00 7.89972773e+00 8.15008452e+00 7.73725367e+00\n",
      " 8.15886746e+00 8.19360260e+00 8.33691157e+00 8.32864734e+00\n",
      " 8.09424958e+00 7.96615857e+00 8.29171838e+00 8.33122367e+00\n",
      " 8.04880147e+00 7.94145948e+00 8.17645842e+00 7.85779808e+00\n",
      " 8.38054947e+00 8.15478891e+00 7.97660204e+00 8.27312448e+00\n",
      " 8.36008800e+00 8.06661788e+00 7.73910082e+00 8.09577114e+00\n",
      " 8.24148757e+00 8.11348283e+00 8.19129224e+00 8.18420007e+00\n",
      " 8.33541405e+00 8.25657367e+00 8.21167511e+00 8.13106487e+00\n",
      " 7.84444838e+00 8.00500183e+00 8.19024330e+00 8.07192590e+00\n",
      " 8.30509639e+00 8.39184745e+00 8.09769689e+00 8.23424580e+00\n",
      " 8.06598147e+00 7.90230151e+00 8.10629074e+00 8.19563938e+00\n",
      " 8.13747554e+00 8.18963190e+00 8.14414199e+00 8.21145548e+00\n",
      " 8.26810842e+00 8.18937620e+00 8.06218921e+00 7.92806016e+00\n",
      " 8.15416569e+00 8.14584849e+00 8.05837385e+00 8.07871663e+00\n",
      " 8.05089739e+00 8.08878276e+00 8.09721052e+00 8.19077569e+00\n",
      " 8.27497103e+00 8.48008314e+00 8.19963800e+00 7.99490501e+00\n",
      " 8.12941978e+00 8.49630874e+00 8.10387410e+00 8.37292799e+00\n",
      " 8.38992163e+00 8.13876916e+00 8.12143017e+00 8.28896807e+00\n",
      " 8.11181284e+00 8.25634341e+00 8.06170706e+00 8.26995233e+00\n",
      " 7.96608762e+00 8.29842571e+00 8.48170107e+00 8.08803959e+00\n",
      " 8.55293951e+00 8.45853576e+00 8.06312406e+00 8.19105854e+00\n",
      " 8.24588473e+00 8.27038667e+00 8.43140558e+00 8.28875825e+00\n",
      " 8.21579574e+00 8.29182144e+00 8.17954830e+00 8.15550643e+00\n",
      " 8.69203968e+00 8.18542337e+00 8.36688960e+00 8.15084856e+00\n",
      " 8.37358006e+00 8.25044978e+00 8.27231647e+00 8.47052764e+00\n",
      " 8.41023594e+00 8.22484660e+00 8.22167116e+00 8.34662587e+00\n",
      " 8.33776981e+00 8.34508470e+00 8.57932575e+00 8.53260060e+00\n",
      " 8.41676564e+00 8.20535743e+00 8.28810962e+00 8.35500140e+00\n",
      " 8.31184871e+00 8.38313942e+00 8.25338519e+00 8.42996814e+00\n",
      " 8.32430915e+00 8.41817790e+00 8.36960536e+00 8.33060788e+00\n",
      " 8.35836839e+00 8.54280472e+00 8.56776366e+00 8.24277577e+00\n",
      " 8.11916851e+00 8.33577522e+00 8.24148892e+00 8.52081075e+00\n",
      " 8.36605793e+00 8.55122457e+00 8.50560438e+00 8.36852540e+00\n",
      " 8.29179124e+00 8.24774706e+00 8.64071173e+00 8.23047217e+00\n",
      " 8.40319047e+00 8.39211992e+00 8.41678672e+00 8.30620486e+00\n",
      " 8.30012306e+00 8.38945249e+00 8.51200378e+00 8.43931390e+00\n",
      " 8.66382618e+00 8.42887478e+00 8.45889664e+00 8.58799737e+00\n",
      " 8.39241575e+00 8.40189748e+00 8.53537734e+00 8.38115635e+00\n",
      " 8.24994413e+00 8.49678997e+00 8.56752529e+00 8.31337955e+00\n",
      " 8.21558378e+00 8.56778073e+00 8.25171596e+00 8.37146015e+00\n",
      " 8.38140744e+00 8.15297949e+00 8.65953823e+00 8.40378610e+00\n",
      " 8.08493524e+00 8.57829630e+00 8.68102046e+00 8.44072347e+00\n",
      " 8.34630462e+00 8.85828476e+00 8.48863169e+00 8.48129004e+00\n",
      " 8.68565416e+00 8.52344608e+00 8.56913948e+00 8.76247560e+00\n",
      " 8.58502754e+00 8.55402275e+00 8.37707903e+00 8.65624349e+00\n",
      " 8.65601249e+00 8.45730579e+00 8.50799974e+00 8.66584830e+00\n",
      " 8.70605820e+00 8.81069142e+00 8.61153907e+00 8.42195162e+00\n",
      " 8.77662115e+00 8.58104033e+00 8.59835900e+00 8.60287595e+00\n",
      " 8.90739396e+00 8.71331360e+00 8.63810475e+00 8.47174835e+00\n",
      " 8.87870864e+00 8.70299549e+00 8.66491505e+00 8.60333423e+00\n",
      " 8.61241168e+00 8.62166159e+00 8.61720585e+00 8.57210740e+00\n",
      " 8.92848997e+00 8.52922501e+00 8.63718483e+00 8.79593490e+00\n",
      " 8.67120672e+00 8.67401726e+00 8.72862788e+00 8.72520882e+00\n",
      " 8.62269737e+00 8.38607746e+00 8.86462897e+00 8.78279980e+00\n",
      " 8.85531827e+00 8.72984476e+00 8.80263980e+00 9.01454735e+00\n",
      " 8.63382070e+00 8.75170294e+00 8.88272174e+00 8.54468112e+00\n",
      " 8.66047609e+00 8.62493106e+00 8.70558264e+00 8.64019153e+00\n",
      " 8.69271655e+00 8.59376665e+00 8.83981998e+00 9.08846040e+00\n",
      " 8.86192696e+00 8.64963567e+00 9.00861644e+00 8.55903355e+00\n",
      " 8.98172543e+00 8.74442242e+00 8.77867228e+00 8.80608504e+00\n",
      " 8.95548969e+00 8.71064213e+00 8.90751027e+00 8.85426888e+00\n",
      " 8.81461094e+00 8.87947488e+00 8.56868647e+00 8.75114530e+00\n",
      " 9.07332387e+00 8.88924673e+00 8.80034052e+00 9.14192380e+00\n",
      " 8.88406405e+00 8.85510880e+00 8.99291213e+00 8.82943690e+00\n",
      " 8.88105795e+00 9.14268465e+00 9.10264667e+00 9.06746748e+00\n",
      " 8.88737752e+00 9.03882517e+00 9.19407111e+00 8.93267529e+00\n",
      " 8.98510663e+00 8.99421041e+00 9.28976932e+00 9.02983519e+00\n",
      " 9.06156504e+00 8.85840605e+00 9.14536834e+00 9.03497181e+00\n",
      " 8.90906718e+00 9.18614066e+00 8.97762307e+00 8.96895757e+00\n",
      " 9.11002759e+00 8.93057341e+00 9.08088988e+00 9.13644725e+00\n",
      " 9.00415873e+00 9.05473084e+00 9.20498185e+00 9.16510524e+00\n",
      " 9.29783483e+00 9.17814042e+00 9.08933814e+00 9.08962329e+00\n",
      " 9.42580915e+00 9.03990607e+00 9.27815047e+00 9.25578765e+00\n",
      " 9.00654045e+00 8.80191151e+00 9.21764922e+00 9.03435206e+00\n",
      " 9.04191105e+00 9.54525950e+00 8.89248642e+00 9.51174257e+00\n",
      " 9.14497835e+00 9.30287388e+00 9.03377056e+00 9.23722540e+00\n",
      " 9.31666515e+00 9.21773377e+00 9.08731860e+00 9.54802822e+00\n",
      " 9.21376023e+00 9.19008936e+00 9.28687194e+00 8.75393985e+00\n",
      " 9.37489903e+00 9.30397587e+00 9.27764539e+00 9.48066824e+00\n",
      " 9.28986777e+00 9.24404065e+00 9.14133888e+00 9.26984000e+00\n",
      " 9.29462657e+00 9.45308477e+00 9.35665841e+00 9.39397691e+00\n",
      " 9.11936024e+00 9.03306364e+00 9.28080998e+00 9.46313754e+00\n",
      " 9.33177063e+00 9.33756227e+00 9.28278346e+00 9.37988970e+00\n",
      " 9.13566612e+00 9.37070804e+00 9.36100887e+00 9.43088033e+00\n",
      " 9.59340675e+00 9.53229962e+00 9.50677098e+00 9.31269607e+00\n",
      " 9.41034222e+00 9.40406107e+00 9.38495262e+00 9.65519697e+00\n",
      " 9.64003747e+00 9.72316549e+00 9.46867212e+00 9.44920926e+00\n",
      " 9.38561300e+00 9.54915733e+00 9.67841602e+00 9.54071924e+00\n",
      " 9.51005778e+00 9.47985928e+00 9.45773484e+00 9.42936393e+00\n",
      " 9.22912148e+00 1.00179995e+01 9.67230302e+00 9.47355915e+00\n",
      " 9.82916315e+00 9.26843715e+00 9.75795863e+00 9.93070575e+00\n",
      " 9.59635989e+00 9.35326712e+00 1.00458280e+01 9.44607164e+00\n",
      " 9.63059993e+00 9.66236832e+00 9.54206115e+00 9.70591984e+00\n",
      " 9.90719162e+00 9.35851391e+00 9.67634995e+00 9.75141101e+00\n",
      " 9.71991439e+00 9.56033222e+00 9.64536353e+00 9.72518599e+00\n",
      " 9.94760934e+00 9.62859443e+00 9.78204450e+00 9.72148028e+00\n",
      " 9.58293012e+00 9.59943820e+00 9.97697077e+00 9.83783239e+00\n",
      " 9.86211155e+00 9.55485312e+00 9.91972008e+00 9.78198954e+00\n",
      " 9.68816003e+00 9.67343436e+00 9.78168650e+00 9.89881848e+00\n",
      " 1.00097602e+01 9.92091796e+00 9.70561313e+00 9.54828628e+00\n",
      " 9.76874165e+00 9.86282054e+00 9.72837084e+00 9.89040007e+00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39.83694184362577"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "loss = Loss(\"mean_squared_error\", sequential=True, verbose=False)\n",
    "a = Input(input_shape=1, verbose=False)\n",
    "b = Recurrent(a, input_shape=1, output_shape=1, learning_rate=0.01, clipping=10.0, verbose=False)\n",
    "c = Recurrent(b, input_shape=1, output_shape=1, learning_rate=0.01, clipping=10.0, verbose=False)\n",
    "d = Output(b, loss_forward=loss.forward, loss_backward=loss.backward, output_shape=1, verbose=False)\n",
    "\n",
    "e = Network(d, verbose=True)\n",
    "e.fit(x, y, batch_size=1, epoch=1000, random_state=0)\n",
    "np.sum(mean_squared_error(e.predict(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit@ <__main__.Network object at 0x1182d2090>\n",
      "[1.91725419e+04 7.44176716e+00 1.96733109e+01 1.30550945e+01\n",
      " 1.39840276e+01 1.72194636e+01 2.55401967e+01 1.30740759e+01\n",
      " 4.57464351e+00 1.67322472e+01 1.37642656e+01 7.77225887e+00\n",
      " 9.12876103e+00 1.16736765e+01 1.72696648e+01 7.40654524e+00\n",
      " 1.26062563e+01 3.64549510e+00 1.30068802e+00 3.78349032e+00\n",
      " 2.79670273e+00 5.41066413e+00 1.21175450e+01 7.69367062e+00\n",
      " 4.14777142e+00 2.17591958e+00 6.14474847e+00 2.93755742e+01\n",
      " 1.42537254e+00 1.24618161e+01 1.81114447e+00 3.53782645e+00\n",
      " 8.41236990e+00 2.00095592e+00 1.87862423e+00 4.61894056e+00\n",
      " 1.52784009e+01 3.33130370e+00 1.49391754e+00 3.59777626e+00\n",
      " 5.03665814e+00 1.63641932e+01 2.78602553e+00 1.45273166e+00\n",
      " 1.00633902e+00 1.14847065e+00 8.78403399e-01 4.11163580e+00\n",
      " 1.18738717e+01 2.46639597e+00 1.02950374e+00 9.18014127e+00\n",
      " 7.88455941e-01 1.86583576e+01 1.74716938e+01 5.62019361e+00\n",
      " 2.48360927e+00 4.94378492e-01 1.46744159e+00 2.78570659e+00\n",
      " 6.48840078e-01 1.03299637e+01 3.71518052e+00 4.88548723e+00\n",
      " 1.32418618e+00 7.44319211e-01 1.50742149e+01 3.43888332e+00\n",
      " 2.87800830e-01 5.73087181e-01 8.47141707e-01 8.83147309e-01\n",
      " 1.36352512e+00 1.24090002e+00 3.52114025e-01 5.75473110e-01\n",
      " 2.99871763e-01 9.73748982e-01 2.20234612e+00 6.16294901e-01\n",
      " 2.55356222e-01 3.59242365e-01 2.83168784e+00 5.00136090e-01\n",
      " 5.59633790e-01 4.00834187e-01 1.07011094e+00 1.08213509e+00\n",
      " 2.49498133e-01 7.97864678e-01 2.44757647e+00 2.79714986e-01\n",
      " 4.18932742e-01 2.71171147e-01 2.59863347e+00 8.97610432e-01\n",
      " 4.39180258e-01 4.12386346e-01 2.29103908e-01 7.78029258e+00\n",
      " 2.22933493e-01 9.30006116e+00 1.44610013e-01 2.59276196e-01\n",
      " 1.07439203e+00 6.95111695e-01 8.03211652e+00 8.94137426e-01\n",
      " 1.58182595e+00 6.80770136e-01 2.66343349e-01 1.53796189e+00\n",
      " 2.82974586e-01 1.99623288e-01 1.87802886e+00 8.78715332e-01\n",
      " 2.30159414e-01 6.56194441e-01 1.33849323e-01 5.93281307e-02\n",
      " 1.62223909e-01 4.32794142e-01 3.18050110e-01 2.47995062e-01\n",
      " 1.90879782e-01 2.98431349e-01 1.03804662e-01 2.64691597e-01\n",
      " 1.52969481e+00 1.43363628e-01 1.75609987e+00 3.28464994e-01\n",
      " 3.82905897e-01 7.10518649e-02 7.74582824e-01 2.29289059e-01\n",
      " 5.63316619e-01 4.04793464e-02 2.33155246e-02 1.13411492e-01\n",
      " 1.62023562e-01 7.92982859e-02 1.29866989e-01 2.44448904e-01\n",
      " 1.81408029e-01 7.74114932e-02 7.89688570e-02 7.65420695e-02\n",
      " 6.52621397e-02 7.74555212e-02 5.66240016e-01 7.15263159e-01\n",
      " 4.07382549e-02 6.17954736e-02 4.86481800e-02 2.02270647e-01\n",
      " 3.16899807e-02 5.67861694e-02 2.62900774e-01 1.07044329e-01\n",
      " 3.86915685e-02 1.24276622e-01 2.56014010e-01 2.24011847e-01\n",
      " 5.30894741e-02 4.54069801e-02 4.44163141e-02 4.03441212e-02\n",
      " 1.81514213e-02 1.34774770e-01 2.92322674e+00 3.16471762e-01\n",
      " 3.86765088e-02 1.55489105e-01 6.76947866e-02 2.52038942e-02\n",
      " 4.75494623e-02 6.67190213e-02 1.80822544e-01 6.08280320e-02\n",
      " 4.14070692e-02 4.29096966e-02 1.76523948e-02 4.85132436e-02\n",
      " 6.79416854e-01 1.58734224e-01 2.76980130e-02 2.07472759e-02\n",
      " 5.17111519e-02 1.04944416e-01 1.19879178e-02 2.94498643e-02\n",
      " 3.13917836e-02 1.20787101e-02 2.35289481e-01 8.00114822e+00\n",
      " 2.17499130e-02 8.93488757e-03 6.59568810e-02 1.11844432e-02\n",
      " 6.21917989e-03 8.62727973e-03 2.98334552e-02 6.68278951e-03\n",
      " 1.38981065e-02 1.70704928e-02 1.11936674e-02 2.03036549e-02\n",
      " 1.05878795e-02 1.50913593e-02 1.72567317e-02 6.68752893e-03\n",
      " 6.87176991e-02 2.28523812e-02 5.16750882e-02 5.86940635e-01\n",
      " 9.36052551e-03 1.30555220e-02 1.05902163e-02 1.28262944e-02\n",
      " 1.56893919e-02 1.14148613e-01 4.62209170e-02 1.50891234e-02\n",
      " 1.01986478e-02 2.41926399e-02 1.32890660e-02 5.48641207e-03\n",
      " 2.39451016e-02 4.27251832e-03 4.35082274e-03 2.00163299e-01\n",
      " 5.41703104e-03 7.38413444e-03 1.42471064e-01 2.00908728e-03\n",
      " 1.37597890e-02 5.53562835e-03 1.36690119e-02 4.56348189e-03\n",
      " 6.21865654e-03 6.06035342e-03 2.00352494e-03 3.95994459e-03\n",
      " 7.55799129e-03 1.12820963e-02 1.17752844e-03 1.44262718e-02\n",
      " 7.30769733e-02 4.99066550e-03 4.02899147e-03 4.66328883e-02\n",
      " 3.91081715e-03 1.44369626e-01 1.64671605e-03 2.54577732e-03\n",
      " 7.48813647e-04 7.75326542e-03 3.82510108e-03 6.59878977e-03\n",
      " 1.01992663e-02 1.27502666e-03 2.66238279e-03 4.47383846e-03\n",
      " 7.25507699e-04 2.09604702e-03 4.09249638e-02 7.71576946e-02\n",
      " 2.02270368e-03 2.90293311e-03 3.65518113e-03 2.07935671e-02\n",
      " 5.42469671e-04 8.76394669e-04 4.05981737e-03 8.43061991e-04\n",
      " 2.04712716e-03 6.43349582e-04 3.58421308e-02 1.00928614e-03\n",
      " 1.39384840e-03 1.25487038e-03 3.35578771e-03 1.91950899e-03\n",
      " 8.45584892e-04 1.77043498e-02 6.84984637e-03 2.45887942e-03\n",
      " 2.99472402e-03 4.81850246e-04 7.12759034e-04 8.28570773e-04\n",
      " 8.22595413e-04 7.43871905e-04 7.34351692e-04 5.40288341e-04\n",
      " 6.01420536e-04 1.89732166e-03 7.17417005e-04 1.49177987e-03\n",
      " 1.73297312e-03 4.61449238e-04 7.94756953e-04 1.89141397e-04\n",
      " 6.78780793e-04 1.90272332e-03 3.74930420e-04 4.71007081e-04\n",
      " 4.29279760e-04 6.76326802e-04 3.87652146e-04 2.96101323e-04\n",
      " 2.07806569e-03 3.92485076e-04 2.42775284e-04 2.20995821e-04\n",
      " 5.47057774e-04 3.02632750e-02 5.31230683e-04 5.06572458e-04\n",
      " 2.19832963e-03 2.28459482e-04 2.20627099e-04 7.81869798e-05\n",
      " 3.83683388e-04 1.92078425e-04 1.98824382e-04 1.57922646e-04\n",
      " 1.42798117e-03 2.48923771e-04 9.60921468e-04 3.12514583e-04\n",
      " 8.40860264e-05 1.02810227e-04 1.81154707e-04 3.90453888e-03\n",
      " 9.95155231e-04 7.72185953e-05 2.57822159e-04 2.04633311e-04\n",
      " 1.03822065e-04 2.65338670e-04 6.64500212e-04 1.76003018e-04\n",
      " 2.33617819e-03 1.25509108e-04 1.08633752e-04 1.78843630e-04\n",
      " 7.69011847e-04 6.63467062e-02 7.63858617e-05 1.90474270e-04\n",
      " 1.23985567e-03 7.29572884e-04 1.54171299e-04 5.98125674e-04\n",
      " 4.14637056e-04 2.63025872e-04 1.37310386e-04 3.42855503e-03\n",
      " 5.03672062e-05 3.81171837e-04 2.43694862e-04 1.17170427e-04\n",
      " 4.66652599e-05 3.63403936e-05 2.06658539e-04 3.87105765e-04\n",
      " 9.91932461e-04 1.52634236e-04 1.43130554e-04 1.14280450e-04\n",
      " 8.04192503e-05 8.85901626e-05 9.34644550e-05 6.79892763e-05\n",
      " 1.98706637e-04 1.81414077e-04 3.24229418e-05 2.37414922e-04\n",
      " 7.38509312e-05 2.12888835e-05 3.68246480e-04 8.74368243e-05\n",
      " 2.15372741e-05 5.99558991e-04 1.93601752e-05 4.10235529e-04\n",
      " 8.55486518e-05 3.69666281e-05 4.50781316e-05 3.90000853e-05\n",
      " 2.39736906e-05 6.48123575e-05 4.46922103e-05 1.53409348e-04\n",
      " 4.53772722e-05 1.01191072e-04 2.87787314e-04 2.24824655e-05\n",
      " 1.09651333e-04 4.01343283e-05 4.20524440e-05 3.01997597e-05\n",
      " 5.79412493e-05 3.76163736e-05 1.67003934e-04 1.63420507e-05\n",
      " 1.22183110e-04 2.21215220e-05 1.57727492e-05 5.97498973e-05\n",
      " 3.34244597e-05 2.48637757e-05 3.39460183e-05 2.87929767e-05\n",
      " 6.97719023e-05 1.74436776e-05 6.44385163e-05 3.92651962e-05\n",
      " 3.15012115e-05 3.81453563e-06 9.86186121e-06 3.22556617e-05\n",
      " 2.34581753e-04 1.20947402e-05 7.82875855e-06 9.79606257e-06\n",
      " 1.24747368e-05 1.23291226e-05 4.11665000e-05 2.89319182e-05\n",
      " 2.86998991e-05 2.67030415e-06 4.77442699e-05 2.81084483e-04\n",
      " 3.43815093e-05 4.46609615e-06 1.15010764e-05 4.64901556e-06\n",
      " 1.88159072e-05 1.87413061e-05 6.84019123e-05 4.94893938e-06\n",
      " 1.59888489e-05 1.49475454e-04 3.27179526e-06 1.88620394e-05\n",
      " 1.05738180e-04 1.86075398e-05 7.06862558e-06 1.02218258e-05\n",
      " 1.13226130e-05 6.32809491e-06 8.85049704e-06 2.37596154e-05\n",
      " 9.97792129e-06 1.29485854e-05 3.60588295e-04 2.03398173e-05\n",
      " 2.42102327e-05 6.12208373e-06 3.18650369e-05 6.36187639e-06\n",
      " 6.08287374e-06 2.35919002e-06 3.26455498e-06 2.23492570e-05\n",
      " 5.90422331e-06 1.04202243e-04 1.91177388e-05 7.25320496e-06\n",
      " 6.47662978e-06 5.59643703e-06 1.55200273e-06 6.91442612e-06\n",
      " 1.79883822e-05 6.64876139e-05 3.66686081e-06 6.52749312e-06\n",
      " 7.21959708e-05 1.54136638e-05 1.39948189e-06 3.02412570e-06\n",
      " 4.88846538e-06 5.36882092e-05 1.05667632e-06 1.38522655e-06\n",
      " 1.46578610e-06 1.49977749e-06 2.23210455e-06 1.70292192e-05\n",
      " 7.83231829e-06 4.29612991e-06 2.85684974e-06 1.60510424e-05\n",
      " 1.26492549e-06 1.89473704e-06 5.45249209e-06 1.54195151e-06\n",
      " 1.14072880e-05 1.57313497e-06 1.55292306e-05 1.03777822e-06\n",
      " 1.04122206e-05 1.80617498e-06 3.69913809e-06 1.90622109e-06\n",
      " 1.43245861e-06 2.78745531e-06 1.48237696e-06 1.07108177e-06\n",
      " 6.48826391e-07 1.58199881e-06 1.24626016e-06 1.37836152e-06\n",
      " 9.11543226e-07 6.63268219e-07 2.61620770e-06 8.72515674e-07\n",
      " 3.30841791e-06 6.65466237e-07 6.73133329e-05 2.82703194e-05\n",
      " 4.60928532e-07 8.44987191e-07 2.15067626e-06 5.89537815e-06\n",
      " 1.73367766e-06 2.86983657e-06 1.40961070e-05 9.28903745e-07\n",
      " 1.14698439e-06 3.45806579e-07 2.95578234e-06 1.13657747e-05\n",
      " 9.02380170e-07 3.58937538e-06 1.28802271e-06 5.73599822e-07\n",
      " 8.05696624e-05 1.12684700e-06 1.32457479e-06 9.33448777e-07\n",
      " 3.53275933e-07 2.08266516e-07 9.34508777e-07 3.73499648e-06\n",
      " 2.76878642e-07 4.06449969e-06 9.77902970e-07 5.18364749e-07\n",
      " 1.17251266e-06 3.97797366e-07 1.54696345e-07 2.89111448e-07\n",
      " 2.98046684e-07 5.95875446e-06 6.33604156e-07 3.43561258e-07\n",
      " 9.10201726e-07 2.03633368e-07 3.00358319e-07 1.71955510e-07\n",
      " 3.77932389e-06 2.57715893e-07 9.97245501e-07 1.24957211e-06\n",
      " 1.45904786e-06 5.25630959e-07 3.36083267e-07 3.04489772e-06\n",
      " 1.01709923e-07 5.72066335e-07 1.30785398e-06 6.79249604e-07\n",
      " 1.18129634e-07 2.89276007e-07 2.07184456e-06 2.07701882e-07\n",
      " 2.52336442e-07 3.68610762e-07 5.46554468e-07 8.10841070e-07\n",
      " 5.12299222e-07 4.28708159e-07 3.71385122e-07 1.38545443e-07\n",
      " 1.61774379e-07 2.85635845e-07 3.91985040e-07 3.82938532e-07\n",
      " 5.36875107e-07 2.55576428e-07 3.20927231e-07 2.31846633e-07\n",
      " 2.26030981e-07 1.51195101e-06 2.87215400e-07 1.11943791e-06\n",
      " 2.67776456e-07 1.49052893e-06 3.92201016e-08 4.82995198e-08\n",
      " 5.55061649e-07 5.87163944e-07 1.45418438e-07 2.91994251e-08\n",
      " 1.62989420e-07 3.79994279e-08 2.55535117e-07 1.18354291e-06\n",
      " 3.42982313e-08 1.87693142e-06 5.88710799e-08 1.44732175e-06\n",
      " 3.44322453e-06 6.91197857e-08 2.28864002e-06 8.03638121e-08\n",
      " 8.66601710e-08 6.82420428e-08 1.14673560e-07 1.90987579e-07\n",
      " 8.55258479e-08 2.69964844e-08 9.09218983e-08 3.30130401e-08\n",
      " 1.03903142e-07 1.02653912e-07 7.26195659e-06 5.52408701e-08\n",
      " 1.78517553e-07 4.38825265e-08 1.80713550e-07 1.00020787e-07\n",
      " 5.38566275e-07 2.02558640e-08 4.46633905e-08 9.59982068e-07\n",
      " 1.24871618e-07 6.18282953e-08 1.40327664e-07 1.43298825e-08\n",
      " 7.39123970e-07 5.50199427e-09 2.75367445e-08 1.18717805e-08\n",
      " 3.75198385e-07 7.25003607e-09 2.27887130e-07 5.15083031e-07\n",
      " 4.02943521e-08 6.01088438e-07 8.04590947e-08 5.03140261e-08\n",
      " 1.00115406e-08 5.57789691e-09 1.97674575e-07 1.16741978e-08\n",
      " 1.63429710e-08 9.52779154e-09 1.66101578e-08 6.25527439e-09\n",
      " 5.65647660e-08 6.47580706e-08 5.07874473e-08 7.23711043e-09\n",
      " 2.89519222e-09 9.86904889e-09 1.23824630e-08 8.77971643e-08\n",
      " 5.91299173e-08 6.17635696e-07 3.70882928e-08 1.87515483e-08\n",
      " 1.77285191e-08 9.44975654e-09 1.86804207e-07 2.66721263e-08\n",
      " 1.17182463e-08 5.71355069e-09 3.80792242e-09 3.08513866e-08\n",
      " 1.25282249e-08 7.85300129e-09 4.14961045e-08 8.39566058e-09\n",
      " 5.26815232e-09 1.29340120e-07 7.26645365e-09 4.14982732e-09\n",
      " 1.11594729e-08 1.74841230e-08 6.85241749e-09 7.22791661e-08\n",
      " 3.05599403e-08 4.12676227e-08 6.66985837e-09 6.50729210e-09\n",
      " 1.43145019e-08 9.11684197e-08 4.32340670e-09 3.46561266e-09\n",
      " 6.45723491e-09 6.40661018e-09 2.14566984e-09 3.39062069e-09\n",
      " 4.19239702e-09 3.87089170e-09 8.95651904e-09 7.32614818e-08\n",
      " 1.63346906e-09 1.69038827e-08 2.58577214e-09 2.42254067e-09\n",
      " 6.26394018e-09 6.98150494e-09 1.06679604e-09 2.03916800e-09\n",
      " 1.20164373e-08 2.01721120e-08 1.89169177e-08 2.28062350e-09\n",
      " 4.07822663e-09 2.56227602e-09 1.90347698e-09 2.30515992e-09\n",
      " 8.13882496e-10 5.52630010e-10 1.39368190e-08 2.06908733e-09\n",
      " 2.52986486e-09 6.61535641e-10 1.11010664e-09 7.06019133e-09\n",
      " 5.83655382e-09 9.11781404e-10 1.38311642e-09 1.55077779e-09\n",
      " 4.58674508e-09 5.57093002e-09 1.13784488e-08 3.13589798e-09\n",
      " 1.91518475e-09 1.41643310e-08 9.39893537e-10 4.40121598e-09\n",
      " 2.28847191e-09 2.44873415e-09 4.06770629e-09 2.08246904e-09\n",
      " 7.64573469e-09 1.11896907e-09 8.31794247e-09 9.01541249e-10\n",
      " 2.35523940e-09 6.43638442e-10 2.29231146e-09 6.02589669e-10\n",
      " 1.98119918e-09 1.43150495e-09 7.55450255e-10 1.21297223e-09\n",
      " 3.79335176e-09 9.64578612e-10 7.01192191e-10 3.76141533e-10\n",
      " 1.05139295e-09 2.52818581e-09 6.18886958e-10 2.19051983e-10\n",
      " 5.57436562e-10 9.78177793e-10 7.65505209e-10 7.97714367e-10\n",
      " 5.75494903e-10 1.74830197e-09 4.85468981e-10 1.32231640e-09\n",
      " 7.08600492e-10 1.16557622e-09 1.93390654e-09 7.65321298e-10\n",
      " 4.89873314e-10 3.24242329e-10 9.41868891e-09 8.73168812e-10\n",
      " 2.81305714e-09 7.40791509e-10 1.49274008e-09 1.50255797e-10\n",
      " 1.01589242e-10 2.82977191e-10 1.58092730e-09 2.89991705e-10\n",
      " 5.89402154e-10 7.04887934e-10 4.89152041e-10 7.02550681e-11\n",
      " 8.91926762e-10 6.94074092e-10 8.50795651e-10 2.79625817e-10\n",
      " 1.10794588e-10 3.86488858e-10 1.67523088e-10 1.08126619e-09\n",
      " 4.20147791e-09 2.19911408e-10 3.62145174e-10 9.17066912e-10\n",
      " 4.72654620e-10 2.59236525e-10 1.32088320e-10 1.33634886e-09\n",
      " 5.10195625e-10 6.50518953e-09 8.47687240e-09 9.92302313e-11\n",
      " 2.14061821e-10 4.66487027e-10 2.18730568e-10 1.13099789e-10\n",
      " 6.68878556e-10 3.50469419e-10 2.52093678e-10 2.49226037e-10\n",
      " 3.39586566e-10 1.64465634e-09 1.38971317e-10 1.12066695e-10\n",
      " 1.39349725e-10 1.10968607e-10 2.79404611e-10 1.91802886e-10\n",
      " 4.15101310e-10 2.12868499e-10 3.66357265e-11 4.01934226e-10\n",
      " 2.36653644e-10 1.60731745e-09 1.00085075e-10 2.62653202e-10\n",
      " 2.78857229e-11 1.50026290e-10 1.21613383e-10 2.73241984e-11\n",
      " 5.74801923e-10 7.42102468e-11 3.98816259e-11 2.59071745e-10\n",
      " 6.33951307e-11 4.88734436e-10 3.17474513e-11 9.89090740e-11\n",
      " 4.74405735e-11 5.97610461e-11 3.77640710e-10 8.03844572e-10\n",
      " 1.25461909e-10 6.17377077e-11 3.98029838e-10 4.79244890e-11\n",
      " 1.15691377e-10 1.78981257e-11 3.20742432e-11 1.07581003e-10\n",
      " 1.78872130e-10 2.48645612e-11 1.02653951e-10 2.29144919e-11\n",
      " 2.72733490e-11 5.37428977e-10 3.81488676e-10 9.87653841e-10\n",
      " 1.43809667e-11 8.96548774e-11 1.63193330e-10 2.85133546e-11\n",
      " 5.96938049e-11 2.32935161e-11 1.38156973e-10 1.84548404e-09\n",
      " 4.46791683e-11 9.41132207e-11 2.00948448e-09 5.78966166e-11\n",
      " 1.59798778e-10 3.33244481e-11 2.78757426e-11 3.89759367e-11\n",
      " 1.00287811e-10 2.58580466e-10 7.59046983e-11 1.19816461e-09\n",
      " 1.33941760e-11 3.91360172e-11 7.22252322e-11 2.57260848e-11\n",
      " 8.11451141e-12 1.30085086e-11 4.58570968e-11 2.05691216e-10\n",
      " 4.47903473e-11 6.01482198e-12 5.28516028e-11 5.11207909e-11\n",
      " 2.67374279e-11 1.30290792e-11 5.09750135e-12 2.64624756e-11\n",
      " 1.37861915e-11 3.34303093e-11 1.60369666e-10 4.73746423e-11\n",
      " 6.48814179e-12 5.51427364e-12 1.15756421e-11 2.98041228e-11\n",
      " 1.60783657e-11 5.47028259e-11 7.26147142e-11 2.78784985e-11\n",
      " 3.17363449e-12 2.24665702e-11 1.36693111e-10 4.72516877e-11\n",
      " 1.29389374e-11 9.10474219e-12 8.64900248e-12 5.09556151e-12\n",
      " 1.39679194e-11 2.03554514e-12 3.70396998e-12 3.35372227e-12\n",
      " 2.41910919e-12 1.69403930e-11 2.47586789e-11 6.36985110e-12\n",
      " 1.03736057e-11 4.05889408e-12 1.72470219e-11 5.48416019e-12\n",
      " 9.67898904e-12 2.79382255e-11 5.47749886e-12 6.37967831e-12\n",
      " 3.86461778e-12 1.47081188e-10 1.64096345e-11 1.37013022e-11\n",
      " 4.12388585e-12 3.75454337e-12 2.41865603e-12 7.60708596e-12\n",
      " 3.07933389e-11 1.48995002e-12 3.23192108e-12 8.50815238e-12\n",
      " 6.08534836e-12 9.30827537e-12 8.68132014e-11 1.04705970e-12\n",
      " 6.83142651e-12 2.66837740e-12 4.39591486e-12 2.08882308e-12\n",
      " 1.25164464e-11 1.60947667e-11 1.86372672e-12 4.30061711e-12\n",
      " 6.03128044e-12 2.06352694e-11 1.37985697e-12 1.66037994e-12\n",
      " 1.87740537e-11 1.30650272e-11 5.11954305e-13 3.08665983e-11\n",
      " 9.13491457e-13 7.95409760e-12 1.90764019e-12 1.30137555e-12\n",
      " 8.42687570e-12 2.71017615e-12 2.12564895e-12 2.77815969e-12\n",
      " 4.73883251e-12 1.63438979e-12 4.92562110e-12 5.55654095e-13\n",
      " 1.57155549e-12 1.52332766e-12 1.67094846e-12 1.14623279e-12\n",
      " 1.20629147e-12 8.88757790e-13 8.19635392e-13 1.54004246e-11]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4035436111928946e-12"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "loss = Loss(\"mean_squared_error\", sequential=True, verbose=False)\n",
    "a = Input(input_shape=1, verbose=False)\n",
    "b = Recurrent(a, input_shape=1, output_shape=3, learning_rate=0.001, clipping=10.0, verbose=False)\n",
    "c = Dense(b, input_shape=3, output_shape=1, learning_rate=0.01, sequential=True, verbose=False)\n",
    "d = Output(c, loss_forward=loss.forward, loss_backward=loss.backward, output_shape=1, verbose=False)\n",
    "e = Network(d, verbose=True)\n",
    "e.fit(x, y, batch_size=1, epoch=1000, random_state=0)\n",
    "\n",
    "np.sum(mean_squared_error(e.predict(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 4, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = np.arange(24, dtype=np.float64).reshape(2, 3, 4)\n",
    "dx = np.arange(18, dtype=np.float64).reshape(2, 3, 3)\n",
    "# y = np.arange(6, dtype=np.float64).reshape(2, 3, 1)\n",
    "x = np.arange(100).reshape(20,5,1)[:, :-1]\n",
    "y = np.arange(100).reshape(20,5,1)[:, 1:]\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6., 4.]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.empty((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(6).reshape((1,2,3))\n",
    "a[(0,0,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([\n",
    "\n",
    "[[[1,0],[0,2]],\n",
    " [[0,0],[0,0]]],\n",
    "\n",
    "[[[0,0],[0,0]],\n",
    " [[3,0],[0,4]]]\n",
    " \n",
    "])\n",
    "\n",
    "b = np.array([\n",
    " [[1,0],[0,2]],\n",
    " [[3,0],[0,4]],\n",
    "])\n",
    "\n",
    "a[0,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "diag([[1,2],[3,4]], shape=(2,2,2,2), axis=(2,3))\n",
    "->\n",
    "[\n",
    "\n",
    "[[[1,0],[0,2]],\n",
    " [[0,0],[0,0]]],\n",
    "\n",
    "[[[0,0],[0,0]],\n",
    " [[3,0],[0,4]]]\n",
    " \n",
    "]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 後回し\n",
    "# 再帰的に処理すれば良い\n",
    "# axis: (xを埋める軸たち......, バインドされる軸たち)\n",
    "def diag(x, axis):\n",
    "    n = x.ndim\n",
    "    free = axis[:n]\n",
    "    bound = axis[n:]\n",
    "    \n",
    "    dim = x.ndim\n",
    "    y = np.empty(shape)\n",
    "    for d in range(dim):\n",
    "        if d in axis:\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill(x, shape):\n",
    "    m, n = x.shape[0], x.shape[1]\n",
    "    y = np.empty((m, n, *shape))\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            y[i][j].fill(x[i][j])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[2., 2., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 2., 2.]]]])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill(np.array([[1,2]]), (3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]],\n",
       "\n",
       "       [[ 0.,  0.,  0.,  0.],\n",
       "        [12., 13., 14., 15.],\n",
       "        [16., 17., 18., 19.],\n",
       "        [20., 21., 22., 23.]]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0番目の要素を追加する\n",
    "def padding(x):\n",
    "    batch_size, seq_length, input_shape = x.shape\n",
    "    y = np.concatenate([np.zeros((batch_size, 1, input_shape)), x], axis=1)\n",
    "    return y\n",
    "\n",
    "a = np.arange(24).reshape((2,3,4))\n",
    "padding(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: hの初期値を与えられるようにする\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def dsigmoid(x, y=None):\n",
    "    z = sigmoid(x) if (y is None) else y\n",
    "    return z * (1.0 - z)\n",
    "\n",
    "def tanh(x):\n",
    "    exp_x = np.exp(x)\n",
    "    exp_minus_x = np.exp(-x)\n",
    "    return (exp_x - exp_minus_x) / (exp_x + exp_minus_x)\n",
    "\n",
    "def dtanh(x, y=None):\n",
    "    exp_x = np.exp(x)\n",
    "    exp_minus_x = np.exp(-x)\n",
    "    return 4.0 / (exp_x + exp_minus_x)**2\n",
    "\n",
    "class LSTM():\n",
    "    def __init__(self, layer, input_shape, output_shape, learning_rate=0.001, verbose=False):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.x = None\n",
    "        self.h = None\n",
    "        self.c = None\n",
    "        self.gate_f = None\n",
    "        self.gate_g = None\n",
    "        self.gate_i = None\n",
    "        self.gate_o = None\n",
    "        # (forget, gain, input, output)\n",
    "        self.W_x = np.random.randn(4, output_shape, input_shape)\n",
    "        self.W_h = np.random.randn(4, output_shape, output_shape)\n",
    "        self.b = np.random.randn(4, output_shape)\n",
    "        self.dx = None\n",
    "        self.dh = None\n",
    "        self.dc = None\n",
    "        self.dW_x = np.empty((4, output_shape, input_shape))\n",
    "        self.dW_h = np.empty((4, output_shape, output_shape))\n",
    "        self.db = np.empty((4, output_shape))\n",
    "        self.lr = learning_rate\n",
    "        self.layer = layer\n",
    "        self.verbose = verbose\n",
    "        if (self.verbose):\n",
    "            print(\"init@\", self)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # NOTE: index 0は h,c のみが利用する\n",
    "        self.x = padding(self.layer.forward(x))\n",
    "        batch_size, seq_length, _ = self.x.shape\n",
    "        self.c = np.empty((batch_size, seq_length, self.output_shape))\n",
    "        self.h = np.empty((batch_size, seq_length, self.output_shape))\n",
    "        self.gate_f = np.random.randn(batch_size, seq_length, self.output_shape) * 0.01\n",
    "        self.gate_g = np.random.randn(batch_size, seq_length, self.output_shape) * 0.01\n",
    "        self.gate_i = np.random.randn(batch_size, seq_length, self.output_shape) * 0.01\n",
    "        self.gate_o = np.random.randn(batch_size, seq_length, self.output_shape) * 0.01\n",
    "            \n",
    "        self.c[:, 0, :] = np.zeros_like(self.c[:, 0, :])\n",
    "        self.h[:, 0, :] = np.zeros_like(self.h[:, 0, :])\n",
    "        \n",
    "        for t in range(1, seq_length):\n",
    "            self.gate_f[:, t, :] = sigmoid(np.dot(self.x[:, t, :], self.W_x[0, ...].T) + np.dot(self.h[:, t, :], self.W_h[0, ...].T) + np.tile(self.b[0, ...], (batch_size, 1)))\n",
    "            self.gate_g[:, t, :] = sigmoid(np.dot(self.x[:, t, :], self.W_x[1, ...].T) + np.dot(self.h[:, t, :], self.W_h[1, ...].T) + np.tile(self.b[1, ...], (batch_size, 1)))\n",
    "            self.gate_i[:, t, :] = tanh(np.dot(self.x[:, t, :], self.W_x[2, ...].T) + np.dot(self.h[:, t, :], self.W_h[2, ...].T) + np.tile(self.b[2, ...], (batch_size, 1)))\n",
    "            self.gate_o[:, t, :] = sigmoid(np.dot(self.x[:, t, :], self.W_x[3, ...].T) + np.dot(self.h[:, t, :], self.W_h[3, ...].T) + np.tile(self.b[3, ...], (batch_size, 1)))\n",
    "            self.c[:, t, :] = self.c[:, t-1, :] * self.gate_f[:, t, :] + self.gate_g[:, t, :] * self.gate_i[:, t, :]\n",
    "            self.h[:, t, :] = tanh(self.c[:, t, :]) * self.gate_o[:, t, :]\n",
    "                \n",
    "        if (self.verbose):\n",
    "            print(\"forward@\", self, \" and return value is\")\n",
    "            print(self.h[:, 1:, :])\n",
    "        return self.h[:, 1:, :]\n",
    "    \n",
    "    # TODO: Back Propagationを実装する\n",
    "    def backward(self, dy):\n",
    "        batch_size, seq_length, _ = dy.shape\n",
    "        seq_length += 1\n",
    "        self.dx = np.empty((batch_size, seq_length, self.input_shape))\n",
    "        self.dh = np.empty((batch_size, seq_length, self.output_shape))\n",
    "        \n",
    "        # dx, dh, dc\n",
    "        dc = np.zeros((batch_size, self.output_shape))\n",
    "        dh = dy[:, -1, :]\n",
    "        \n",
    "        for t in reversed(range(1, seq_length-1)):\n",
    "            # FIXME: renameする\n",
    "            tmp = dc + dtanh(self.c[:, t, :]) * self.gate_o[:, t, :] * dh\n",
    "            \n",
    "            dc = self.gate_f[:, t, :] * tmp\n",
    "            df = self.c[:, t, :] * tmp * dsigmoid(_, self.gate_f[:, t, :])\n",
    "            dg = self.gate_i[:, t, :] * tmp * dsigmoid(_, self.gate_g[:, t, :])\n",
    "            di = self.gate_g[:, t, :] * tmp * dtanh(self.gate_i[:, t, :])\n",
    "            do = tanh(self.c[:, t, :]) * dh * dsigmoid(_, self.gate_o[:, t, :])\n",
    "        \n",
    "            dx = np.dot(df, self.W_x[0]) + np.dot(dg, self.W_x[1]) + np.dot(di, self.W_x[2]) + np.dot(do, self.W_x[3])\n",
    "            dh = np.dot(df, self.W_h[0]) + np.dot(dg, self.W_h[1]) + np.dot(di, self.W_h[2]) + np.dot(do, self.W_h[3])\n",
    "        \n",
    "        dc_dWx = np.zeros((batch_size, self.output_shape, self.output_shape, self.input_shape))\n",
    "        dc_dWh = np.zeros((batch_size, self.output_shape, self.output_shape, self.output_shape))\n",
    "        dc_db = np.zeros((batch_size, self.output_shape, self.output_shape))\n",
    "        dh_dWx = np.zeros((batch_size, self.output_shape, self.output_shape, self.input_shape))\n",
    "        dh_dWh = np.zeros((batch_size, self.output_shape, self.output_shape, self.output_shape))\n",
    "        dh_db = np.zeros((batch_size, self.output_shape, self.output_shape))\n",
    "        \n",
    "        for t in range(1,seq_length):\n",
    "            # dh_dWx\n",
    "            df_dWx = np.array([identity(self.x[n, t, :], self.output_shape) + np.dot(self.W_h[0], dh_dWx[n, ...]) for n in range(batch_size)])\n",
    "            dg_dWx = np.array([identity(self.x[n, t, :], self.output_shape) + np.dot(self.W_h[1], dh_dWx[n, ...]) for n in range(batch_size)])\n",
    "            di_dWx = np.array([identity(self.x[n, t, :], self.output_shape) + np.dot(self.W_h[2], dh_dWx[n, ...]) for n in range(batch_size)])\n",
    "            do_dWx = np.array([identity(self.x[n, t, :], self.output_shape) + np.dot(self.W_h[3], dh_dWx[n, ...]) for n in range(batch_size)])\n",
    "\n",
    "            gate_f = fill(self.gate_f[:, t, :], (self.output_shape, self.input_shape))\n",
    "            gate_g = fill(self.gate_g[:, t, :], (self.output_shape, self.input_shape))\n",
    "            gate_i = fill(self.gate_i[:, t, :], (self.output_shape, self.input_shape))\n",
    "            gate_o = fill(self.gate_o[:, t, :], (self.output_shape, self.input_shape))\n",
    "            c_prev = fill(self.c[:, t-1, :], (self.output_shape, self.input_shape))\n",
    "            c_pos = fill(self.c[:, t, :], (self.output_shape, self.input_shape))\n",
    "            \n",
    "            dc_dWx = df_dWx * c_prev + gate_f * dc_dWx + dg_dWx * gate_i + gate_g * di_dWx\n",
    "            dh_dWx = dtanh(c_pos) * gate_o + tanh(c_pos) * do_dWx\n",
    "\n",
    "            # dh_dWh\n",
    "            df_dWh = np.array([identity(self.h[n, t, :], self.output_shape) + np.dot(self.W_h[0], dh_dWh[n, ...]) for n in range(batch_size)])\n",
    "            dg_dWh = np.array([identity(self.h[n, t, :], self.output_shape) + np.dot(self.W_h[1], dh_dWh[n, ...]) for n in range(batch_size)])\n",
    "            di_dWh = np.array([identity(self.h[n, t, :], self.output_shape) + np.dot(self.W_h[2], dh_dWh[n, ...]) for n in range(batch_size)])\n",
    "            do_dWh = np.array([identity(self.h[n, t, :], self.output_shape) + np.dot(self.W_h[3], dh_dWh[n, ...]) for n in range(batch_size)])\n",
    "            \n",
    "            gate_f = fill(self.gate_f[:, t, :], (self.output_shape, self.output_shape))\n",
    "            gate_g = fill(self.gate_g[:, t, :], (self.output_shape, self.output_shape))\n",
    "            gate_i = fill(self.gate_i[:, t, :], (self.output_shape, self.output_shape))\n",
    "            gate_o = fill(self.gate_o[:, t, :], (self.output_shape, self.output_shape))\n",
    "            c_prev = fill(self.c[:, t-1, :], (self.output_shape, self.output_shape))\n",
    "            c_pos = fill(self.c[:, t, :], (self.output_shape, self.output_shape))\n",
    "            \n",
    "            dc_dWh = df_dWh * c_prev + gate_f * dc_dWh + dg_dWh * gate_i + gate_g * di_dWh\n",
    "            dh_dWh = dtanh(c_pos) * gate_o + tanh(c_pos) * do_dWh\n",
    "            \n",
    "            # dh_db\n",
    "            df_db = np.array([np.eye(self.output_shape) + np.dot(self.b[0], dh_db[n, ...]) for n in range(batch_size)])\n",
    "            dg_db = np.array([np.eye(self.output_shape) + np.dot(self.b[1], dh_db[n, ...]) for n in range(batch_size)])\n",
    "            di_db = np.array([np.eye(self.output_shape) + np.dot(self.b[2], dh_db[n, ...]) for n in range(batch_size)])\n",
    "            do_db = np.array([np.eye(self.output_shape) + np.dot(self.b[3], dh_db[n, ...]) for n in range(batch_size)])\n",
    "            \n",
    "            gate_f = fill(self.gate_f[:, t, :], (self.output_shape,))\n",
    "            gate_g = fill(self.gate_g[:, t, :], (self.output_shape,))\n",
    "            gate_i = fill(self.gate_i[:, t, :], (self.output_shape,))\n",
    "            gate_o = fill(self.gate_o[:, t, :], (self.output_shape,))\n",
    "            c_prev = fill(self.c[:, t-1, :], (self.output_shape, self.output_shape))\n",
    "            c_pos = fill(self.c[:, t, :], (self.output_shape, self.output_shape))\n",
    "            \n",
    "            dc_db = df_db * c_prev + gate_f * dc_db + dg_db * gate_i + gate_g * di_db\n",
    "            dh_db = dtanh(c_pos) * gate_o + tanh(c_pos) * do_db\n",
    "        \n",
    "        self.dW_x = np.sum([np.dot(self.dh[n, -1, :], dh_dWx[n, ...]) for n in range(batch_size)], axis=0)\n",
    "        self.dW_h = np.sum([np.dot(self.dh[n, -1, :], dh_dWh[n, ...]) for n in range(batch_size)], axis=0)\n",
    "        self.db = np.sum(self.dh, axis=(0,1))\n",
    "            \n",
    "        if (self.verbose):\n",
    "            print(\"bakcward@\", self, \" and return value is \")\n",
    "            print(self.dx)\n",
    "        return self.layer.backward(self.dx)\n",
    "        \n",
    "        # dx, dh\n",
    "#         self.dx[:, -1, :] = np.dot(dy[:, -1, :], self.W_x)\n",
    "#         self.dh[:, -1, :] = np.dot(dy[:, -1, :], self.W_h)\n",
    "#         for t in reversed(range(seq_length-1)):\n",
    "#             self.dx[:, t, :] = np.dot(dy[:, t, :] + self.dh[:, t+1, :], self.W_x)\n",
    "#             self.dh[:, t, :] = np.dot(dy[:, t, :] + self.dh[:, t+1, :], self.W_h)\n",
    "#         # dW_x, dW_h, db\n",
    "#         # NOTE: 変数のdW_xは\\frac{\\partial h}{\\partial W}であることに注意する(名前を変えよう)\n",
    "#         dW_x = np.array([identity(self.x[n, 0, :], self.output_shape) for n in range(batch_size)])\n",
    "#         dW_h = np.array([identity(self.h[n, 0, :], self.output_shape) for n in range(batch_size)])\n",
    "#         for t in range(1, seq_length):\n",
    "#             dW_x = np.array([identity(self.x[n, t, :], self.output_shape) + np.dot(self.W_h, dW_x[n, ...]) for n in range(batch_size)])\n",
    "#             dW_h = np.array([identity(self.h[n, t, :], self.output_shape) + np.dot(self.W_h, dW_h[n, ...]) for n in range(batch_size)])\n",
    "#         self.dW_x = np.sum([np.dot(self.dh[n, -1, :], dW_x[n, ...]) for n in range(batch_size)], axis=0)\n",
    "#         self.dW_h = np.sum([np.dot(self.dh[n, -1, :], dW_h[n, ...]) for n in range(batch_size)], axis=0)\n",
    "#         self.db = np.sum(dy, axis=(0,1))\n",
    "#         \n",
    "    \n",
    "    def update(self):\n",
    "        self.W_x -= self.lr * self.dW_x\n",
    "        self.W_h -= self.lr * self.dW_h\n",
    "        self.b -= self.lr * self.db\n",
    "        self.layer.update()\n",
    "        if (self.verbose):\n",
    "            print(\"update@\", self, \" with\")\n",
    "            print(self.lr * np.mean(self.dW_x))\n",
    "    \n",
    "    # DEBUG:\n",
    "    def print_shape(self):\n",
    "        print(\"xs: \", self.xs.shape)\n",
    "        print(\"hs: \", self.hs.shape)\n",
    "        print(\"W_x: \", self.W_x.shape)\n",
    "        print(\"W_h: \", self.W_h.shape)\n",
    "        print(\"b: \", self.b.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit@ <__main__.Network object at 0x11bd57950>\n",
      "0\r",
      "[nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "loss = Loss(\"mean_squared_error\", sequential=True, verbose=False)\n",
    "a = Input(input_shape=1, verbose=False)\n",
    "b = LSTM(a, input_shape=1, output_shape=3, learning_rate=0.001, verbose=False)\n",
    "c = Dense(b, input_shape=3, output_shape=1, learning_rate=0.001, sequential=True, verbose=False)\n",
    "d = Output(c, loss_forward=loss.forward, loss_backward=loss.backward, output_shape=1, verbose=False)\n",
    "\n",
    "e = Network(d, verbose=True)\n",
    "e.fit(x, y, batch_size=1, epoch=1, random_state=0)\n",
    "np.sum(mean_squared_error(e.predict(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
